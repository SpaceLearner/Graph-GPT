<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="relation" for="edge" attr.name="relation" attr.type="string" />
  <key id="abstract" for="node" attr.name="abstract" attr.type="string" />
  <key id="title" for="node" attr.name="title" attr.type="string" />
  <graph edgedefault="undirected">
    <node id="P4441">
      <data key="title">do altmetrics correlate with the quality of papers a large scale empirical study based on f1000prime data</data>
      <data key="abstract">In this study, we address the question whether (and to what extent, respectively) altmetrics are related to the scientific quality of papers (as measured by peer assessments). Only a few studies have previously investigated the relationship between altmetrics and assessments by peers. In the first step, we analyse the underlying dimensions of measurement for traditional metrics (citation counts) and altmetrics–by using principal component analysis (PCA) and factor analysis (FA). In the second step, we test the relationship between the dimensions and quality of papers (as measured by the post-publication peer-review system of F1000Prime assessments)–using regression analysis. The results of the PCA and FA show that altmetrics operate along different dimensions, whereas Mendeley counts are related to citation counts, and tweets form a separate dimension. The results of the regression analysis indicate that citation-based metrics and readership counts are significantly more related to quality, than tweets. This result on the one hand questions the use of Twitter counts for research evaluation purposes and on the other hand indicates potential use of Mendeley reader counts.</data>
    </node>
    <node id="P126218">
      <data key="title">scholarly twitter metrics</data>
      <data key="abstract">Twitter has arguably been the most popular among the data sources that form the basis of so-called altmetrics. Tweets to scholarly documents have been heralded as both early indicators of citations as well as measures of societal impact. This chapter provides an overview of Twitter activity as the basis for scholarly metrics from a critical point of view and equally describes the potential and limitations of scholarly Twitter metrics. By reviewing the literature on Twitter in scholarly communication and analyzing 24 million tweets linking to scholarly documents, it aims to provide a basic understanding of what tweets can and cannot measure in the context of research evaluation. Going beyond the limited explanatory power of low correlations between tweets and citations, this chapter considers what types of scholarly documents are popular on Twitter, and how, when and by whom they are diffused in order to understand what tweets to scholarly documents measure. Although this chapter is not able to solve the problems associated with the creation of meaningful metrics from social media, it highlights particular issues and aims to provide the basis for advanced scholarly Twitter metrics.</data>
    </node>
    <node id="P83545">
      <data key="title">the role of twitter in the life cycle of a scientific publication</data>
      <data key="abstract">Twitter is a micro-blogging social media platform for short messages that can have a long-term impact on how scientists create and publish ideas. We investigate the usefulness of twitter in the development and distribution of scientific knowledge. At the start of the life cycle of a scientific publication, twitter provides a large virtual department of colleagues that can help to rapidly generate, share and refine new ideas. As ideas become manuscripts, twitter can be used as an informal arena for the pre-review of works in progress. Finally, tweeting published findings can communicate research to a broad audience of other researchers, decision makers, journalists and the general public that can amplify the scientific and social impact of publications. However, there are limitations, largely surrounding issues of intellectual property and ownership, inclusiveness and misrepresentations of science sound bites. Nevertheless, we believe twitter is a useful social media tool that can provide a valuable contribution to scientific publishing in the 21st century.</data>
    </node>
    <node id="P152314">
      <data key="title">is there currently a scientific revolution in scientometrics</data>
      <data key="abstract">The author of this letter to the editor would like to set forth the argument that scientometrics is currently in a phase in which a taxonomic change, and hence a revolution, is taking place. One of the key terms in scientometrics is scientific impact which nowadays is understood to mean not only the impact on science but the impact on every area of society.</data>
    </node>
    <node id="P119139">
      <data key="title">how well developed are altmetrics a cross disciplinary analysis of the presence of alternative metrics in scientific publications</data>
      <data key="abstract">In this paper an analysis of the presence and possibilities of altmetrics for bibliometric and performance analysis is carried out. Using the web based tool Impact Story, we collected metrics for 20,000 random publications from the Web of Science. We studied both the presence and distribution of altmetrics in the set of publications, across fields, document types and over publication years, as well as the extent to which altmetrics correlate with citation indicators. The main result of the study is that the altmetrics source that provides the most metrics is Mendeley, with metrics on readerships for 62.6 % of all the publications studied, other sources only provide marginal information. In terms of relation with citations, a moderate spearman correlation (r = 0.49) has been found between Mendeley readership counts and citation indicators. Other possibilities and limitations of these indicators are discussed and future research lines are outlined.</data>
    </node>
    <node id="P8579">
      <data key="title">measuring book impact based on the multi granularity online review mining</data>
      <data key="abstract">As with articles and journals, the customary methods for measuring books' academic impact mainly involve citations, which is easy but limited to interrogating traditional citation databases and scholarly book reviews, Researchers have attempted to use other metrics, such as Google Books, libcitation, and publisher prestige. However, these approaches lack content-level information and cannot determine the citation intentions of users. Meanwhile, the abundant online review resources concerning academic books can be used to mine deeper information and content utilizing altmetric perspectives. In this study, we measure the impacts of academic books by multi-granularity mining online reviews, and we identify factors that affect a book's impact. First, online reviews of a sample of academic books on Amazon.cn are crawled and processed. Then, multi-granularity review mining is conducted to identify review sentiment polarities and aspects' sentiment values. Lastly, the numbers of positive reviews and negative reviews, aspect sentiment values, star values, and information regarding helpfulness are integrated via the entropy method, and lead to the calculation of the final book impact scores. The results of a correlation analysis of book impact scores obtained via our method versus traditional book citations show that, although there are substantial differences between subject areas, online book reviews tend to reflect the academic impact. Thus, we infer that online reviews represent a promising source for mining book impact within the altmetric perspective and at the multi-granularity content level. Moreover, our proposed method might also be a means by which to measure other books besides academic publications.</data>
    </node>
    <node id="P73929">
      <data key="title">how to calculate the practical significance of citation impact differences an empirical example from evaluative institutional bibliometrics using adjusted predictions and marginal effects</data>
      <data key="abstract">Evaluative bibliometrics is concerned with comparing research units by using statistical procedures. According to Williams (2012) an empirical study should be concerned with the substantive and practical significance of the findings as well as the sign and statistical significance of effects. In this study we will explain what adjusted predictions and marginal effects are and how useful they are for institutional evaluative bibliometrics. As an illustration, we will calculate a regression model using publications (and citation data) produced by four universities in German-speaking countries from 1980 to 2010. We will show how these predictions and effects can be estimated and plotted, and how this makes it far easier to get a practical feel for the substantive meaning of results in evaluative bibliometric studies. We will focus particularly on Average Adjusted Predictions (AAPs), Average Marginal Effects (AMEs), Adjusted Predictions at Representative Values (APRVs) and Marginal Effects at Representative Values (MERVs).</data>
    </node>
    <node id="P103642">
      <data key="title">normalization of zero inflated data an empirical analysis of a new indicator family and its use with altmetrics data</data>
      <data key="abstract">Recently, two new indicators (Equalized Mean-based Normalized Proportion Cited, EMNPC; Mean-based Normalized Proportion Cited, MNPC) were proposed which are intended for sparse scientometrics data. The indicators compare the proportion of mentioned papers (e.g. on Facebook) of a unit (e.g., a researcher or institution) with the proportion of mentioned papers in the corresponding fields and publication years (the expected values). In this study, we propose a third indicator (Mantel-Haenszel quotient, MHq) belonging to the same indicator family. The MHq is based on the MH analysis - an established method in statistics for the comparison of proportions. We test (using citations and assessments by peers, i.e. F1000Prime recommendations) if the three indicators can distinguish between different quality levels as defined on the basis of the assessments by peers. Thus, we test their convergent validity. We find that the indicator MHq is able to distinguish between the quality levels in most cases while MNPC and EMNPC are not. Since the MHq is shown in this study to be a valid indicator, we apply it to six types of zero-inflated altmetrics data and test whether different altmetrics sources are related to quality. The results for the various altmetrics demonstrate that the relationship between altmetrics (Wikipedia, Facebook, blogs, and news data) and assessments by peers is not as strong as the relationship between citations and assessments by peers. Actually, the relationship between citations and peer assessments is about two to three times stronger than the association between altmetrics and assessments by peers.</data>
    </node>
    <node id="P107188">
      <data key="title">usefulness of altmetrics for measuring the broader impact of research a case study using data from plos altmetrics and f1000prime paper tags</data>
      <data key="abstract">Purpose: Whereas citation counts allow the measurement of the impact of research on research itself, an important role in the measurement of the impact of research on other parts of society is ascribed to altmetrics. The present case study investigates the usefulness of altmetrics for measuring the broader impact of research. Methods: This case study is essentially based on a dataset with papers obtained from F1000. The dataset was augmented with altmetrics (such as Twitter counts) which were provided by PLOS (the Public Library of Science). In total, the case study covers a total of 1,082 papers. Findings: The F1000 dataset contains tags on papers which were assigned intellectually by experts and which can characterise a paper. The most interesting tag for altmetric research is "good for teaching". This tag is assigned to papers which could be of interest to a wider circle of readers than the peers in a specialist area. Particularly on Facebook and Twitter, one could expect papers with this tag to be mentioned more often than those without this tag. With respect to the "good for teaching" tag, the results from regression models were able to confirm these expectations: Papers with this tag show significantly higher Facebook and Twitter counts than papers without this tag. This association could not be seen with Mendeley or Figshare counts (that is with counts from platforms which are chiefly of interest in a scientific context). Conclusions: The results of the current study indicate that Facebook and Twitter, but not Figshare or Mendeley, can provide indications of papers which are of interest to a broader circle of readers (and not only for the peers in a specialist area), and seem therefore be useful for societal impact measurement.</data>
    </node>
    <node id="P54536">
      <data key="title">the substantive and practical significance of citation impact differences between institutions guidelines for the analysis of percentiles using effect sizes and confidence intervals</data>
      <data key="abstract">In this chapter we address the statistical analysis of percentiles: How should the citation impact of institutions be compared? In educational and psychological testing, percentiles are already used widely as a standard to evaluate an individual’s test scores—intelligence tests for example—by comparing them with the scores of a calibrated sample. Percentiles, or percentile rank classes, are also a very suitable method for bibliometrics to normalize citations of publications in terms of the subject category and the publication year and, unlike the mean-based indicators (the relative citation rates), percentiles are scarcely affected by skewed distributions of citations. The percentile of a certain publication provides information about the citation impact this publication has achieved in comparison to other similar publications in the same subject category and publication year. Analyses of percentiles, however, have not always been presented in the most effective and meaningful way. New APA guidelines (Association American Psychological, Publication manual of the American Psychological Association (6 ed.). Washington, DC: American Psychological Association (APA), 2010) suggest a lesser emphasis on significance tests and a greater emphasis on the substantive and practical significance of findings. Drawing on work by Cumming (Understanding the new statistics: effect sizes, confidence intervals, and meta-analysis. London: Routledge, 2012) we show how examinations of effect sizes (e.g., Cohen’s d statistic) and confidence intervals can lead to a clear understanding of citation impact differences.</data>
    </node>
    <node id="P76634">
      <data key="title">which percentile based approach should be preferred for calculating normalized citation impact values an empirical comparison of five approaches including a newly developed citation rank approach p100</data>
      <data key="abstract">Percentile-based approaches have been proposed as a non-parametric alternative to parametric central-tendency statistics to normalize observed citation counts. Percentiles are based on an ordered set of citation counts in a reference set, whereby the fraction of papers at or below the citation counts of a focal paper is used as an indicator for its relative citation impact in the set. In this study, we pursue two related objectives: (1) although different percentile-based approaches have been developed, an approach is hitherto missing that satisfies a number of criteria such as scaling of the percentile ranks from zero (all other papers perform better) to 100 (all other papers perform worse), and solving the problem with tied citation ranks unambiguously. We introduce a new citation-rank approach having these properties, namely P100. (2) We compare the reliability of P100 empirically with other percentile-based approaches, such as the approaches developed by the SCImago group, the Centre for Science and Technology Studies (CWTS), and Thomson Reuters (InCites), using all papers published in 1980 in Thomson Reuters Web of Science (WoS). How accurately can the different approaches predict the long-term citation impact in 2010 (in year 31) using citation impact measured in previous time windows (years 1 to 30)? The comparison of the approaches shows that the method used by InCites overestimates citation impact (because of using the highest percentile rank when papers are assigned to more than a single subject category) whereas the SCImago indicator shows higher power in predicting the long-term citation impact on the basis of citation rates in early years. Since the results show a disadvantage in this predictive ability for P100 against the other approaches, there is still room for further improvements.</data>
    </node>
    <node id="P88426">
      <data key="title">alternative metrics in scientometrics a meta analysis of research into three altmetrics</data>
      <data key="abstract">Alternative metrics are currently one of the most popular research topics in scientometric research. This paper provides an overview of research into three of the most important altmetrics: microblogging (Twitter), online reference managers (Mendeley and CiteULike) and blogging. The literature is discussed in relation to the possible use of altmetrics in research evaluation. Since the research was particularly interested in the correlation between altmetrics counts and citation counts, this overview focuses particularly on this correlation. For each altmetric, a meta-analysis is calculated for its correlation with traditional citation counts. As the results of the meta-analyses show, the correlation with traditional citations for micro-blogging counts is negligible (pooled r=0.003), for blog counts it is small (pooled r=0.12) and for bookmark counts from online reference managers, medium to large (CiteULike pooled r=0.23; Mendeley pooled r=0.51).</data>
    </node>
    <node id="P135798">
      <data key="title">scholarly use of social media and altmetrics a review of the literature</data>
      <data key="abstract">Social media has become integrated into the fabric of the scholarly communication system in fundamental ways: principally through scholarly use of social media platforms and the promotion of new indicators on the basis of interactions with these platforms. Research and scholarship in this area has accelerated since the coining and subsequent advocacy for altmetrics -- that is, research indicators based on social media activity. This review provides an extensive account of the state-of-the art in both scholarly use of social media and altmetrics. The review consists of two main parts: the first examines the use of social media in academia, examining the various functions these platforms have in the scholarly communication process and the factors that affect this use. The second part reviews empirical studies of altmetrics, discussing the various interpretations of altmetrics, data collection and methodological limitations, and differences according to platform. The review ends with a critical discussion of the implications of this transformation in the scholarly communication system.</data>
    </node>
    <node id="P139818">
      <data key="title">real time classification of twitter trends</data>
      <data key="abstract">Social media users give rise to social trends as they share about common interests, which can be triggered by different reasons. In this work, we explore the types of triggers that spark trends on Twitter, introducing a typology with following four types: 'news', 'ongoing events', 'memes', and 'commemoratives'. While previous research has analyzed trending topics in a long term, we look at the earliest tweets that produce a trend, with the aim of categorizing trends early on. This would allow to provide a filtered subset of trends to end users. We analyze and experiment with a set of straightforward language-independent features based on the social spread of trends to categorize them into the introduced typology. Our method provides an efficient way to accurately categorize trending topics without need of external data, enabling news organizations to discover breaking news in real-time, or to quickly identify viral memes that might enrich marketing decisions, among others. The analysis of social features also reveals patterns associated with each type of trend, such as tweets about ongoing events being shorter as many were likely sent from mobile devices, or memes having more retweets originating from a few trend-setters.</data>
    </node>
    <node id="P130">
      <data key="title">validity of altmetrics data for measuring societal impact a study using data from altmetric and f1000prime</data>
      <data key="abstract">Can altmetric data be validly used for the measurement of societal impact? The current study seeks to answer this question with a comprehensive dataset (about 100,000 records) from very disparate sources (F1000, Altmetric, and an in-house database based on Web of Science). In the F1000 peer review system, experts attach particular tags to scientific papers which indicate whether a paper could be of interest for science or rather for other segments of society. The results show that papers with the tag "good for teaching" do achieve higher altmetric counts than papers without this tag - if the quality of the papers is controlled. At the same time, a higher citation count is shown especially by papers with a tag that is specifically scientifically oriented ("new finding"). The findings indicate that papers tailored for a readership outside the area of research should lead to societal impact. If altmetric data is to be used for the measurement of societal impact, the question arises of its normalization. In bibliometrics, citations are normalized for the papers' subject area and publication year. This study has taken a second analytic step involving a possible normalization of altmetric data. As the results show there are particular scientific topics which are of especial interest for a wide audience. Since these more or less interesting topics are not completely reflected in Thomson Reuters' journal sets, a normalization of altmetric data should not be based on the level of subject categories, but on the level of topics.</data>
    </node>
    <node id="P132681">
      <data key="title">measuring field normalized impact of papers on specific societal groups an altmetrics study based on mendeley data</data>
      <data key="abstract">Bibliometrics is successful in measuring impact because the target is clearly defined: the publishing scientist who is still active and working. Thus, citations are a target-oriented metric which measures impact on science. In contrast, societal impact measurements based on altmetrics are as a rule intended to measure impact in a broad sense on all areas of society (e.g. science, culture, politics, and economics). This tendency is especially reflected in the efforts to design composite indicators (e.g. the Altmetric Attention Score). We deem appropriate that not only the impact measurement using citations is target-oriented (citations measure the impact of papers on scientists) but also the measurement of impact using altmetrics. Impact measurements only make sense, if the target group—the recipient of academic papers—is clearly defined. Thus, we extend in this study the field-normalized reader impact indicator proposed by us in an earlier study, which is based on Mendeley data (the mean normalized reader score, MNRS), to a target-oriented field-normalized impact indicator (e.g. MNRSED measures reader impact on the sector of educational donation, i.e. teaching). This indicator can show—as demonstrated in empirical examples—the ability of journals, countries, and academic institutions to publish papers which are below or above the average impact of papers on a specific sector in society (e.g. the educational or teaching sector). Thus, the method allows to measure the impact of scientific papers on certain groups—controlling for the field in which the papers have been published and their publication year.</data>
    </node>
    <node id="P54659">
      <data key="title">relative citation ratio rcr an empirical attempt to study a new field normalized bibliometric indicator</data>
      <data key="abstract">Hutchins, Yuan, Anderson, and Santangelo 2015 proposed the Relative Citation Ratio RCR as a new field-normalized impact indicator. This study investigates the RCR by correlating it on the level of single publications with established field-normalized indicators and assessments of the publications by peers. We find that the RCR correlates highly with established field-normalized indicators, but the correlation between RCR and peer assessments is only low to medium.</data>
    </node>
    <node id="P114174">
      <data key="title">normalization of zero inflated data an empirical analysis of a new indicator family</data>
      <data key="abstract">Recently, two new indicators (Equalized Mean-based Normalized Proportion Cited, EMNPC, and Mean-based Normalized Proportion Cited, MNPC) were proposed which are intended for sparse data. We propose a third indicator (Mantel-Haenszel quotient, MHq) belonging to the same indicator family. The MHq is based on the MH analysis - an established method for polling the data from multiple 2x2 contingency tables based on different subgroups. We test (using citations and assessments by peers) if the three indicators can distinguish between different quality levels as defined on the basis of the assessments by peers (convergent validity). We find that the indicator MHq is able to distinguish between the quality levels in most cases while MNPC and EMNPC are not.</data>
    </node>
    <node id="P102784">
      <data key="title">is collaboration among scientists related to the citation impact of papers because their quality increases with collaboration an analysis based on data from f1000prime and normalized citation scores</data>
      <data key="abstract">In recent years, the relationship of collaboration among scientists and the citation impact of papers have been frequently investigated. Most of the studies show that the two variables are closely related: an increasing collaboration activity (measured in terms of number of authors, number of affiliations, and number of countries) is associated with an increased citation impact. However, it is not clear whether the increased citation impact is based on the higher quality of papers which profit from more than one scientist giving expert input or other (citation-specific) factors. Thus, the current study addresses this question by using two comprehensive datasets with publications (in the biomedical area) including quality assessments by experts (F1000Prime member scores) and citation data for the publications. The study is based on nearly 10,000 papers. Robust regression models are used to investigate the relationship between number of authors, number of affiliations, and number of countries, respectively, and citation impact - controlling for the papers' quality (measured by F1000Prime expert ratings). The results point out that the effect of collaboration activities on impact is largely independent of the papers' quality. The citation advantage is apparently not quality-related; citation specific factors (e.g. self-citations) seem to be important here.</data>
    </node>
    <node id="P152744">
      <data key="title">social media metrics for new research evaluation</data>
      <data key="abstract">This chapter approaches, both from a theoretical and practical perspective, the most important principles and conceptual frameworks that can be considered in the application of social media metrics for scientific evaluation. We propose conceptually valid uses for social media metrics in research evaluation. The chapter discusses frameworks and uses of these metrics as well as principles and recommendations for the consideration and application of current (and potentially new) metrics in research evaluation.</data>
    </node>
    <edge source="P4441" target="P119139">
      <data key="relation">reference</data>
    </edge>
    <edge source="P4441" target="P130">
      <data key="relation">reference</data>
    </edge>
    <edge source="P4441" target="P103642">
      <data key="relation">reference</data>
    </edge>
    <edge source="P4441" target="P135798">
      <data key="relation">reference</data>
    </edge>
    <edge source="P4441" target="P73929">
      <data key="relation">reference</data>
    </edge>
    <edge source="P4441" target="P88426">
      <data key="relation">reference</data>
    </edge>
    <edge source="P126218" target="P130">
      <data key="relation">reference</data>
    </edge>
    <edge source="P126218" target="P83545">
      <data key="relation">reference</data>
    </edge>
    <edge source="P126218" target="P135798">
      <data key="relation">reference</data>
    </edge>
    <edge source="P83545" target="P107188">
      <data key="relation">reference</data>
    </edge>
    <edge source="P83545" target="P130">
      <data key="relation">reference</data>
    </edge>
    <edge source="P83545" target="P132681">
      <data key="relation">reference</data>
    </edge>
    <edge source="P83545" target="P88426">
      <data key="relation">reference</data>
    </edge>
    <edge source="P152314" target="P107188">
      <data key="relation">reference</data>
    </edge>
    <edge source="P152314" target="P119139">
      <data key="relation">reference</data>
    </edge>
    <edge source="P152314" target="P130">
      <data key="relation">reference</data>
    </edge>
    <edge source="P119139" target="P107188">
      <data key="relation">reference</data>
    </edge>
    <edge source="P119139" target="P132681">
      <data key="relation">reference</data>
    </edge>
    <edge source="P119139" target="P103642">
      <data key="relation">reference</data>
    </edge>
    <edge source="P119139" target="P152744">
      <data key="relation">reference</data>
    </edge>
    <edge source="P119139" target="P135798">
      <data key="relation">reference</data>
    </edge>
    <edge source="P119139" target="P88426">
      <data key="relation">reference</data>
    </edge>
    <edge source="P8579" target="P130">
      <data key="relation">reference</data>
    </edge>
    <edge source="P73929" target="P107188">
      <data key="relation">reference</data>
    </edge>
    <edge source="P73929" target="P54536">
      <data key="relation">reference</data>
    </edge>
    <edge source="P73929" target="P130">
      <data key="relation">reference</data>
    </edge>
    <edge source="P73929" target="P102784">
      <data key="relation">reference</data>
    </edge>
    <edge source="P73929" target="P76634">
      <data key="relation">reference</data>
    </edge>
    <edge source="P103642" target="P130">
      <data key="relation">reference</data>
    </edge>
    <edge source="P103642" target="P88426">
      <data key="relation">reference</data>
    </edge>
    <edge source="P107188" target="P130">
      <data key="relation">reference</data>
    </edge>
    <edge source="P54536" target="P130">
      <data key="relation">reference</data>
    </edge>
    <edge source="P54536" target="P102784">
      <data key="relation">reference</data>
    </edge>
    <edge source="P76634" target="P130">
      <data key="relation">reference</data>
    </edge>
    <edge source="P88426" target="P132681">
      <data key="relation">reference</data>
    </edge>
    <edge source="P88426" target="P130">
      <data key="relation">reference</data>
    </edge>
    <edge source="P88426" target="P139818">
      <data key="relation">reference</data>
    </edge>
    <edge source="P135798" target="P152744">
      <data key="relation">reference</data>
    </edge>
    <edge source="P135798" target="P130">
      <data key="relation">reference</data>
    </edge>
    <edge source="P139818" target="P130">
      <data key="relation">reference</data>
    </edge>
    <edge source="P130" target="P54659">
      <data key="relation">reference</data>
    </edge>
    <edge source="P130" target="P132681">
      <data key="relation">reference</data>
    </edge>
    <edge source="P130" target="P114174">
      <data key="relation">reference</data>
    </edge>
    <edge source="P130" target="P152744">
      <data key="relation">reference</data>
    </edge>
    <edge source="P130" target="P102784">
      <data key="relation">reference</data>
    </edge>
  </graph>
</graphml>
