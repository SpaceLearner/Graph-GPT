<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="relation" for="edge" attr.name="relation" attr.type="string" />
  <key id="abstract" for="node" attr.name="abstract" attr.type="string" />
  <key id="title" for="node" attr.name="title" attr.type="string" />
  <graph edgedefault="undirected">
    <node id="P157211">
      <data key="title">do deep nets really need to be deep</data>
      <data key="abstract">Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this extended abstract, we show that shallow feed-forward networks can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models. Moreover, in some cases the shallow neural nets can learn these deep functions using a total number of parameters similar to the original deep model. We evaluate our method on the TIMIT phoneme recognition task and are able to train shallow fully-connected nets that perform similarly to complex, well-engineered, deep convolutional architectures. Our success in training shallow neural nets to mimic deeper models suggests that there probably exist better algorithms for training shallow feed-forward nets than those currently available.</data>
    </node>
    <node id="P50953">
      <data key="title">automatic generation of language independent features for cross lingual classification</data>
      <data key="abstract">Many applications require categorization of text documents using predefined categories. The main approach to performing text categorization is learning from labeled examples. For many tasks, it may be difficult to find examples in one language but easy in others. The problem of learning from examples in one or more languages and classifying (categorizing) in another is called cross-lingual learning. In this work, we present a novel approach that solves the general cross-lingual text categorization problem. Our method generates, for each training document, a set of language-independent features. Using these features for training yields a language-independent classifier. At the classification stage, we generate language-independent features for the unlabeled document, and apply the classifier on the new representation. #R##N#To build the feature generator, we utilize a hierarchical language-independent ontology, where each concept has a set of support documents for each language involved. In the preprocessing stage, we use the support documents to build a set of language-independent feature generators, one for each language. The collection of these generators is used to map any document into the language-independent feature space. #R##N#Our methodology works on the most general cross-lingual text categorization problems, being able to learn from any mix of languages and classify documents in any other language. We also present a method for exploiting the hierarchical structure of the ontology to create virtual supporting documents for languages that do not have them. We tested our method, using Wikipedia as our ontology, on the most commonly used test collections in cross-lingual text categorization, and found that it outperforms existing methods.</data>
    </node>
    <node id="P104763">
      <data key="title">adversarial deep averaging networks for cross lingual sentiment classification</data>
      <data key="abstract">In recent years great success has been achieved in sentiment classification for English, thanks in part to the availability of copious annotated resources. Unfortunately, most languages do not enjoy such an abundance of labeled data. To tackle the sentiment classification problem in low-resource languages without adequate annotated data, we propose an Adversarial Deep Averaging Network (ADAN) to transfer the knowledge learned from labeled data on a resource-rich source language to low-resource languages where only unlabeled data exists. ADAN has two discriminative branches: a sentiment classifier and an adversarial language discriminator. Both branches take input from a shared feature extractor to learn hidden representations that are simultaneously indicative for the classification task and invariant across languages. Experiments on Chinese and Arabic sentiment classification demonstrate that ADAN significantly outperforms state-of-the-art systems.</data>
    </node>
    <node id="P76997">
      <data key="title">effective use of word order for text categorization with convolutional neural networks</data>
      <data key="abstract">Convolutional neural network (CNN) is a neural network that can make use of the internal structure of data such as the 2D structure of image data. This paper studies CNN on text categorization to exploit the 1D structure (namely, word order) of text data for accurate prediction. Instead of using low-dimensional word vectors as input as is often done, we directly apply CNN to high-dimensional text data, which leads to directly learning embedding of small text regions for use in classification. In addition to a straightforward adaptation of CNN from image to text, a simple but new variation which employs bag-of-word conversion in the convolution layer is proposed. An extension to combine multiple convolution layers is also explored for higher accuracy. The experiments demonstrate the effectiveness of our approach in comparison with state-of-the-art methods.</data>
    </node>
    <node id="P133316">
      <data key="title">cross modal distillation for supervision transfer</data>
      <data key="abstract">In this work we propose a technique that transfers supervision between images from different modalities. We use learned representations from a large labeled modality as a supervisory signal for training representations for a new unlabeled paired modality. Our method enables learning of rich representations for unlabeled modalities and can be used as a pre-training procedure for new modalities with limited labeled data. We show experimental results where we transfer supervision from labeled RGB images to unlabeled depth and optical flow images and demonstrate large improvements for both these cross modal supervision transfers. Code, data and pre-trained models are available at this https URL</data>
    </node>
    <node id="P72426">
      <data key="title">dependency sensitive convolutional neural networks for modeling sentences and documents</data>
      <data key="abstract">The goal of sentence and document modeling is to accurately represent the meaning of sentences and documents for various Natural Language Processing tasks. In this work, we present Dependency Sensitive Convolutional Neural Networks (DSCNN) as a general-purpose classification system for both sentences and documents. DSCNN hierarchically builds textual representations by processing pretrained word embeddings via Long Short-Term Memory networks and subsequently extracting features with convolution operators. Compared with existing recursive neural models with tree structures, DSCNN does not rely on parsers and expensive phrase labeling, and thus is not restricted to sentence-level tasks. Moreover, unlike other CNN-based models that analyze sentences locally by sliding windows, our system captures both the dependency information within each sentence and relationships across sentences in the same document. Experiment results demonstrate that our approach is achieving state-of-the-art performance on several tasks, including sentiment analysis, question type classification, and subjectivity classification.</data>
    </node>
    <node id="P154342">
      <data key="title">sequence to sequence learning with neural networks</data>
      <data key="abstract">Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.</data>
    </node>
    <node id="P97372">
      <data key="title">multi source cross lingual model transfer learning what to share</data>
      <data key="abstract">Modern NLP applications have enjoyed a great boost utilizing neural networks models. Such deep neural models, however, are not applicable to most human languages due to the lack of annotated training data for various NLP tasks. Cross-lingual transfer learning (CLTL) is a viable method for building NLP models for a low-resource target language by leveraging labeled data from other (source) languages. In this work, we focus on the multilingual transfer setting where training data in multiple source languages is leveraged to further boost target language performance. #R##N#Unlike most existing methods that rely only on language-invariant features for CLTL, our approach coherently utilizes both language-invariant and language-specific features at instance level. Our model leverages adversarial networks to learn language-invariant features, and mixture-of-experts models to dynamically exploit the similarity between the target language and each individual source language. This enables our model to learn effectively what to share between various languages in the multilingual setup. Moreover, when coupled with unsupervised multilingual embeddings, our model can operate in a zero-resource setting where neither target language training data nor cross-lingual resources are available. Our model achieves significant performance gains over prior art, as shown in an extensive set of experiments over multiple text classification and sequence tagging tasks including a large-scale industry dataset.</data>
    </node>
    <node id="P139900">
      <data key="title">cross language text classification via subspace co regularized multi view learning</data>
      <data key="abstract">In many multilingual text classification problems, the documents in different languages often share the same set of categories. To reduce the labeling cost of training a classification model for each individual language, it is important to transfer the label knowledge gained from one language to another language by conducting cross language classification. In this paper we develop a novel subspace co-regularized multi-view learning method for cross language text classification. This method is built on parallel corpora produced by machine translation. It jointly minimizes the training error of each classifier in each language while penalizing the distance between the subspace representations of parallel documents. Our empirical study on a large set of cross language text classification tasks shows the proposed method consistently outperforms a number of inductive methods, domain adaptation methods, and multi-view learning methods.</data>
    </node>
    <node id="P386">
      <data key="title">cross lingual distillation for text classification</data>
      <data key="abstract">Cross-lingual text classification(CLTC) is the task of classifying documents written in different languages into the same taxonomy of categories. This paper presents a novel approach to CLTC that builds on model distillation, which adapts and extends a framework originally proposed for model compression. Using soft probabilistic predictions for the documents in a label-rich language as the (induced) supervisory labels in a parallel corpus of documents, we train classifiers successfully for new languages in which labeled training data are not available. An adversarial feature adaptation technique is also applied during the model training to reduce distribution mismatch. We conducted experiments on two benchmark CLTC datasets, treating English as the source language and German, French, Japan and Chinese as the unlabeled target languages. The proposed approach had the advantageous or comparable performance of the other state-of-art methods.</data>
    </node>
    <node id="P97441">
      <data key="title">marginalized denoising autoencoders for domain adaptation</data>
      <data key="abstract">Stacked denoising autoencoders (SDAs) have been successfully used to learn new representations for domain adaptation. Recently, they have attained record accuracy on standard benchmark tasks of sentiment analysis across different text domains. SDAs learn robust data representations by reconstruction, recovering original features from data that are artificially corrupted with noise. In this paper, we propose marginalized SDA (mSDA) that addresses two crucial limitations of SDAs: high computational cost and lack of scalability to high-dimensional features. In contrast to SDAs, our approach of mSDA marginalizes noise and thus does not require stochastic gradient descent or other optimization algorithms to learn parameters ? in fact, they are computed in closed-form. Consequently, mSDA, which can be implemented in only 20 lines of MATLAB^{TM}, significantly speeds up SDAs by two orders of magnitude. Furthermore, the representations learnt by mSDA are as effective as the traditional SDAs, attaining almost identical accuracies in benchmark tasks.</data>
    </node>
    <node id="P139095">
      <data key="title">an empirical study on sentiment classification of chinese review using word embedding</data>
      <data key="abstract">In this article, how word embeddings can be used as features in Chinese sentiment classification is presented. Firstly, a Chinese opinion corpus is built with a million comments from hotel review websites. Then the word embeddings which represent each comment are used as input in different machine learning methods for sentiment classification, including SVM, Logistic Regression, Convolutional Neural Network (CNN) and ensemble methods. These methods get better performance compared with N-gram models using Naive Bayes (NB) and Maximum Entropy (ME). Finally, a combination of machine learning methods is proposed which presents an outstanding performance in precision, recall and F1 score. After selecting the most useful methods to construct the combinational model and testing over the corpus, the final F1 score is 0.920.</data>
    </node>
    <node id="P110641">
      <data key="title">convolutional neural networks for sentence classification</data>
      <data key="abstract">We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.</data>
    </node>
    <node id="P94874">
      <data key="title">efficient estimation of word representations in vector space</data>
      <data key="abstract">We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.</data>
    </node>
    <node id="P107735">
      <data key="title">style obfuscation by invariance</data>
      <data key="abstract">The task of obfuscating writing style using sequence models has previously been investigated under the framework of obfuscation-by-transfer, where the input text is explicitly rewritten in another style. These approaches also often lead to major alterations to the semantic content of the input. In this work, we propose obfuscation-by-invariance, and investigate to what extent models trained to be explicitly style-invariant preserve semantics. We evaluate our architectures on parallel and non-parallel corpora, and compare automatic and human evaluations on the obfuscated sentences. Our experiments show that style classifier performance can be reduced to chance level, whilst the automatic evaluation of the output is seemingly equal to models applying style-transfer. However, based on human evaluation we demonstrate a trade-off between the level of obfuscation and the observed quality of the output in terms of meaning preservation and grammaticality.</data>
    </node>
    <node id="P64706">
      <data key="title">distilling word embeddings an encoding approach</data>
      <data key="abstract">Distilling knowledge from a well-trained cumbersome network to a small one has recently become a new research topic, as lightweight neural networks with high performance are particularly in need in various resource-restricted systems. This paper addresses the problem of distilling word embeddings for NLP tasks. We propose an encoding approach to distill task-specific knowledge from a set of high-dimensional embeddings, which can reduce model complexity by a large margin as well as retain high accuracy, showing a good compromise between efficiency and performance. Experiments in two tasks reveal the phenomenon that distilling knowledge from cumbersome embeddings is better than directly training neural networks with small embeddings.</data>
    </node>
    <node id="P6146">
      <data key="title">natural language processing almost from scratch</data>
      <data key="abstract">We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.</data>
    </node>
    <node id="P18630">
      <data key="title">fitnets hints for thin deep nets</data>
      <data key="abstract">While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.</data>
    </node>
    <node id="P4215">
      <data key="title">sequence level knowledge distillation</data>
      <data key="abstract">Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical approaches. However to reach competitive performance, NMT models need to be exceedingly large. In this paper we consider applying knowledge distillation approaches (Bucila et al., 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT. We demonstrate that standard knowledge distillation applied to word-level prediction can be effective for NMT, and also introduce two novel sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model). Our best student model runs 10 times faster than its state-of-the-art teacher with little loss in performance. It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight pruning on top of knowledge distillation results in a student model that has 13 times fewer parameters than the original teacher model, with a decrease of 0.4 BLEU.</data>
    </node>
    <node id="P105511">
      <data key="title">semi supervised sequence learning</data>
      <data key="abstract">We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.</data>
    </node>
    <edge source="P157211" target="P18630">
      <data key="relation">reference</data>
    </edge>
    <edge source="P157211" target="P4215">
      <data key="relation">reference</data>
    </edge>
    <edge source="P157211" target="P386">
      <data key="relation">reference</data>
    </edge>
    <edge source="P157211" target="P64706">
      <data key="relation">reference</data>
    </edge>
    <edge source="P50953" target="P386">
      <data key="relation">reference</data>
    </edge>
    <edge source="P104763" target="P97441">
      <data key="relation">reference</data>
    </edge>
    <edge source="P104763" target="P386">
      <data key="relation">reference</data>
    </edge>
    <edge source="P104763" target="P97372">
      <data key="relation">reference</data>
    </edge>
    <edge source="P76997" target="P110641">
      <data key="relation">reference</data>
    </edge>
    <edge source="P76997" target="P6146">
      <data key="relation">reference</data>
    </edge>
    <edge source="P76997" target="P139095">
      <data key="relation">reference</data>
    </edge>
    <edge source="P76997" target="P386">
      <data key="relation">reference</data>
    </edge>
    <edge source="P76997" target="P105511">
      <data key="relation">reference</data>
    </edge>
    <edge source="P133316" target="P386">
      <data key="relation">reference</data>
    </edge>
    <edge source="P133316" target="P18630">
      <data key="relation">reference</data>
    </edge>
    <edge source="P72426" target="P386">
      <data key="relation">reference</data>
    </edge>
    <edge source="P72426" target="P110641">
      <data key="relation">reference</data>
    </edge>
    <edge source="P72426" target="P105511">
      <data key="relation">reference</data>
    </edge>
    <edge source="P154342" target="P4215">
      <data key="relation">reference</data>
    </edge>
    <edge source="P154342" target="P386">
      <data key="relation">reference</data>
    </edge>
    <edge source="P154342" target="P107735">
      <data key="relation">reference</data>
    </edge>
    <edge source="P154342" target="P94874">
      <data key="relation">reference</data>
    </edge>
    <edge source="P154342" target="P105511">
      <data key="relation">reference</data>
    </edge>
    <edge source="P97372" target="P386">
      <data key="relation">reference</data>
    </edge>
    <edge source="P97372" target="P110641">
      <data key="relation">reference</data>
    </edge>
    <edge source="P139900" target="P386">
      <data key="relation">reference</data>
    </edge>
    <edge source="P386" target="P18630">
      <data key="relation">reference</data>
    </edge>
    <edge source="P386" target="P139095">
      <data key="relation">reference</data>
    </edge>
    <edge source="P386" target="P4215">
      <data key="relation">reference</data>
    </edge>
    <edge source="P386" target="P110641">
      <data key="relation">reference</data>
    </edge>
    <edge source="P386" target="P97441">
      <data key="relation">reference</data>
    </edge>
    <edge source="P386" target="P94874">
      <data key="relation">reference</data>
    </edge>
    <edge source="P386" target="P6146">
      <data key="relation">reference</data>
    </edge>
    <edge source="P386" target="P105511">
      <data key="relation">reference</data>
    </edge>
    <edge source="P386" target="P64706">
      <data key="relation">reference</data>
    </edge>
    <edge source="P386" target="P107735">
      <data key="relation">reference</data>
    </edge>
    <edge source="P139095" target="P6146">
      <data key="relation">reference</data>
    </edge>
    <edge source="P110641" target="P6146">
      <data key="relation">reference</data>
    </edge>
    <edge source="P110641" target="P105511">
      <data key="relation">reference</data>
    </edge>
    <edge source="P110641" target="P64706">
      <data key="relation">reference</data>
    </edge>
    <edge source="P64706" target="P4215">
      <data key="relation">reference</data>
    </edge>
    <edge source="P64706" target="P18630">
      <data key="relation">reference</data>
    </edge>
    <edge source="P18630" target="P4215">
      <data key="relation">reference</data>
    </edge>
  </graph>
</graphml>
