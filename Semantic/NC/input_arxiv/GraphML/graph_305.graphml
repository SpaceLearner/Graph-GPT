<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="relation" for="edge" attr.name="relation" attr.type="string" />
  <key id="abstract" for="node" attr.name="abstract" attr.type="string" />
  <key id="title" for="node" attr.name="title" attr.type="string" />
  <graph edgedefault="undirected">
    <node id="P31106">
      <data key="title">imbalanced sentiment classification enhanced with discourse marker</data>
      <data key="abstract">Imbalanced data commonly exists in real world, espacially in sentiment-related corpus, making it difficult to train a classifier to distinguish latent sentiment in text data. We observe that humans often express transitional emotion between two adjacent discourses with discourse markers like "but", "though", "while", etc, and the head discourse and the tail discourse 3 usually indicate opposite emotional tendencies. Based on this observation, we propose a novel plug-and-play method, which first samples discourses according to transitional discourse markers and then validates sentimental polarities with the help of a pretrained attention-based model. Our method increases sample diversity in the first place, can serve as a upstream preprocessing part in data augmentation. We conduct experiments on three public sentiment datasets, with several frequently used algorithms. Results show that our method is found to be consistently effective, even in highly imbalanced scenario, and easily be integrated with oversampling method to boost the performance on imbalanced sentiment classification.</data>
    </node>
    <node id="P48408">
      <data key="title">paraphrases as foreign languages in multilingual neural machine translation</data>
      <data key="abstract">Using paraphrases, the expression of the same semantic meaning in different words, to improve generalization and translation performance is often useful. However, prior works only explore the use of paraphrases at the word or phrase level, not at the sentence or document level. Unlike previous works, we use different translations of the whole training data that are consistent in structure as paraphrases at the corpus level. Our corpus contains parallel paraphrases in multiple languages from various sources. We treat paraphrases as foreign languages, tag source sentences with paraphrase labels, and train in the style of multilingual Neural Machine Translation (NMT). Experimental results indicate that adding paraphrases improves the rare word translation, increases entropy and diversity in lexical choice. Moreover, adding the source paraphrases improves translation performance more effectively than adding the target paraphrases. Combining both the source and the target paraphrases boosts performance further; combining paraphrases with multilingual data also helps but has mixed performance. We achieve a BLEU score of 57.2 for French-to-English translation, training on 24 paraphrases of the Bible, which is ~+27 above the WMT'14 baseline.</data>
    </node>
    <node id="P154342">
      <data key="title">sequence to sequence learning with neural networks</data>
      <data key="abstract">Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.</data>
    </node>
    <node id="P154130">
      <data key="title">strategies for language identification in code mixed low resource languages</data>
      <data key="abstract">In recent years, substantial work has been done on language tagging of code-mixed data, but most of them use large amounts of data to build their models. In this article, we present three strategies to build a word level language tagger for code-mixed data using very low resources. Each of them secured an accuracy higher than our baseline model, and the best performing system got an accuracy around 91%. Combining all, the ensemble system achieved an accuracy of around 92.6%.</data>
    </node>
    <node id="P115359">
      <data key="title">neural machine translation by jointly learning to align and translate</data>
      <data key="abstract">Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.</data>
    </node>
    <node id="P52537">
      <data key="title">rewe regressing word embeddings for regularization of neural machine translation systems</data>
      <data key="abstract">Regularization of neural machine translation is still a significant problem, especially in low-resource settings. To mollify this problem, we propose regressing word embeddings (ReWE) as a new regularization technique in a system that is jointly trained to predict the next word in the translation (categorical value) and its word embedding (continuous value). Such a joint training allows the proposed system to learn the distributional properties represented by the word embeddings, empirically improving the generalization to unseen sentences. Experiments over three translation datasets have showed a consistent improvement over a strong baseline, ranging between 0.91 and 2.54 BLEU points, and also a marked improvement over a state-of-the-art system.</data>
    </node>
    <node id="P41574">
      <data key="title">data augmentation by pairing samples for images classification</data>
      <data key="abstract">Data augmentation is a widely used technique in many machine learning tasks, such as image classification, to virtually enlarge the training dataset size and avoid overfitting. Traditional data augmentation techniques for image classification tasks create new samples from the original training data by, for example, flipping, distorting, adding a small amount of noise to, or cropping a patch from an original image. In this paper, we introduce a simple but surprisingly effective data augmentation technique for image classification tasks. With our technique, named SamplePairing, we synthesize a new sample from one image by overlaying another image randomly chosen from the training data (i.e., taking an average of two images for each pixel). By using two images randomly selected from the training set, we can generate N^2 new samples from N training samples. This simple data augmentation technique significantly improved classification accuracy for all the tested datasets; for example, the top-1 error rate was reduced from 33.5% to 29.0% for the ILSVRC 2012 dataset with GoogLeNet and from 8.22% to 6.93% in the CIFAR-10 dataset. We also show that our SamplePairing technique largely improved accuracy when the number of samples in the training set was very small. Therefore, our technique is more valuable for tasks with a limited amount of training data, such as medical imaging tasks.</data>
    </node>
    <node id="P47184">
      <data key="title">eda easy data augmentation techniques for boosting performance on text classification tasks</data>
      <data key="abstract">We present EDA: easy data augmentation techniques for boosting performance on text classification tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that EDA improves performance for both convolutional and recurrent neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with EDA while using only 50% of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use.</data>
    </node>
    <node id="P4475">
      <data key="title">on the impact of various types of noise on neural machine translation</data>
      <data key="abstract">We examine how various types of noise in the parallel training data impact the quality of neural machine translation systems. We create five types of artificial noise and analyze how they degrade performance in neural and statistical machine translation. We find that neural models are generally more harmed by noise than statistical models. For one especially egregious type of noise they learn to just copy the input sentence.</data>
    </node>
    <node id="P380">
      <data key="title">data augmentation for low resource neural machine translation</data>
      <data key="abstract">The quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in computer vision, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.</data>
    </node>
    <node id="P147249">
      <data key="title">transfer learning for low resource neural machine translation</data>
      <data key="abstract">The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages. We present a transfer learning method that significantly improves Bleu scores across a range of low-resource languages. Our key idea is to first train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our transfer learning method we improve baseline NMT models by an average of 5.6 Bleu on four low-resource language pairs. Ensembling and unknown word replacement add another 2 Bleu which brings the NMT performance on low-resource machine translation close to a strong syntax based machine translation (SBMT) system, exceeding its performance on one language pair. Additionally, using the transfer learning model for re-scoring, we can improve the SBMT system by an average of 1.3 Bleu, improving the state-of-the-art on low-resource machine translation.</data>
    </node>
    <node id="P9796">
      <data key="title">an axiomatic approach to regularizing neural ranking models</data>
      <data key="abstract">Axiomatic information retrieval (IR) seeks a set of principle properties desirable in IR models. These properties when formally expressed provide guidance in the search for better relevance estimation functions. Neural ranking models typically contain a large number of parameters. The training of these models involve a search for appropriate parameter values based on large quantities of labeled examples. Intuitively, axioms that can guide the search for better traditional IR models should also help in better parameter estimation for machine learning based rankers. This work explores the use of IR axioms to augment the direct supervision from labeled data for training neural ranking models. We modify the documents in our dataset along the lines of well-known axioms during training and add a regularization loss based on the agreement between the ranking model and the axioms on which version of the document---the original or the perturbed---should be preferred. Our experiments show that the neural ranking model achieves faster convergence and better generalization with axiomatic regularization.</data>
    </node>
    <node id="P29312">
      <data key="title">on the properties of neural machine translation encoder decoder approaches</data>
      <data key="abstract">Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.</data>
    </node>
    <node id="P100944">
      <data key="title">dynamic data selection for neural machine translation</data>
      <data key="abstract">Intelligent selection of training data has proven a successful technique to simultaneously increase training efficiency and translation performance for phrase-based machine translation (PBMT). With the recent increase in popularity of neural machine translation (NMT), we explore in this paper to what extent and how NMT can also benefit from data selection. While state-of-the-art data selection (Axelrod et al., 2011) consistently performs well for PBMT, we show that gains are substantially lower for NMT. Next, we introduce dynamic data selection for NMT, a method in which we vary the selected subset of training data between different training epochs. Our experiments show that the best results are achieved when applying a technique we call gradual fine-tuning, with improvements up to +2.6 BLEU over the original data selection approach and up to +3.1 BLEU over a general baseline.</data>
    </node>
    <node id="P7939">
      <data key="title">improving neural machine translation models with monolingual data</data>
      <data key="abstract">Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Target-side monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for NMT. In contrast to previous work, which combines NMT models with separately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolingual data without changing the neural network architecture. By pairing monolingual training data with an automatic back-translation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task English German (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 task Turkish-&gt;English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English-&gt;German.</data>
    </node>
    <node id="P622">
      <data key="title">data augmentation for neural online chat response selection</data>
      <data key="abstract">Data augmentation seeks to manipulate the available data for training to improve the generalization ability of models. We investigate two data augmentation proxies, permutation and flipping, for neural dialog response selection task on various models over multiple datasets, including both Chinese and English languages. Different from standard data augmentation techniques, our method combines the original and synthesized data for prediction. Empirical results show that our approach can gain 1 to 3 recall-at-1 points over baseline models in both full-scale and small-scale settings.</data>
    </node>
    <node id="P76991">
      <data key="title">neural machine translation of rare words with subword units</data>
      <data key="abstract">Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.</data>
    </node>
    <node id="P61654">
      <data key="title">data augmentation for spoken language understanding via joint variational generation</data>
      <data key="abstract">Data scarcity is one of the main obstacles of domain adaptation in spoken language understanding (SLU) due to the high cost of creating manually tagged SLU datasets. Recent works in neural text generative models, particularly latent variable models such as variational autoencoder (VAE), have shown promising results in regards to generating plausible and natural sentences. In this paper, we propose a novel generative architecture which leverages the generative power of latent variable models to jointly synthesize fully annotated utterances. Our experiments show that existing SLU models trained on the additional synthetic examples achieve performance gains. Our approach not only helps alleviate the data scarcity issue in the SLU task for many datasets but also indiscriminately improves language understanding performances for various SLU models, supported by extensive experiments and rigorous statistical testing.</data>
    </node>
    <node id="P30510">
      <data key="title">return of the devil in the details delving deep into convolutional nets</data>
      <data key="abstract">The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source code and models to reproduce the experiments in the paper is made publicly available.</data>
    </node>
    <node id="P70857">
      <data key="title">token level and sequence level loss smoothing for rnn language models</data>
      <data key="abstract">Despite the effectiveness of recurrent neural network language models, their maximum likelihood estimation suffers from two limitations. It treats all sentences that do not match the ground truth as equally poor, ignoring the structure of the output space. Second, it suffers from "exposure bias": during training tokens are predicted given ground-truth sequences, while at test time prediction is conditioned on generated output sequences. To overcome these limitations we build upon the recent reward augmented maximum likelihood approach \ie sequence-level smoothing that encourages the model to predict sentences close to the ground truth according to a given performance metric. We extend this approach to token-level loss smoothing, and propose improvements to the sequence-level smoothing approach. Our experiments on two different tasks, image captioning and machine translation, show that token-level and sequence-level loss smoothing are complementary, and significantly improve results.</data>
    </node>
    <edge source="P31106" target="P380">
      <data key="relation">reference</data>
    </edge>
    <edge source="P48408" target="P76991">
      <data key="relation">reference</data>
    </edge>
    <edge source="P48408" target="P380">
      <data key="relation">reference</data>
    </edge>
    <edge source="P154342" target="P76991">
      <data key="relation">reference</data>
    </edge>
    <edge source="P154342" target="P115359">
      <data key="relation">reference</data>
    </edge>
    <edge source="P154342" target="P29312">
      <data key="relation">reference</data>
    </edge>
    <edge source="P154342" target="P7939">
      <data key="relation">reference</data>
    </edge>
    <edge source="P154342" target="P147249">
      <data key="relation">reference</data>
    </edge>
    <edge source="P154342" target="P380">
      <data key="relation">reference</data>
    </edge>
    <edge source="P154342" target="P100944">
      <data key="relation">reference</data>
    </edge>
    <edge source="P154342" target="P70857">
      <data key="relation">reference</data>
    </edge>
    <edge source="P154342" target="P622">
      <data key="relation">reference</data>
    </edge>
    <edge source="P154342" target="P52537">
      <data key="relation">reference</data>
    </edge>
    <edge source="P154130" target="P380">
      <data key="relation">reference</data>
    </edge>
    <edge source="P115359" target="P76991">
      <data key="relation">reference</data>
    </edge>
    <edge source="P115359" target="P29312">
      <data key="relation">reference</data>
    </edge>
    <edge source="P115359" target="P7939">
      <data key="relation">reference</data>
    </edge>
    <edge source="P115359" target="P147249">
      <data key="relation">reference</data>
    </edge>
    <edge source="P115359" target="P380">
      <data key="relation">reference</data>
    </edge>
    <edge source="P115359" target="P100944">
      <data key="relation">reference</data>
    </edge>
    <edge source="P115359" target="P70857">
      <data key="relation">reference</data>
    </edge>
    <edge source="P115359" target="P4475">
      <data key="relation">reference</data>
    </edge>
    <edge source="P115359" target="P52537">
      <data key="relation">reference</data>
    </edge>
    <edge source="P52537" target="P76991">
      <data key="relation">reference</data>
    </edge>
    <edge source="P52537" target="P380">
      <data key="relation">reference</data>
    </edge>
    <edge source="P52537" target="P70857">
      <data key="relation">reference</data>
    </edge>
    <edge source="P41574" target="P380">
      <data key="relation">reference</data>
    </edge>
    <edge source="P47184" target="P7939">
      <data key="relation">reference</data>
    </edge>
    <edge source="P47184" target="P380">
      <data key="relation">reference</data>
    </edge>
    <edge source="P4475" target="P76991">
      <data key="relation">reference</data>
    </edge>
    <edge source="P4475" target="P7939">
      <data key="relation">reference</data>
    </edge>
    <edge source="P4475" target="P380">
      <data key="relation">reference</data>
    </edge>
    <edge source="P380" target="P76991">
      <data key="relation">reference</data>
    </edge>
    <edge source="P380" target="P30510">
      <data key="relation">reference</data>
    </edge>
    <edge source="P380" target="P29312">
      <data key="relation">reference</data>
    </edge>
    <edge source="P380" target="P7939">
      <data key="relation">reference</data>
    </edge>
    <edge source="P380" target="P147249">
      <data key="relation">reference</data>
    </edge>
    <edge source="P380" target="P100944">
      <data key="relation">reference</data>
    </edge>
    <edge source="P380" target="P70857">
      <data key="relation">reference</data>
    </edge>
    <edge source="P380" target="P622">
      <data key="relation">reference</data>
    </edge>
    <edge source="P380" target="P61654">
      <data key="relation">reference</data>
    </edge>
    <edge source="P380" target="P9796">
      <data key="relation">reference</data>
    </edge>
    <edge source="P147249" target="P100944">
      <data key="relation">reference</data>
    </edge>
    <edge source="P100944" target="P76991">
      <data key="relation">reference</data>
    </edge>
    <edge source="P100944" target="P7939">
      <data key="relation">reference</data>
    </edge>
    <edge source="P7939" target="P76991">
      <data key="relation">reference</data>
    </edge>
  </graph>
</graphml>
