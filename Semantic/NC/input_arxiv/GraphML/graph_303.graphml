<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="relation" for="edge" attr.name="relation" attr.type="string" />
  <key id="abstract" for="node" attr.name="abstract" attr.type="string" />
  <key id="title" for="node" attr.name="title" attr.type="string" />
  <graph edgedefault="undirected">
    <node id="P71636">
      <data key="title">adadelta an adaptive learning rate method</data>
      <data key="abstract">We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.</data>
    </node>
    <node id="P56879">
      <data key="title">a large scale empirical study of geotagging behavior on twitter</data>
      <data key="abstract">Geotagging on social media has become an important proxy for understanding people's mobility and social events. Research that uses geotags to infer public opinions relies on several key assumptions about the behavior of geotagged and non-geotagged users. However, these assumptions have not been fully validated. Lack of understanding the geotagging behavior prohibits people further utilizing it. In this paper, we present an empirical study of geotagging behavior on Twitter based on more than 40 billion tweets collected from 20 million users. There are three main findings that may challenge these common assumptions. Firstly, different groups of users have different geotagging preferences. For example, less than 3% of users speaking in Korean are geotagged, while more than 40% of users speaking in Indonesian use geotags. Secondly, users who report their locations in profiles are more likely to use geotags, which may affects the generability of those location prediction systems on non-geotagged users. Thirdly, strong homophily effect exists in users' geotagging behavior, that users tend to connect to friends with similar geotagging preferences.</data>
    </node>
    <node id="P47792">
      <data key="title">beyond temporal pooling recurrence and temporal convolutions for gesture recognition in video</data>
      <data key="abstract">Recent studies have demonstrated the power of recurrent neural networks for machine translation, image captioning and speech recognition. For the task of capturing temporal structure in video, however, there still remain numerous open research questions. Current research suggests using a simple temporal feature pooling strategy to take into account the temporal aspect of video. We demonstrate that this method is not sufficient for gesture recognition, where temporal information is more discriminative compared to general video classification tasks. We explore deep architectures for gesture recognition in video and propose a new end-to-end trainable neural network architecture incorporating temporal convolutions and bidirectional recurrence. Our main contributions are twofold; first, we show that recurrence is crucial for this task; second, we show that adding temporal convolutions leads to significant improvements. We evaluate the different approaches on the Montalbano gesture recognition dataset, where we achieve state-of-the-art results.</data>
    </node>
    <node id="P35756">
      <data key="title">building end to end dialogue systems using generative hierarchical neural network models</data>
      <data key="abstract">We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings.</data>
    </node>
    <node id="P137543">
      <data key="title">predicting deep zero shot convolutional neural networks using textual descriptions</data>
      <data key="abstract">One of the main challenges in Zero-Shot Learning of visual categories is gathering semantic attributes to accompany images. Recent work has shown that learning from textual descriptions, such as Wikipedia articles, avoids the problem of having to explicitly define these attributes. We present a new model that can classify unseen categories from their textual description. Specifically, we use text features to predict the output weights of both the convolutional and the fully connected layers in a deep convolutional neural network (CNN). We take advantage of the architecture of CNNs and learn features at different layers, rather than just learning an embedding space for both modalities, as is common with existing approaches. The proposed model also allows us to automatically generate a list of pseudo- attributes for each visual category consisting of words from Wikipedia articles. We train our models end-to-end us- ing the Caltech-UCSD bird and flower datasets and evaluate both ROC and Precision-Recall curves. Our empirical results show that the proposed model significantly outperforms previous methods.</data>
    </node>
    <node id="P74472">
      <data key="title">where in the world are you geolocation and language identification in twitter</data>
      <data key="abstract">The movements of ideas and content between locations and languages are unquestionably crucial concerns to researchers of the information age, and Twitter has emerged as a central, global platform on which hundreds of millions of people share knowledge and information. A variety of research has attempted to harvest locational and linguistic metadata from tweets to understand important questions related to the 300 million tweets that flow through the platform each day. Much of this work is carried out with only limited understandings of how best to work with the spatial and linguistic contexts in which the information was produced, however. Furthermore, standard, well-accepted practices have yet to emerge. As such, this article studies the reliability of key methods used to determine language and location of content in Twitter. It compares three automated language identification packages to Twitter's user interface language setting and to a human coding of languages to identify common sources of disagreemen...</data>
    </node>
    <node id="P114159">
      <data key="title">a probabilistic framework for location inference from social media</data>
      <data key="abstract">We study the extent to which we can infer users' geographical locations from social media. Location inference from social media can benefit many applications, such as disaster management, targeted advertising, and news content tailoring. In recent years, a number of algorithms have been proposed for identifying user locations on social media platforms such as Twitter and Facebook from message contents, friend networks, and interactions between users. In this paper, we propose a novel probabilistic model based on factor graphs for location inference that offers several unique advantages for this task. First, the model generalizes previous methods by incorporating content, network, and deep features learned from social context. The model is also flexible enough to support both supervised learning and semi-supervised learning. Second, we explore several learning algorithms for the proposed model, and present a Two-chain Metropolis-Hastings (MH+) algorithm, which improves the inference accuracy. Third, we validate the proposed model on three different genres of data - Twitter, Weibo, and Facebook - and demonstrate that the proposed model can substantially improve the inference accuracy (+3.3-18.5% by F1-score) over that of several state-of-the-art methods.</data>
    </node>
    <node id="P29089">
      <data key="title">discover your social identity from what you tweet a content based approach</data>
      <data key="abstract">The increasing popularity of social media promotes the proliferation of fake news, which has caused significant negative societal effects. Therefore, fake news detection on social media has recently become an emerging research area of great concern. With the development of multimedia technology, fake news attempts to utilize multimedia content with images or videos to attract and mislead consumers for rapid dissemination, which makes visual content an important part of fake news. Despite the importance of visual content, our understanding of the role of visual content in fake news detection is still limited. This chapter presents a comprehensive review of the visual content in fake news, including the basic concepts, effective visual features, representative detection methods and challenging issues of multimedia fake news detection. This chapter can help readers to understand the role of visual content in fake news detection, and effectively utilize visual content to assist in detecting multimedia fake news.</data>
    </node>
    <node id="P70326">
      <data key="title">embed to control a locally linear latent dynamics model for control from raw images</data>
      <data key="abstract">We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.</data>
    </node>
    <node id="P110641">
      <data key="title">convolutional neural networks for sentence classification</data>
      <data key="abstract">We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.</data>
    </node>
    <node id="P169210">
      <data key="title">location order recovery in trails with low temporal resolution</data>
      <data key="abstract">Researchers who study object movement problems related to topics like traffic flow analysis, patient monitoring, and software operation, need to know the correct order in which objects move. Here, we use the term trail to refer to a series of movements by an object. This paper introduces a new missing data problem that occurs when analyzing trails where there is inadequate temporal resolution on the events. The temporal resolution is inadequate when an object, which can only be in one place at one time, appears in the data to be in two or more locations at once. We refer to this lack of resolution as a broken point. Broken points prevent us from knowing the correct order of movement. We propose a three-phase framework for recovering the location order. Based on the Markov transition network, we are able to find the route with the highest probability. Our results show that this framework can efficiently find the correct location order in trails with low temporal resolution. We also demonstrate that by correcting the location order, the criticality of locations can change significantly.</data>
    </node>
    <node id="P375">
      <data key="title">on predicting geolocation of tweets using convolutional neural networks</data>
      <data key="abstract">In many Twitter studies, it is important to know where a tweet came from in order to use the tweet content to study regional user behavior. However, researchers using Twitter to understand user behavior often lack sufficient geo-tagged data. Given the huge volume of Twitter data there is a need for accurate automated geolocating solutions. Herein, we present a new method to predict a Twitter userâ€™s location based on the information in a single tweet. We integrate text and user profile meta-data into a single model using a convolutional neural network. Our experiments demonstrate that our neural model substantially outperforms baseline methods, achieving 52.8% accuracy and 92.1% accuracy on city-level and country-level prediction respectively.</data>
    </node>
    <node id="P54474">
      <data key="title">the ubuntu dialogue corpus a large dataset for research in unstructured multi turn dialogue systems</data>
      <data key="abstract">This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter. We also describe two neural learning architectures suitable for analyzing this dataset, and provide benchmark performance on the task of selecting the best next response.</data>
    </node>
    <node id="P106699">
      <data key="title">distributed representations of words and phrases and their compositionality</data>
      <data key="abstract">The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.</data>
    </node>
    <node id="P26477">
      <data key="title">incremental lstm based dialog state tracker</data>
      <data key="abstract">A dialog state tracker is an important component in modern spoken dialog systems. We present an incremental dialog state tracker, based on LSTM networks. It directly uses automatic speech recognition hypotheses to track the state. We also present the key non-standard aspects of the model that bring its performance close to the state-of-the-art and experimentally analyze their contribution: including the ASR confidence scores, abstracting scarcely represented values, including transcriptions in the training data, and model averaging.</data>
    </node>
    <node id="P157313">
      <data key="title">parameterized convolutional neural networks for aspect level sentiment classification</data>
      <data key="abstract">We introduce a novel parameterized convolutional neural network for aspect level sentiment classification. Using parameterized filters and parameterized gates, we incorporate aspect information into convolutional neural networks (CNN). Experiments demonstrate that our parameterized filters and parameterized gates effectively capture the aspect-specific features, and our CNN-based models achieve excellent results on SemEval 2014 datasets.</data>
    </node>
    <node id="P48315">
      <data key="title">social media news and political information during the us election was polarizing content concentrated in swing states</data>
      <data key="abstract">US voters shared large volumes of polarizing political news and information in the form of links to content from Russian, WikiLeaks and junk news sources. Was this low quality political information distributed evenly around the country, or concentrated in swing states and particular parts of the country? In this data memo we apply a tested dictionary of sources about political news and information being shared over Twitter over a ten day period around the 2016 Presidential Election. Using self-reported location information, we place a third of users by state and create a simple index for the distribution of polarizing content around the country. We find that (1) nationally, Twitter users got more misinformation, polarizing and conspiratorial content than professionally produced news. (2) Users in some states, however, shared more polarizing political news and information than users in other states. (3) Average levels of misinformation were higher in swing states than in uncontested states, even when weighted for the relative size of the user population in each state. We conclude with some observations about the impact of strategically disseminated polarizing information on public life.</data>
    </node>
    <node id="P85882">
      <data key="title">a hierarchical location prediction neural network for twitter user geolocation</data>
      <data key="abstract">Accurate estimation of user location is important for many online services. Previous neural network based methods largely ignore the hierarchical structure among locations. In this paper, we propose a hierarchical location prediction neural network for Twitter user geolocation. Our model first predicts the home country for a user, then uses the country result to guide the city-level prediction. In addition, we employ a character-aware word embedding layer to overcome the noisy information in tweets. With the feature fusion layer, our model can accommodate various feature combinations and achieves state-of-the-art results over three commonly used benchmarks under different feature settings. It not only improves the prediction accuracy but also greatly reduces the mean error distance.</data>
    </node>
    <node id="P113001">
      <data key="title">aspect level sentiment classification with attention over attention neural networks</data>
      <data key="abstract">Aspect-level sentiment classification aims to identify the sentiment expressed towards some aspects given context sentences. In this paper, we introduce an attention-over-attention (AOA) neural network for aspect level sentiment classification. Our approach models aspects and sentences in a joint way and explicitly captures the interaction between aspects and context sentences. With the AOA module, our model jointly learns the representations for aspects and sentences, and automatically focuses on the important parts in sentences. Our experiments on laptop and restaurant datasets demonstrate our approach outperforms previous LSTM-based architectures.</data>
    </node>
    <node id="P1353">
      <data key="title">adam a method for stochastic optimization</data>
      <data key="abstract">We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.</data>
    </node>
    <edge source="P71636" target="P54474">
      <data key="relation">reference</data>
    </edge>
    <edge source="P71636" target="P1353">
      <data key="relation">reference</data>
    </edge>
    <edge source="P71636" target="P110641">
      <data key="relation">reference</data>
    </edge>
    <edge source="P56879" target="P74472">
      <data key="relation">reference</data>
    </edge>
    <edge source="P56879" target="P114159">
      <data key="relation">reference</data>
    </edge>
    <edge source="P56879" target="P375">
      <data key="relation">reference</data>
    </edge>
    <edge source="P56879" target="P113001">
      <data key="relation">reference</data>
    </edge>
    <edge source="P56879" target="P157313">
      <data key="relation">reference</data>
    </edge>
    <edge source="P56879" target="P169210">
      <data key="relation">reference</data>
    </edge>
    <edge source="P47792" target="P1353">
      <data key="relation">reference</data>
    </edge>
    <edge source="P35756" target="P1353">
      <data key="relation">reference</data>
    </edge>
    <edge source="P35756" target="P106699">
      <data key="relation">reference</data>
    </edge>
    <edge source="P137543" target="P1353">
      <data key="relation">reference</data>
    </edge>
    <edge source="P137543" target="P106699">
      <data key="relation">reference</data>
    </edge>
    <edge source="P74472" target="P375">
      <data key="relation">reference</data>
    </edge>
    <edge source="P74472" target="P85882">
      <data key="relation">reference</data>
    </edge>
    <edge source="P114159" target="P1353">
      <data key="relation">reference</data>
    </edge>
    <edge source="P114159" target="P375">
      <data key="relation">reference</data>
    </edge>
    <edge source="P114159" target="P169210">
      <data key="relation">reference</data>
    </edge>
    <edge source="P114159" target="P85882">
      <data key="relation">reference</data>
    </edge>
    <edge source="P29089" target="P1353">
      <data key="relation">reference</data>
    </edge>
    <edge source="P29089" target="P375">
      <data key="relation">reference</data>
    </edge>
    <edge source="P29089" target="P110641">
      <data key="relation">reference</data>
    </edge>
    <edge source="P29089" target="P85882">
      <data key="relation">reference</data>
    </edge>
    <edge source="P70326" target="P1353">
      <data key="relation">reference</data>
    </edge>
    <edge source="P110641" target="P375">
      <data key="relation">reference</data>
    </edge>
    <edge source="P110641" target="P113001">
      <data key="relation">reference</data>
    </edge>
    <edge source="P110641" target="P106699">
      <data key="relation">reference</data>
    </edge>
    <edge source="P110641" target="P157313">
      <data key="relation">reference</data>
    </edge>
    <edge source="P169210" target="P375">
      <data key="relation">reference</data>
    </edge>
    <edge source="P169210" target="P85882">
      <data key="relation">reference</data>
    </edge>
    <edge source="P375" target="P1353">
      <data key="relation">reference</data>
    </edge>
    <edge source="P375" target="P106699">
      <data key="relation">reference</data>
    </edge>
    <edge source="P375" target="P48315">
      <data key="relation">reference</data>
    </edge>
    <edge source="P375" target="P113001">
      <data key="relation">reference</data>
    </edge>
    <edge source="P375" target="P157313">
      <data key="relation">reference</data>
    </edge>
    <edge source="P375" target="P85882">
      <data key="relation">reference</data>
    </edge>
    <edge source="P54474" target="P1353">
      <data key="relation">reference</data>
    </edge>
    <edge source="P26477" target="P1353">
      <data key="relation">reference</data>
    </edge>
    <edge source="P157313" target="P1353">
      <data key="relation">reference</data>
    </edge>
    <edge source="P157313" target="P113001">
      <data key="relation">reference</data>
    </edge>
    <edge source="P85882" target="P1353">
      <data key="relation">reference</data>
    </edge>
    <edge source="P113001" target="P1353">
      <data key="relation">reference</data>
    </edge>
  </graph>
</graphml>
