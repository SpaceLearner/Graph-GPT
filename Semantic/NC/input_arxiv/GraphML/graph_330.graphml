<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="relation" for="edge" attr.name="relation" attr.type="string" />
  <key id="abstract" for="node" attr.name="abstract" attr.type="string" />
  <key id="title" for="node" attr.name="title" attr.type="string" />
  <graph edgedefault="undirected">
    <node id="P10706">
      <data key="title">unsupervised cross domain image generation</data>
      <data key="abstract">We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given function f, which accepts inputs in either domains, would remain unchanged. Other than the function f, the training data is unsupervised and consist of a set of samples from each domain. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f-constancy component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.</data>
    </node>
    <node id="P144635">
      <data key="title">learning to generate reviews and discovering sentiment</data>
      <data key="abstract">We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.</data>
    </node>
    <node id="P9459">
      <data key="title">style transfer from non parallel text by cross alignment</data>
      <data key="abstract">This paper focuses on style transfer on the basis of non-parallel text. This is an instance of a broad family of problems including machine translation, decipherment, and sentiment modification. The key challenge is to separate the content from other aspects such as style. We assume a shared latent content distribution across different text corpora, and propose a method that leverages refined alignment of latent representations to perform style transfer. The transferred sentences from one style should match example sentences from the other style as a population. We demonstrate the effectiveness of this cross-alignment method on three tasks: sentiment modification, decipherment of word substitution ciphers, and recovery of word order.</data>
    </node>
    <node id="P408">
      <data key="title">toward controlled generation of text</data>
      <data key="abstract">Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.</data>
    </node>
    <node id="P115359">
      <data key="title">neural machine translation by jointly learning to align and translate</data>
      <data key="abstract">Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.</data>
    </node>
    <node id="P6796">
      <data key="title">emotional chatting machine emotional conversation generation with internal and external memory</data>
      <data key="abstract">Perception and expression of emotion are key factors to the success of dialogue systems or conversational agents. However, this problem has not been studied in large-scale conversation generation so far. In this paper, we propose Emotional Chatting Machine (ECM) that can generate appropriate responses not only in content (relevant and grammatical) but also in emotion (emotionally consistent). To the best of our knowledge, this is the first work that addresses the emotion factor in large-scale conversation generation. ECM addresses the factor using three new mechanisms that respectively (1) models the high-level abstraction of emotion expressions by embedding emotion categories, (2) captures the change of implicit internal emotion states, and (3) uses explicit emotion expressions with an external emotion vocabulary. Experiments show that the proposed model can generate responses appropriate not only in content but also in emotion.</data>
    </node>
    <node id="P22838">
      <data key="title">seqgan sequence generative adversarial nets with policy gradient</data>
      <data key="abstract">As a new way of training generative models, Generative Adversarial Nets (GAN) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real-valued data. However, it has limitations when the goal is for generating sequences of discrete tokens. A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model. Also, the discriminative model can only assess a complete sequence, while for a partially generated sequence, it is non-trivial to balance its current score and the future one once the entire sequence has been generated. In this paper, we propose a sequence generation framework, called SeqGAN, to solve the problems. Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update. The RL reward signal comes from the GAN discriminator judged on a complete sequence, and is passed back to the intermediate state-action steps using Monte Carlo search. Extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over strong baselines.</data>
    </node>
    <node id="P44028">
      <data key="title">generating sentences from a continuous space</data>
      <data key="abstract">The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.</data>
    </node>
    <node id="P35983">
      <data key="title">unsupervised representation learning with deep convolutional generative adversarial networks</data>
      <data key="abstract">In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.</data>
    </node>
    <node id="P72023">
      <data key="title">adversarial generation of natural language</data>
      <data key="abstract">Generative Adversarial Networks (GANs) have gathered a lot of attention from the computer vision community, yielding impressive results for image generation. Advances in the adversarial generation of natural language from noise however are not commensurate with the progress made in generating images, and still lag far behind likelihood based methods. In this paper, we take a step towards generating natural language with a GAN objective alone. We introduce a simple baseline that addresses the discrete output space problem without relying on gradient estimators and show that it is able to achieve state-of-the-art results on a Chinese poem generation dataset. We present quantitative results on generating sentences from context-free and probabilistic context-free grammars, and qualitative language modeling results. A conditional version is also described that can generate sequences conditioned on sentence characteristics.</data>
    </node>
    <node id="P152461">
      <data key="title">infogan interpretable representation learning by information maximizing generative adversarial nets</data>
      <data key="abstract">This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.</data>
    </node>
    <node id="P20148">
      <data key="title">harnessing deep neural networks with logic rules</data>
      <data key="abstract">Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.</data>
    </node>
    <node id="P73361">
      <data key="title">on unifying deep generative models</data>
      <data key="abstract">Deep generative models have achieved impressive success in recent years. Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as emerging families for generative model learning, have largely been considered as two distinct paradigms and received extensive independent studies respectively. This paper aims to establish formal connections between GANs and VAEs through a new formulation of them. We interpret sample generation in GANs as performing posterior inference, and show that GANs and VAEs involve minimizing KL divergences of respective posterior and inference distributions with opposite directions, extending the two learning phases of classic wake-sleep algorithm, respectively. The unified view provides a powerful tool to analyze a diverse set of existing model variants, and enables to transfer techniques across research lines in a principled way. For example, we apply the importance weighting method in VAE literatures for improved GAN learning, and enhance VAEs with an adversarial mechanism that leverages generated samples. Experiments show generality and effectiveness of the transferred techniques.</data>
    </node>
    <node id="P127175">
      <data key="title">autoencoding beyond pixels using a learned similarity metric</data>
      <data key="abstract">We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder with a generative adversarial network we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic.</data>
    </node>
    <node id="P65228">
      <data key="title">generating images with perceptual similarity metrics based on deep networks</data>
      <data key="abstract">Image-generating machine learning models are typically trained with loss functions based on distance in the image space. This often leads to over-smoothed results. We propose a class of loss functions, which we call deep perceptual similarity metrics (DeePSiM), that mitigate this problem. Instead of computing distances in the image space, we compute distances between image features extracted by deep neural networks. This metric better reflects perceptually similarity of images and thus leads to better results. We show three applications: autoencoder training, a modification of a variational autoencoder, and inversion of deep convolutional networks. In all cases, the generated images look sharp and resemble natural images.</data>
    </node>
    <node id="P69420">
      <data key="title">a deep network with visual text composition behavior</data>
      <data key="abstract">While natural languages are compositional, how state-of-the-art neural models achieve compositionality is still unclear. We propose a deep network, which not only achieves competitive accuracy for text classification, but also exhibits compositional behavior. That is, while creating hierarchical representations of a piece of text, such as a sentence, the lower layers of the network distribute their layer-specific attention weights to individual words. In contrast, the higher layers compose meaningful phrases and clauses, whose lengths increase as the networks get deeper until fully composing the sentence.</data>
    </node>
    <node id="P160327">
      <data key="title">training deep neural networks on noisy labels with bootstrapping</data>
      <data key="abstract">Current state-of-the-art deep learning systems for visual object recognition and detection use purely supervised training with regularization such as dropout to avoid overfitting. The performance depends critically on the amount of labeled examples, and in current practice the labels are assumed to be unambiguous and accurate. However, this assumption often does not hold; e.g. in recognition, class labels may be missing; in detection, objects in the image may not be localized; and in general, the labeling may be subjective. In this work we propose a generic way to handle noisy and incomplete labeling by augmenting the prediction objective with a notion of consistency. We consider a prediction consistent if the same prediction is made given similar percepts, where the notion of similarity is between deep network features computed from the input data. In experiments we demonstrate that our approach yields substantial robustness to label noise on several datasets. On MNIST handwritten digits, we show that our model is robust to label corruption. On the Toronto Face Database, we show that our model handles well the case of subjective labels in emotion recognition, achieving state-of-the- art results, and can also benefit from unlabeled face images with no modification to our method. On the ILSVRC2014 detection challenge data, we show that our approach extends to very deep networks, high resolution images and structured outputs, and results in improved scalable detection.</data>
    </node>
    <node id="P68021">
      <data key="title">improved variational autoencoders for text modeling using dilated convolutions</data>
      <data key="abstract">Recent work on generative modeling of text has found that variational auto-encoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder's dilation architecture, we control the effective context from previously generated words. In experiments, we find that there is a trade off between the contextual capacity of the decoder and the amount of encoding information used. We show that with the right decoder, VAE can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive experimental result on the use VAE for generative modeling of text. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.</data>
    </node>
    <node id="P30486">
      <data key="title">a neural algorithm of artistic style</data>
      <data key="abstract">In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.</data>
    </node>
    <node id="P1353">
      <data key="title">adam a method for stochastic optimization</data>
      <data key="abstract">We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.</data>
    </node>
    <edge source="P10706" target="P1353">
      <data key="relation">reference</data>
    </edge>
    <edge source="P10706" target="P35983">
      <data key="relation">reference</data>
    </edge>
    <edge source="P10706" target="P65228">
      <data key="relation">reference</data>
    </edge>
    <edge source="P10706" target="P9459">
      <data key="relation">reference</data>
    </edge>
    <edge source="P10706" target="P408">
      <data key="relation">reference</data>
    </edge>
    <edge source="P144635" target="P1353">
      <data key="relation">reference</data>
    </edge>
    <edge source="P144635" target="P408">
      <data key="relation">reference</data>
    </edge>
    <edge source="P9459" target="P22838">
      <data key="relation">reference</data>
    </edge>
    <edge source="P9459" target="P408">
      <data key="relation">reference</data>
    </edge>
    <edge source="P408" target="P68021">
      <data key="relation">reference</data>
    </edge>
    <edge source="P408" target="P6796">
      <data key="relation">reference</data>
    </edge>
    <edge source="P408" target="P72023">
      <data key="relation">reference</data>
    </edge>
    <edge source="P408" target="P73361">
      <data key="relation">reference</data>
    </edge>
    <edge source="P408" target="P69420">
      <data key="relation">reference</data>
    </edge>
    <edge source="P408" target="P1353">
      <data key="relation">reference</data>
    </edge>
    <edge source="P408" target="P30486">
      <data key="relation">reference</data>
    </edge>
    <edge source="P408" target="P160327">
      <data key="relation">reference</data>
    </edge>
    <edge source="P408" target="P115359">
      <data key="relation">reference</data>
    </edge>
    <edge source="P408" target="P35983">
      <data key="relation">reference</data>
    </edge>
    <edge source="P408" target="P127175">
      <data key="relation">reference</data>
    </edge>
    <edge source="P408" target="P44028">
      <data key="relation">reference</data>
    </edge>
    <edge source="P408" target="P65228">
      <data key="relation">reference</data>
    </edge>
    <edge source="P408" target="P20148">
      <data key="relation">reference</data>
    </edge>
    <edge source="P408" target="P152461">
      <data key="relation">reference</data>
    </edge>
    <edge source="P408" target="P22838">
      <data key="relation">reference</data>
    </edge>
    <edge source="P115359" target="P44028">
      <data key="relation">reference</data>
    </edge>
    <edge source="P115359" target="P20148">
      <data key="relation">reference</data>
    </edge>
    <edge source="P115359" target="P22838">
      <data key="relation">reference</data>
    </edge>
    <edge source="P115359" target="P6796">
      <data key="relation">reference</data>
    </edge>
    <edge source="P115359" target="P72023">
      <data key="relation">reference</data>
    </edge>
    <edge source="P115359" target="P69420">
      <data key="relation">reference</data>
    </edge>
    <edge source="P22838" target="P1353">
      <data key="relation">reference</data>
    </edge>
    <edge source="P22838" target="P72023">
      <data key="relation">reference</data>
    </edge>
    <edge source="P44028" target="P68021">
      <data key="relation">reference</data>
    </edge>
    <edge source="P35983" target="P1353">
      <data key="relation">reference</data>
    </edge>
    <edge source="P35983" target="P127175">
      <data key="relation">reference</data>
    </edge>
    <edge source="P35983" target="P152461">
      <data key="relation">reference</data>
    </edge>
    <edge source="P35983" target="P72023">
      <data key="relation">reference</data>
    </edge>
    <edge source="P35983" target="P73361">
      <data key="relation">reference</data>
    </edge>
    <edge source="P72023" target="P1353">
      <data key="relation">reference</data>
    </edge>
    <edge source="P152461" target="P1353">
      <data key="relation">reference</data>
    </edge>
    <edge source="P152461" target="P73361">
      <data key="relation">reference</data>
    </edge>
    <edge source="P20148" target="P73361">
      <data key="relation">reference</data>
    </edge>
    <edge source="P73361" target="P68021">
      <data key="relation">reference</data>
    </edge>
    <edge source="P73361" target="P127175">
      <data key="relation">reference</data>
    </edge>
    <edge source="P127175" target="P30486">
      <data key="relation">reference</data>
    </edge>
    <edge source="P68021" target="P1353">
      <data key="relation">reference</data>
    </edge>
  </graph>
</graphml>
