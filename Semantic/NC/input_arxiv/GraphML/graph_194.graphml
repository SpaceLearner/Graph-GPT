<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="relation" for="edge" attr.name="relation" attr.type="string" />
  <key id="abstract" for="node" attr.name="abstract" attr.type="string" />
  <key id="title" for="node" attr.name="title" attr.type="string" />
  <graph edgedefault="undirected">
    <node id="P146392">
      <data key="title">on using retrained and incremental machine learning for modeling performance of adaptable software an empirical comparison</data>
      <data key="abstract">Given the ever-increasing complexity of adaptable software systems and their commonly hidden internal information (e.g., software runs in the public cloud), machine learning based performance modeling has gained momentum for evaluating, understanding and predicting software performance, which facilitates better informed self-adaptations. As performance data accumulates during the run of the software, updating the performance models becomes necessary. To this end, there are two conventional modeling methods: the retrained modeling that always discard the old model and retrain a new one using all available data; or the incremental modeling that retains the existing model and tunes it using one newly arrival data sample. Generally, literature on machine learning based performance modeling for adaptable software chooses either of those methods according to a general belief, but they provide insufficient evidences or references to justify their choice. This paper is the first to report on a comprehensive empirical study that examines both modeling methods under distinct domains of adaptable software, 5 performance indicators, 8 learning algorithms and settings, covering a total of 1,360 different conditions. Our findings challenge the general belief, which is shown to be only partially correct, and reveal some of the important, statistically significant factors that are often overlooked in existing work, providing evidence-based insights on the choice.</data>
    </node>
    <node id="P12226">
      <data key="title">about being the tortoise or the hare a position paper on making cloud applications too fast and furious for attackers</data>
      <data key="abstract">Cloud applications expose - beside service endpoints - also potential or actual vulnerabilities. And attackers have several advantages on their side. They can select the weapons, the point of time and the point of attack. Very often cloud application security engineering efforts focus to harden the fortress walls but seldom assume that attacks may be successful. So, cloud applications rely on their defensive walls but seldom attack intruders actively. Biological systems are different. They accept that defensive "walls" can be breached at several layers and therefore make use of an active and adaptive defense system to attack potential intruders - an immune system. This position paper proposes such an immune system inspired approach to ensure that even undetected intruders can be purged out of cloud applications. This makes it much harder for intruders to maintain a presence on victim systems. Evaluation experiments with popular cloud service infrastructures (Amazon Web Services, Google Compute Engine, Azure and OpenStack) showed that this could minimize the undetected acting period of intruders down to minutes.</data>
    </node>
    <node id="P80911">
      <data key="title">experiences of using a hybrid cloud to construct an environmental virtual observatory</data>
      <data key="abstract">Environmental science is often fragmented: data is collected using mismatched formats and conventions, and models are misaligned and run in isolation. Cloud computing offers a lot of potential in the way of resolving such issues by supporting data from different sources and at various scales, by facilitating the integration of models to create more sophisticated software services, and by providing a sustainable source of suitable computational and storage resources. In this paper, we highlight some of our experiences in building the Environmental Virtual Observatory pilot (EVOp), a tailored cloud-based infrastructure and associated web-based tools designed to enable users from different backgrounds to access data concerning different environmental issues. We review our architecture design, the current deployment and prototypes. We also reflect on lessons learned. We believe that such experiences are of benefit to other scientific communities looking to assemble virtual observatories or similar virtual research environments.</data>
    </node>
    <node id="P135448">
      <data key="title">same same but different a descriptive differentiation of intra cloud iaas services</data>
      <data key="abstract">Users of cloud computing are overwhelmed with choice, even within the services offered by one provider. As such, many users select cloud services based on description alone. We investigate the degree to which such strategy is optimal. In this quantitative study, we investigate the services of 2 of major IaaS providers. We use 2 representative applications to obtain longitudinal observations over 7 days of the week and over different times of the day, totalling over 14,000 executions. We give evidence of significant variations of performance offered within IaaS services, calling for brokers to use automated and adaptive decision making processes with means for incorporating expressive user constraints.</data>
    </node>
    <node id="P84239">
      <data key="title">a manifesto for future generation cloud computing research directions for the next decade</data>
      <data key="abstract">The Cloud computing paradigm has revolutionised the computer science horizon during the past decade and has enabled the emergence of computing as the fifth utility. It has captured significant attention of academia, industries, and government bodies. Now, it has emerged as the backbone of modern economy by offering subscription-based services anytime, anywhere following a pay-as-you-go model. This has instigated (1) shorter establishment times for start-ups, (2) creation of scalable global enterprise applications, (3) better cost-to-value associativity for scientific and high performance computing applications, and (4) different invocation/execution models for pervasive and ubiquitous applications. The recent technological developments and paradigms such as serverless computing, software-defined networking, Internet of Things, and processing at network edge are creating new opportunities for Cloud computing. However, they are also posing several new challenges and creating the need for new approaches and research strategies, as well as the re-evaluation of the models that were developed to address issues such as scalability, elasticity, reliability, security, sustainability, and application models. The proposed manifesto addresses them by identifying the major open challenges in Cloud computing, emerging trends, and impact areas. It then offers research directions for the next decade, thus helping in the realisation of Future Generation Cloud Computing.</data>
    </node>
    <node id="P113460">
      <data key="title">the cloud adoption toolkit supporting cloud adoption decisions in the enterprise</data>
      <data key="abstract">Cloud computing promises a radical shift in the provisioning of computing resource within the enterprise. This paper describes the challenges that decision makers face when assessing the feasibility of the adoption of cloud computing in their organisations, and describes our Cloud Adoption Toolkit, which has been developed to support this process. The toolkit provides a framework to support decision makers in identifying their concerns, and matching these concerns to appropriate tools/techniques that can be used to address them. Cost Modeling is the most mature tool in the toolkit, and this paper shows its effectiveness by demonstrating how practitioners can use it to examine the costs of deploying their IT systems on the cloud. The Cost Modeling tool is evaluated using a case study of an organization that is considering the migration of some of its IT systems to the cloud. The case study shows that running systems on the cloud using a traditional "always on" approach can be less cost effective, and the elastic nature of the cloud has to be used to reduce costs. Therefore, decision makers have to be able to model the variations in resource usage and their systems deployment options to obtain accurate cost estimates.</data>
    </node>
    <node id="P126340">
      <data key="title">don t wait to be breached creating asymmetric uncertainty of cloud applications via moving target defenses</data>
      <data key="abstract">Cloud applications expose - besides service endpoints - also potential or actual vulnerabilities. Therefore, cloud security engineering efforts focus on hardening the fortress walls but seldom assume that attacks may be successful. At least against zero-day exploits, this approach is often toothless. Other than most security approaches and comparable to biological systems we accept that defensive "walls" can be breached at several layers. Instead of hardening the "fortress" walls we propose to make use of an (additional) active and adaptive defense system to attack potential intruders - an immune system that is inspired by the concept of a moving target defense. This "immune system" works on two layers. On the infrastructure layer, virtual machines are continuously regenerated (cell regeneration) to wipe out even undetected intruders. On the application level, the vertical and horizontal attack surface is continuously modified to circumvent successful replays of formerly scripted attacks. Our evaluations with two common cloud-native reference applications in popular cloud service infrastructures (Amazon Web Services, Google Compute Engine, Azure and OpenStack) show that it is technically possible to limit the time of attackers acting undetected down to minutes. Further, more than 98% of an attack surface can be changed automatically and minimized which makes it hard for intruders to replay formerly successful scripted attacks. So, even if intruders get a foothold in the system, it is hard for them to maintain it.</data>
    </node>
    <node id="P49049">
      <data key="title">optimizing deep learning inference on embedded systems through adaptive model selection</data>
      <data key="abstract">Deep neural networks ( DNNs ) are becoming a key enabling technology for many application domains. However, on-device inference on battery-powered, resource-constrained embedding systems is often infeasible due to prohibitively long inferencing time and resource requirements of many DNNs. Offloading computation into the cloud is often unacceptable due to privacy concerns, high latency, or the lack of connectivity. While compression algorithms often succeed in reducing inferencing times, they come at the cost of reduced accuracy. This paper presents a new, alternative approach to enable efficient execution of DNNs on embedded devices. Our approach dynamically determines which DNN to use for a given input, by considering the desired accuracy and inference time. It employs machine learning to develop a low-cost predictive model to quickly select a pre-trained DNN to use for a given input and the optimization constraint. We achieve this by first off-line training a predictive model, and then using the learned model to select a DNN model to use for new, unseen inputs. We apply our approach to two representative DNN domains: image classification and machine translation. We evaluate our approach on a Jetson TX2 embedded deep learning platform and consider a range of influential DNN models including convolutional and recurrent neural networks. For image classification, we achieve a 1.8x reduction in inference time with a 7.52% improvement in accuracy, over the most-capable single DNN model. For machine translation, we achieve a 1.34x reduction in inference time over the most-capable single model, with little impact on the quality of translation.</data>
    </node>
    <node id="P245">
      <data key="title">daleel simplifying cloud instance selection using machine learning</data>
      <data key="abstract">Decision making in cloud environments is quite challenging due to the diversity in service offerings and pricing models, especially considering that the cloud market is an incredibly fast moving one. In addition, there are no hard and fast rules; each customer has a specific set of constraints (e.g. budget) and application requirements (e.g. minimum computational resources). Machine learning can help address some of the complicated decisions by carrying out customer-specific analytics to determine the most suitable instance type(s) and the most opportune time for starting or migrating instances. We employ machine learning techniques to develop an adaptive deployment policy, providing an optimal match between the customer demands and the available cloud service offerings. We provide an experimental study based on extensive set of job executions over a major public cloud infrastructure.</data>
    </node>
    <node id="P86020">
      <data key="title">cloud services brokerage a survey and research roadmap</data>
      <data key="abstract">A Cloud Services Brokerage (CSB) acts as an intermediary between cloud service providers (e.g., Amazon and Google) and cloud service end users, providing a number of value adding services. CSBs as a research topic are in there infancy. The goal of this paper is to provide a concise survey of existing CSB technologies in a variety of areas and highlight a roadmap, which details five future opportunities for research.</data>
    </node>
    <node id="P108004">
      <data key="title">research challenges in nextgen service orchestration</data>
      <data key="abstract">Fog/edge computing, function as a service, and programmable infrastructures, like software-defined networking or network function virtualisation, are becoming ubiquitously used in modern Information Technology infrastructures. These technologies change the characteristics and capabilities of the underlying computational substrate where services run (e.g. higher volatility, scarcer computational power, or programmability). As a consequence, the nature of the services that can be run on them changes too (smaller codebases, more fragmented state, etc.). These changes bring new requirements for service orchestrators, which need to evolve so as to support new scenarios where a close interaction between service and infrastructure becomes essential to deliver a seamless user experience. Here, we present the challenges brought forward by this new breed of technologies and where current orchestration techniques stand with regards to the new challenges. We also present a set of promising technologies that can help tame this brave new world.</data>
    </node>
    <node id="P50741">
      <data key="title">on mobile cloud for smart city applications</data>
      <data key="abstract">This paper is devoted to mobile cloud services in Smart City projects. As per mobile cloud computing paradigm, the data processing and storage are moved from the mobile device to a cloud. In the same time, Smart City services typically contain a set of applications with data sharing options. Most of the services in Smart Cities are actually mashups combined data from several sources. This means that access to all available data is vital to the services. And the mobile cloud is vital because the mobile terminals are one of the main sources for data gathering. In our work, we discuss criteria for selecting mobile cloud services.</data>
    </node>
    <node id="P137115">
      <data key="title">cloud brokerage a systematic survey</data>
      <data key="abstract">Background: The proliferation of cloud providers and provisioning levels has opened a space for cloud brokerage services. Brokers intermediate between cloud customers and providers to assist the customer in selecting the most suitable cloud service, helping to manage the dimensionality, heterogeneity, and uncertainty associated with cloud services. Objective: This paper identifies and classifies approaches to realise cloud brokerage. By doing so, this paper presents an understanding of the state of the art and a novel taxonomy to characterise cloud brokers. Method: We conducted a systematic literature survey to compile studies related to cloud brokerage and explore how cloud brokers are engineered. We analysed the studies from multiple perspectives, such as motivation, functionality, engineering approach, and evaluation methodology. Results: The survey resulted in a knowledge base of current proposals for realising cloud brokers. The survey identified surprising differences between the studies' implementations, with engineering efforts directed at combinations of market-based solutions, middlewares, toolkits, algorithms, semantic frameworks, and conceptual frameworks. Conclusion: Our comprehensive meta-analysis shows that cloud brokerage is still a formative field. There is no doubt that progress has been achieved in the field but considerable challenges remain to be addressed. This survey identifies such challenges and directions for future research.</data>
    </node>
    <node id="P140357">
      <data key="title">cloud benchmarking for performance</data>
      <data key="abstract">How can applications be deployed on the cloud to achieve maximum performance? This question has become significant and challenging with the availability of a wide variety of Virtual Machines (VMs) with different performance capabilities in the cloud. The above question is addressed by proposing a six step benchmarking methodology in which a user provides a set of four weights that indicate how important each of the following groups: memory, processor, computation and storage are to the application that needs to be executed on the cloud. The weights along with cloud benchmarking data are used to generate a ranking of VMs that can maximise performance of the application. The rankings are validated through an empirical analysis using two case study applications, the first is a financial risk application and the second is a molecular dynamics simulation, which are both representative of workloads that can benefit from execution on the cloud. Both case studies validate the feasibility of the methodology and highlight that maximum performance can be achieved on the cloud by selecting the top ranked VMs produced by the methodology.</data>
    </node>
    <node id="P65773">
      <data key="title">transferable knowledge for low cost decision making in cloud environments</data>
      <data key="abstract">Users of cloud computing are increasingly overwhelmed with the wide range of providers and services offered by each provider. As such, many users select cloud services based on description alone. An emerging alternative is to use a decision support system (DSS), which typically relies on gaining insights from observational data in order to assist a customer in making decisions regarding optimal deployment or redeployment of cloud applications. The primary activity of such systems is the generation of a prediction model (e.g. using machine learning), which requires a significantly large amount of training data. However, considering the varying architectures of applications, cloud providers, and cloud offerings, this activity is not sustainable as it incurs additional time and cost to collect training data and subsequently train the models. We overcome this through developing a Transfer Learning (TL) approach where the knowledge (in the form of the prediction model and associated data set) gained from running an application on a particular cloud infrastructure is transferred in order to substantially reduce the overhead of building new models for the performance of new applications and/or cloud infrastructures. In this paper, we present our approach and evaluate it through extensive experimentation involving three real world applications over two major public cloud providers, namely Amazon and Google. Our evaluation shows that our novel two-mode TL scheme increases overall efficiency with a factor of 60\% reduction in the time and cost of generating a new prediction model. We test this under a number of cross-application and cross-cloud scenarios.</data>
    </node>
    <node id="P130520">
      <data key="title">academic cloud computing research five pitfalls and five opportunities</data>
      <data key="abstract">This discussion paper argues that there are five fundamental pitfalls, which can restrict academics from conducting cloud computing research at the infrastructure level, which is currently where the vast majority of academic research lies. Instead academics should be conducting higher risk research, in order to gain understanding and open up entirely new areas. #R##N#We call for a renewed mindset and argue that academic research should focus less upon physical infrastructure and embrace the abstractions provided by clouds through five opportunities: user driven research, new programming models, PaaS environments, and improved tools to support elasticity and large-scale debugging. The objective of this paper is to foster discussion, and to define a roadmap forward, which will allow academia to make longer-term impacts to the cloud computing community.</data>
    </node>
    <node id="P78426">
      <data key="title">doclite a docker based lightweight cloud benchmarking tool</data>
      <data key="abstract">Existing benchmarking methods are time consuming processes as they typically benchmark the entire Virtual Machine (VM) in order to generate accurate performance data, making them less suitable for real-time analytics. The research in this paper is aimed to surmount the above challenge by presenting DocLite - Docker Container-based Lightweight benchmarking tool. DocLite explores lightweight cloud benchmarking methods for rapidly executing benchmarks in near real-time. DocLite is built on the Docker container technology, which allows a user-defined memory size and number of CPU cores of the VM to be benchmarked. The tool incorporates two benchmarking methods - the first referred to as the native method employs containers to benchmark a small portion of the VM and generate performance ranks, and the second uses historic benchmark data along with the native method as a hybrid to generate VM ranks. The proposed methods are evaluated on three use-cases and are observed to be up to 91 times faster than benchmarking the entire VM. In both methods, small containers provide the same quality of rankings as a large container. The native method generates ranks with over 90% and 86% accuracy for sequential and parallel execution of an application compared against benchmarking the whole VM. The hybrid method did not improve the quality of the rankings significantly.</data>
    </node>
    <node id="P81308">
      <data key="title">adaptive selection of deep learning models on embedded systems</data>
      <data key="abstract">The recent ground-breaking advances in deep learning networks ( DNNs ) make them attractive for embedded systems. However, it can take a long time for DNNs to make an inference on resource-limited embedded devices. Offloading the computation into the cloud is often infeasible due to privacy concerns, high latency, or the lack of connectivity. As such, there is a critical need to find a way to effectively execute the DNN models locally on the devices. This paper presents an adaptive scheme to determine which DNN model to use for a given input, by considering the desired accuracy and inference time. Our approach employs machine learning to develop a predictive model to quickly select a pre-trained DNN to use for a given input and the optimization constraint. We achieve this by first training off-line a predictive model, and then use the learnt model to select a DNN model to use for new, unseen inputs. We apply our approach to the image classification task and evaluate it on a Jetson TX2 embedded deep learning platform using the ImageNet ILSVRC 2012 validation dataset. We consider a range of influential DNN models. Experimental results show that our approach achieves a 7.52% improvement in inference accuracy, and a 1.8x reduction in inference time over the most-capable single DNN model.</data>
    </node>
    <node id="P138374">
      <data key="title">learning software configuration spaces a systematic literature review</data>
      <data key="abstract">Most modern software systems (operating systems like Linux or Android, Web browsers like Firefox or Chrome, video encoders like ffmpeg, x264 or VLC, mobile and cloud applications, etc.) are highly-configurable. Hundreds of configuration options, features, or plugins can be combined, each potentially with distinct functionality and effects on execution time, security, energy consumption, etc. Due to the combinatorial explosion and the cost of executing software, it is quickly impossible to exhaustively explore the whole configuration space. Hence, numerous works have investigated the idea of learning it from a small sample of configurationsâ€™ measurements. The pattern "sampling, measuring, learning" has emerged in the literature, with several practical interests for both software developers and end-users of configurable systems. In this survey, we report on the different application objectives (e.g., performance prediction, configuration optimization, constraint mining), use-cases, targeted software systems and application domains. We review the various strategies employed to gather a representative and cost-effective sample. We describe automated software techniques used to measure functional and non-functional properties of configurations. We classify machine learning algorithms and how they relate to the pursued application. Finally, we also describe how researchers evaluate the quality of the learning process. The findings from this systematic review show that the potential application objective is important; there are a vast number of case studies reported in the literature from the basis of several domains and software systems. Yet, the huge variant space of configurable systems is still challenging and calls to further investigate the synergies between artificial intelligence and software engineering.</data>
    </node>
    <node id="P62898">
      <data key="title">price and performance of cloud hosted virtual network functions analysis and future challenges</data>
      <data key="abstract">The concept of Network Function Virtualization (NFV) has been introduced as a new paradigm in the recent few years. NFV offers a number of benefits including significantly increased maintainability and reduced deployment overhead. Several works have been done to optimize deployment (also called embedding) of virtual network functions (VNFs). However, no work to date has looked into optimizing the selection of cloud instances for a given VNF and its specific requirements. In this paper, we evaluate the performance of VNFs when embedded on different Amazon EC2 cloud instances. Specifically, we evaluate three VNFs (firewall, IDS, and NAT) in terms of arrival packet rate, resources utilization, and packet loss. Our results indicate that performance varies across instance types, departing from the intuition of "you get what you pay for" with cloud instances. We also find out that CPU is the critical resource for the tested VNFs, although their peak packet processing capacities differ considerably from each other. Finally, based on the obtained results, we identify key research challenges related to VNF instance selection and service chain provisioning.</data>
    </node>
    <edge source="P146392" target="P245">
      <data key="relation">reference</data>
    </edge>
    <edge source="P12226" target="P86020">
      <data key="relation">reference</data>
    </edge>
    <edge source="P80911" target="P245">
      <data key="relation">reference</data>
    </edge>
    <edge source="P80911" target="P137115">
      <data key="relation">reference</data>
    </edge>
    <edge source="P135448" target="P140357">
      <data key="relation">reference</data>
    </edge>
    <edge source="P135448" target="P245">
      <data key="relation">reference</data>
    </edge>
    <edge source="P135448" target="P137115">
      <data key="relation">reference</data>
    </edge>
    <edge source="P84239" target="P245">
      <data key="relation">reference</data>
    </edge>
    <edge source="P113460" target="P86020">
      <data key="relation">reference</data>
    </edge>
    <edge source="P113460" target="P137115">
      <data key="relation">reference</data>
    </edge>
    <edge source="P126340" target="P86020">
      <data key="relation">reference</data>
    </edge>
    <edge source="P49049" target="P245">
      <data key="relation">reference</data>
    </edge>
    <edge source="P49049" target="P65773">
      <data key="relation">reference</data>
    </edge>
    <edge source="P245" target="P86020">
      <data key="relation">reference</data>
    </edge>
    <edge source="P245" target="P62898">
      <data key="relation">reference</data>
    </edge>
    <edge source="P245" target="P81308">
      <data key="relation">reference</data>
    </edge>
    <edge source="P245" target="P65773">
      <data key="relation">reference</data>
    </edge>
    <edge source="P245" target="P138374">
      <data key="relation">reference</data>
    </edge>
    <edge source="P245" target="P108004">
      <data key="relation">reference</data>
    </edge>
    <edge source="P245" target="P137115">
      <data key="relation">reference</data>
    </edge>
    <edge source="P86020" target="P140357">
      <data key="relation">reference</data>
    </edge>
    <edge source="P86020" target="P130520">
      <data key="relation">reference</data>
    </edge>
    <edge source="P86020" target="P78426">
      <data key="relation">reference</data>
    </edge>
    <edge source="P86020" target="P50741">
      <data key="relation">reference</data>
    </edge>
    <edge source="P86020" target="P137115">
      <data key="relation">reference</data>
    </edge>
    <edge source="P137115" target="P65773">
      <data key="relation">reference</data>
    </edge>
    <edge source="P137115" target="P62898">
      <data key="relation">reference</data>
    </edge>
    <edge source="P137115" target="P130520">
      <data key="relation">reference</data>
    </edge>
    <edge source="P140357" target="P78426">
      <data key="relation">reference</data>
    </edge>
    <edge source="P140357" target="P65773">
      <data key="relation">reference</data>
    </edge>
    <edge source="P65773" target="P62898">
      <data key="relation">reference</data>
    </edge>
    <edge source="P130520" target="P78426">
      <data key="relation">reference</data>
    </edge>
  </graph>
</graphml>
