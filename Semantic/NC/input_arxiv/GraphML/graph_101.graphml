<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="relation" for="edge" attr.name="relation" attr.type="string" />
  <key id="abstract" for="node" attr.name="abstract" attr.type="string" />
  <key id="title" for="node" attr.name="title" attr.type="string" />
  <graph edgedefault="undirected">
    <node id="P139059">
      <data key="title">a fast approach for overcomplete sparse decomposition based on smoothed ell 0 norm</data>
      <data key="abstract">In this paper, a fast algorithm for overcomplete sparse decomposition, called SL0, is proposed. The algorithm is essentially a method for obtaining sparse solutions of underdetermined systems of linear equations, and its applications include underdetermined sparse component analysis (SCA), atomic decomposition on overcomplete dictionaries, compressed sensing, and decoding real field codes. Contrary to previous methods, which usually solve this problem by minimizing the l 1 norm using linear programming (LP) techniques, our algorithm tries to directly minimize the l 1 norm. It is experimentally shown that the proposed algorithm is about two to three orders of magnitude faster than the state-of-the-art interior-point LP solvers, while providing the same (or better) accuracy.</data>
    </node>
    <node id="P63804">
      <data key="title">compressed sensing of analog signals in shift invariant spaces</data>
      <data key="abstract">A traditional assumption underlying most data converters is that the signal should be sampled at a rate exceeding twice the highest frequency. This statement is based on a worst-case scenario in which the signal occupies the entire available bandwidth. In practice, many signals are sparse so that only part of the bandwidth is used. In this paper, we develop methods for low-rate sampling of continuous-time sparse signals in shift-invariant (SI) spaces, generated by m kernels with period T . We model sparsity by treating the case in which only k out of the m generators are active, however, we do not know which k are chosen. We show how to sample such signals at a rate much lower than m/T, which is the minimal sampling rate without exploiting sparsity. Our approach combines ideas from analog sampling in a subspace with a recently developed block diagram that converts an infinite set of sparse equations to a finite counterpart. Using these two components we formulate our problem within the framework of finite compressed sensing (CS) and then rely on algorithms developed in that context. The distinguishing feature of our results is that in contrast to standard CS, which treats finite-length vectors, we consider sampling of analog signals for which no underlying finite-dimensional model exists. The proposed framework allows to extend much of the recent literature on CS to the analog domain.</data>
    </node>
    <node id="P69228">
      <data key="title">iterative hard thresholding for compressed sensing</data>
      <data key="abstract">Compressed sensing is a technique to sample compressible signals below the Nyquist rate, whilst still allowing near optimal reconstruction of the signal. In this paper we present a theoretical analysis of the iterative hard thresholding algorithm when applied to the compressed sensing recovery problem. We show that the algorithm has the following properties (made more precise in the main text of the paper) #R##N#- It gives near-optimal error guarantees. #R##N#- It is robust to observation noise. #R##N#- It succeeds with a minimum number of observations. #R##N#- It can be used with any sampling operator for which the operator and its adjoint can be computed. #R##N#- The memory requirement is linear in the problem size. #R##N#- Its computational complexity per iteration is of the same order as the application of the measurement operator or its adjoint. #R##N#- It requires a fixed number of iterations depending only on the logarithm of a form of signal to noise ratio of the signal. #R##N#- Its performance guarantees are uniform in that they only depend on properties of the sampling operator and signal sparsity.</data>
    </node>
    <node id="P79581">
      <data key="title">euclidean distance matrices essential theory algorithms and applications</data>
      <data key="abstract">Euclidean distance matrices (EDMs) are matrices of the squared distances between points. The definition is deceivingly simple; thanks to their many useful properties, they have found applications in psychometrics, crystallography, machine learning, wireless sensor networks, acoustics, and more. Despite the usefulness of EDMs, they seem to be insufficiently known in the signal processing community. Our goal is to rectify this mishap in a concise tutorial. We review the fundamental properties of EDMs, such as rank or (non)definiteness, and show how the various EDM properties can be used to design algorithms for completing and denoising distance data. Along the way, we demonstrate applications to microphone position calibration, ultrasound tomography, room reconstruction from echoes, and phase retrieval. By spelling out the essential algorithms, we hope to fast-track the readers in applying EDMs to their own problems. The code for all of the described algorithms and to generate the figures in the article is available online at http://lcav.epfl.ch/ivan.dokmanic. Finally, we suggest directions for further research.</data>
    </node>
    <node id="P83024">
      <data key="title">matrix completion from noisy entries</data>
      <data key="abstract">Given a matrix M of low-rank, we consider the problem of reconstructing it from noisy observations of a small, random subset of its entries. The problem arises in a variety of applications, from collaborative filtering (the `Netflix problem') to structure-from-motion and positioning. We study a low complexity algorithm introduced by Keshavan et al.(2009), based on a combination of spectral techniques and manifold optimization, that we call here OptSpace. We prove performance guarantees that are order-optimal in a number of circumstances.</data>
    </node>
    <node id="P71769">
      <data key="title">doa estimation in partially correlated noise using low rank sparse matrix decomposition</data>
      <data key="abstract">We consider the problem of direction-of-arrival (DOA) estimation in unknown partially correlated noise environments where the noise covariance matrix is sparse. A sparse noise covariance matrix is a common model for a sparse array of sensors consisted of several widely separated subarrays. Since interelement spacing among sensors in a subarray is small, the noise in the subarray is in general spatially correlated, while, due to large distances between subarrays, the noise between them is uncorrelated. Consequently, the noise covariance matrix of such an array has a block diagonal structure which is indeed sparse. Moreover, in an ordinary nonsparse array, because of small distance between adjacent sensors, there is noise coupling between neighboring sensors, whereas one can assume that non-adjacent sensors have spatially uncorrelated noise which makes again the array noise covariance matrix sparse. Utilizing some recently available tools in low-rank/sparse matrix decomposition, matrix completion, and sparse representation, we propose a novel method which can resolve possibly correlated or even coherent sources in the aforementioned partly correlated noise. In particular, when the sources are uncorrelated, our approach involves solving a second-order cone programming (SOCP), and if they are correlated or coherent, one needs to solve a computationally harder convex program. We demonstrate the effectiveness of the proposed algorithm by numerical simulations and comparison to the Cramer-Rao bound (CRB).</data>
    </node>
    <node id="P164186">
      <data key="title">performance guarantees for schatten p quasi norm minimization in recovery of low rank matrices</data>
      <data key="abstract">We address some theoretical guarantees for Schatten-$p$ quasi-norm minimization ($p \in (0,1]$) in recovering low-rank matrices from compressed linear measurements. Firstly, using null space properties of the measurement operator, we provide a sufficient condition for exact recovery of low-rank matrices. This condition guarantees unique recovery of matrices of ranks equal or larger than what is guaranteed by nuclear norm minimization. Secondly, this sufficient condition leads to a theorem proving that all restricted isometry property (RIP) based sufficient conditions for $\ell_p$ quasi-norm minimization generalize to Schatten-$p$ quasi-norm minimization. Based on this theorem, we provide a few RIP-based recovery conditions.</data>
    </node>
    <node id="P138634">
      <data key="title">matrix completion with noise</data>
      <data key="abstract">On the heels of compressed sensing, a remarkable new field has very recently emerged. This field addresses a broad range of problems of significant practical interest, namely, the recovery of a data matrix from what appears to be incomplete, and perhaps even corrupted, information. In its simplest form, the problem is to recover a matrix from a small sample of its entries, and comes up in many areas of science and engineering including collaborative filtering, machine learning, control, remote sensing, and computer vision to name a few. #R##N#This paper surveys the novel literature on matrix completion, which shows that under some suitable conditions, one can recover an unknown low-rank matrix from a nearly minimal set of entries by solving a simple convex optimization problem, namely, nuclear-norm minimization subject to data constraints. Further, this paper introduces novel results showing that matrix completion is provably accurate even when the few observed entries are corrupted with a small amount of noise. A typical result is that one can recover an unknown n x n matrix of low rank r from just about nr log^2 n noisy samples with an error which is proportional to the noise level. We present numerical results which complement our quantitative analysis and show that, in practice, nuclear norm minimization accurately fills in the many missing entries of large low-rank matrices from just a few noisy samples. Some analogies between matrix completion and compressed sensing are discussed throughout.</data>
    </node>
    <node id="P73918">
      <data key="title">generalized approximate message passing for estimation with random linear mixing</data>
      <data key="abstract">We consider the estimation of an i.i.d.\ random vector observed through a linear transform followed by a componentwise, probabilistic (possibly nonlinear) measurement channel. A novel algorithm, called generalized approximate message passing (GAMP), is presented that provides computationally efficient approximate implementations of max-sum and sum-problem loopy belief propagation for such problems. The algorithm extends earlier approximate message passing methods to incorporate arbitrary distributions on both the input and output of the transform and can be applied to a wide range of problems in nonlinear compressed sensing and learning. #R##N#Extending an analysis by Bayati and Montanari, we argue that the asymptotic componentwise behavior of the GAMP method under large, i.i.d. Gaussian transforms is described by a simple set of state evolution (SE) equations. From the SE equations, one can \emph{exactly} predict the asymptotic value of virtually any componentwise performance metric including mean-squared error or detection accuracy. Moreover, the analysis is valid for arbitrary input and output distributions, even when the corresponding optimization problems are non-convex. The results match predictions by Guo and Wang for relaxed belief propagation on large sparse matrices and, in certain instances, also agree with the optimal performance predicted by the replica method. The GAMP methodology thus provides a computationally efficient methodology, applicable to a large class of non-Gaussian estimation problems with precise asymptotic performance guarantees.</data>
    </node>
    <node id="P73105">
      <data key="title">recovery of low rank matrices under affine constraints via a smoothed rank function</data>
      <data key="abstract">In this paper, the problem of matrix rank minimization under affine constraints is addressed. The state-of-the-art algorithms can recover matrices with a rank much less than what is sufficient for the uniqueness of the solution of this optimization problem. We propose an algorithm based on a smooth approximation of the rank function, which practically improves recovery limits on the rank of the solution. This approximation leads to a non-convex program; thus, to avoid getting trapped in local solutions, we use the following scheme. Initially, a rough approximation of the rank function subject to the affine constraints is optimized. As the algorithm proceeds, finer approximations of the rank are optimized and the solver is initialized with the solution of the previous approximation until reaching the desired accuracy. On the theoretical side, benefiting from the spherical section property, we will show that the sequence of the solutions of the approximating programs converges to the minimum rank solution. On the experimental side, it will be shown that the proposed algorithm, termed SRF standing for smoothed rank function, can recover matrices, which are unique solutions of the rank minimization problem and yet not recoverable by nuclear norm minimization. Furthermore, it will be demonstrated that, in completing partially observed matrices, the accuracy of SRF is considerably and consistently better than some famous algorithms when the number of revealed entries is close to the minimum number of parameters that uniquely represent a low-rank matrix.</data>
    </node>
    <node id="P161471">
      <data key="title">successive concave sparsity approximation near oracle performance in a wide range of sparsity levels</data>
      <data key="abstract">In this paper, based on a successively accuracy-increasing approximation of the $\ell_0$ norm, we propose a new algorithm for recovery of sparse vectors from underdetermined measurements. The approximations are realized with a certain class of concave functions that aggressively induce sparsity and their closeness to the $\ell_0$ norm can be controlled. We prove that the series of the approximations asymptotically coincides with the $\ell_1$ and $\ell_0$ norms when the approximation accuracy changes from the worst fitting to the best fitting. When measurements are noise-free, an optimization scheme is proposed which leads to a number of weighted $\ell_1$ minimization programs, whereas, in the presence of noise, we propose two iterative thresholding methods that are computationally appealing. A convergence guarantee for the iterative thresholding method is provided, and, for a particular function in the class of the approximating functions, we derive the closed-form thresholding operator. We further present some theoretical analyses via the restricted isometry, null space, and spherical section properties. Our extensive numerical simulations indicate that the proposed algorithm closely follows the performance of the oracle estimator for a range of sparsity levels wider than those of the state-of-the-art algorithms.</data>
    </node>
    <node id="P124">
      <data key="title">iterative concave rank approximation for recovering low rank matrices</data>
      <data key="abstract">In this paper, we propose a new algorithm for recovery of low-rank matrices from compressed linear measurements. The underlying idea of this algorithm is to closely approximate the rank function with a smooth function of singular values, and then minimize the resulting approximation subject to the linear constraints. The accuracy of the approximation is controlled via a scaling parameter δ, where a smaller δ corresponds to a more accurate fitting. The consequent optimization problem for any finite δ is nonconvex. Therefore, to decrease the risk of ending up in local minima, a series of optimizations is performed, starting with optimizing a rough approximation (a large δ) and followed by successively optimizing finer approximations of the rank with smaller δ's. To solve the optimization problem for any δ &gt; 0, it is converted to a new program in which the cost is a function of two auxiliary positive semidefinite variables. The paper shows that this new program is concave and applies a majorize-minimize technique to solve it which, in turn, leads to a few convex optimization iterations. This optimization scheme is also equivalent to a reweighted Nuclear Norm Minimization (NNM). For any δ &gt; 0, we derive a necessary and sufficient condition for the exact recovery which are weaker than those corresponding to NNM. On the numerical side, the proposed algorithm is compared to NNM and a reweighted NNM in solving affine rank minimization and matrix completion problems showing its considerable and consistent superiority in terms of success rate.</data>
    </node>
    <node id="P32047">
      <data key="title">a class of nonconvex penalties preserving overall convexity in optimization based mean filtering</data>
      <data key="abstract">$\ell _1$   mean filtering is a conventional, optimization-based method to estimate the positions of jumps in a piecewise constant signal perturbed by additive noise. In this method, the   $\ell _1$   norm penalizes sparsity of the first-order derivative of the signal. Theoretical results, however, show that in some situations, which can occur frequently in practice, even when the jump amplitudes tend to    $\infty$  , the conventional method identifies false change points. This issue, which is referred to as the stair-casing problem herein, restricts practical importance of   $\ell _1$   mean filtering. In this paper, sparsity is penalized more tightly than the   $\ell _1$    norm by exploiting a certain class of nonconvex functions, while the strict convexity of the consequent optimization problem is preserved. This results in a higher performance in detecting change points. To theoretically justify the performance improvements over   $\ell _1$    mean filtering, deterministic and stochastic sufficient conditions for exact change point recovery are derived. In particular, theoretical results show that in the stair-casing problem, our approach might be able to exclude the false change points, while   $\ell _1$   mean filtering may fail. A number of numerical simulations assist to show superiority of our method over   $\ell _1$   mean filtering and another state-of-the-art algorithm that promotes sparsity tighter than the   $\ell _1$   norm. Specifically, it is shown that our approach can consistently detect change points when the jump amplitudes become sufficiently large, while the two other competitors cannot.</data>
    </node>
    <node id="P92164">
      <data key="title">matrix completion from a few entries</data>
      <data key="abstract">Let M be a random (alpha n) x n matrix of rank r&lt;&lt;n, and assume that a uniformly random subset E of its entries is observed. We describe an efficient algorithm that reconstructs M from |E| = O(rn) observed entries with relative root mean square error RMSE &lt;= C(rn/|E|)^0.5 . Further, if r=O(1), M can be reconstructed exactly from |E| = O(n log(n)) entries. These results apply beyond random matrices to general low-rank incoherent matrices. #R##N#This settles (in the case of bounded rank) a question left open by Candes and Recht and improves over the guarantees for their reconstruction algorithm. The complexity of our algorithm is O(|E|r log(n)), which opens the way to its use for massive data sets. In the process of proving these statements, we obtain a generalization of a celebrated result by Friedman-Kahn-Szemeredi and Feige-Ofek on the spectrum of sparse random matrices.</data>
    </node>
    <node id="P6588">
      <data key="title">calibration using matrix completion with application to ultrasound tomography</data>
      <data key="abstract">We study the application of matrix completion in the process of calibrating physical devices. In particular we propose an algorithm together with reconstruction bounds for calibrating circular ultrasound tomography devices. We use the time-of-flight (ToF) measurements between sensor pairs in a homogeneous medium to calibrate the system. The calibration process consists of a low-rank matrix completion algorithm to de-noise and estimate random and structured missing ToFs, and the classic multi-dimensional scaling method to estimate the sensor positions from the ToF measurements. We provide theoretical bounds on the calibration error. Several simulations are conducted to evaluate the theoretical results presented in this paper.</data>
    </node>
    <node id="P139554">
      <data key="title">ell_0 motivated low rank sparse subspace clustering</data>
      <data key="abstract">In many applications, high-dimensional data points can be well represented by low-dimensional subspaces. To identify the subspaces, it is important to capture a global and local structure of the data which is achieved by imposing low-rank and sparseness constraints on the data representation matrix. In low-rank sparse subspace clustering (LRSSC), nuclear and    ${\ell _{1}}$   -norms are used to measure rank and sparsity. However, the use of nuclear and    ${\ell _{1}}$   -norms leads to an overpenalized problem and only approximates the original problem. In this paper, we propose two    ${\ell _{0}}$    quasi-norm-based regularizations. First, this paper presents regularization based on multivariate generalization of minimax-concave penalty (GMC-LRSSC), which contains the global minimizers of a    ${\ell _{0}}$    quasi-norm regularized objective. Afterward, we introduce the Schatten-0 (   ${S_{0}}$   ) and    ${\ell _{0}}$   -regularized objective and approximate the proximal map of the joint solution using a proximal average method (   ${S_{0}/\ell _{0}}$   -LRSSC). The resulting nonconvex optimization problems are solved using an alternating direction method of multipliers with established convergence conditions of both algorithms. Results obtained on synthetic and four real-world datasets show the effectiveness of GMC-LRSSC and    ${S_{0}/\ell _{0}}$   -LRSSC when compared to state-of-the-art methods.</data>
    </node>
    <node id="P146548">
      <data key="title">exact matrix completion via convex optimization</data>
      <data key="abstract">We consider a problem of considerable practical interest: the recovery of a data matrix from a sampling of its entries. Suppose that we observe m entries selected uniformly at random from a matrix M. Can we complete the matrix and recover the entries that we have not seen? #R##N#We show that one can perfectly recover most low-rank matrices from what appears to be an incomplete set of entries. We prove that if the number m of sampled entries obeys m &gt;= C n^{1.2} r log n for some positive numerical constant C, then with very high probability, most n by n matrices of rank r can be perfectly recovered by solving a simple convex optimization program. This program finds the matrix with minimum nuclear norm that fits the data. The condition above assumes that the rank is not too large. However, if one replaces the 1.2 exponent with 1.25, then the result holds for all values of the rank. Similar results hold for arbitrary rectangular matrices as well. Our results are connected with the recent literature on compressed sensing, and show that objects other than signals and images can be perfectly reconstructed from very limited information.</data>
    </node>
    <node id="P115654">
      <data key="title">a high resolution doa estimation method with a family of nonconvex penalties</data>
      <data key="abstract">The low-rank matrix reconstruction (LRMR) approach is widely used in direction-of-arrival (DOA) estimation. As the rank norm penalty in an LRMR is NP-hard to compute, the nuclear norm (or the trace norm for a positive semidefinite (PSD) matrix) has been often employed as a convex relaxation of the rank norm. However, solving a nuclear norm convex problem may lead to a suboptimal solution of the original rank norm problem. In this paper, we propose to apply a family of nonconvex penalties on the singular values of the covariance matrix as the sparsity metrics to approximate the rank norm. In particular, we formulate a nonconvex minimization problem and solve it by using a locally convergent iterative reweighted strategy in order to enhance the sparsity and resolution. The problem in each iteration is convex and hence can be solved by using the optimization toolbox. Convergence analysis shows that the new method is able to obtain a suboptimal solution. The connection between the proposed method and the sparse signal reconstruction (SSR) is explored showing that our method can be regarded as a sparsity-based method with the number of sampling grids approaching infinity. Two feasible implementation algorithms that are based on solving a duality problem and deducing a closed-form solution of the simplified problem are also provided for the convex problem at each iteration to expedite the convergence. Extensive simulation studies are conducted to show the superiority of the proposed methods.</data>
    </node>
    <node id="P32382">
      <data key="title">nonconvex and nonsmooth sparse optimization via adaptively iterative reweighted methods</data>
      <data key="abstract">We present a general formulation of nonconvex and nonsmooth sparse optimization problems with a convexset constraint, which takes into account most existing types of nonconvex sparsity-inducing terms. It thus brings strong applicability to a wide range of applications. We further design a general algorithmic framework of adaptively iterative reweighted algorithms for solving the nonconvex and nonsmooth sparse optimization problems. This is achieved by solving a sequence of weighted convex penalty subproblems with adaptively updated weights. The first-order optimality condition is then derived and the global convergence results are provided under loose assumptions. This makes our theoretical results a practical tool for analyzing a family of various iteratively reweighted algorithms. In particular, for the iteratively reweighed $\ell_1$-algorithm, global convergence analysis is provided for cases with diminishing relaxation parameter. For the iteratively reweighed $\ell_2$-algorithm, adaptively decreasing relaxation parameter is applicable and the existence of the cluster point to the algorithm is established. The effectiveness and efficiency of our proposed formulation and the algorithms are demonstrated in numerical experiments in various sparse optimization problems.</data>
    </node>
    <node id="P101499">
      <data key="title">a simpler approach to matrix completion</data>
      <data key="abstract">This paper provides the best bounds to date on the number of randomly sampled entries required to reconstruct an unknown low rank matrix. These results improve on prior work by Candes and Recht, Candes and Tao, and Keshavan, Montanari, and Oh. The reconstruction is accomplished by minimizing the nuclear norm, or sum of the singular values, of the hidden matrix subject to agreement with the provided entries. If the underlying matrix satisfies a certain incoherence condition, then the number of entries required is equal to a quadratic logarithmic factor times the number of parameters in the singular value decomposition. The proof of this assertion is short, self contained, and uses very elementary analysis. The novel techniques herein are based on recent work in quantum information theory.</data>
    </node>
    <edge source="P139059" target="P161471">
      <data key="relation">reference</data>
    </edge>
    <edge source="P139059" target="P124">
      <data key="relation">reference</data>
    </edge>
    <edge source="P139059" target="P73105">
      <data key="relation">reference</data>
    </edge>
    <edge source="P139059" target="P139554">
      <data key="relation">reference</data>
    </edge>
    <edge source="P63804" target="P161471">
      <data key="relation">reference</data>
    </edge>
    <edge source="P69228" target="P161471">
      <data key="relation">reference</data>
    </edge>
    <edge source="P69228" target="P139554">
      <data key="relation">reference</data>
    </edge>
    <edge source="P79581" target="P6588">
      <data key="relation">reference</data>
    </edge>
    <edge source="P79581" target="P92164">
      <data key="relation">reference</data>
    </edge>
    <edge source="P79581" target="P83024">
      <data key="relation">reference</data>
    </edge>
    <edge source="P83024" target="P6588">
      <data key="relation">reference</data>
    </edge>
    <edge source="P83024" target="P92164">
      <data key="relation">reference</data>
    </edge>
    <edge source="P83024" target="P146548">
      <data key="relation">reference</data>
    </edge>
    <edge source="P83024" target="P101499">
      <data key="relation">reference</data>
    </edge>
    <edge source="P83024" target="P138634">
      <data key="relation">reference</data>
    </edge>
    <edge source="P71769" target="P124">
      <data key="relation">reference</data>
    </edge>
    <edge source="P71769" target="P164186">
      <data key="relation">reference</data>
    </edge>
    <edge source="P71769" target="P146548">
      <data key="relation">reference</data>
    </edge>
    <edge source="P164186" target="P124">
      <data key="relation">reference</data>
    </edge>
    <edge source="P164186" target="P6588">
      <data key="relation">reference</data>
    </edge>
    <edge source="P164186" target="P138634">
      <data key="relation">reference</data>
    </edge>
    <edge source="P164186" target="P32047">
      <data key="relation">reference</data>
    </edge>
    <edge source="P164186" target="P115654">
      <data key="relation">reference</data>
    </edge>
    <edge source="P138634" target="P124">
      <data key="relation">reference</data>
    </edge>
    <edge source="P138634" target="P73105">
      <data key="relation">reference</data>
    </edge>
    <edge source="P138634" target="P92164">
      <data key="relation">reference</data>
    </edge>
    <edge source="P138634" target="P146548">
      <data key="relation">reference</data>
    </edge>
    <edge source="P138634" target="P101499">
      <data key="relation">reference</data>
    </edge>
    <edge source="P73918" target="P161471">
      <data key="relation">reference</data>
    </edge>
    <edge source="P73105" target="P161471">
      <data key="relation">reference</data>
    </edge>
    <edge source="P73105" target="P124">
      <data key="relation">reference</data>
    </edge>
    <edge source="P73105" target="P92164">
      <data key="relation">reference</data>
    </edge>
    <edge source="P73105" target="P146548">
      <data key="relation">reference</data>
    </edge>
    <edge source="P73105" target="P32047">
      <data key="relation">reference</data>
    </edge>
    <edge source="P161471" target="P124">
      <data key="relation">reference</data>
    </edge>
    <edge source="P124" target="P6588">
      <data key="relation">reference</data>
    </edge>
    <edge source="P124" target="P146548">
      <data key="relation">reference</data>
    </edge>
    <edge source="P124" target="P32047">
      <data key="relation">reference</data>
    </edge>
    <edge source="P124" target="P115654">
      <data key="relation">reference</data>
    </edge>
    <edge source="P124" target="P32382">
      <data key="relation">reference</data>
    </edge>
    <edge source="P124" target="P139554">
      <data key="relation">reference</data>
    </edge>
    <edge source="P92164" target="P6588">
      <data key="relation">reference</data>
    </edge>
    <edge source="P92164" target="P146548">
      <data key="relation">reference</data>
    </edge>
    <edge source="P92164" target="P101499">
      <data key="relation">reference</data>
    </edge>
    <edge source="P6588" target="P146548">
      <data key="relation">reference</data>
    </edge>
    <edge source="P6588" target="P101499">
      <data key="relation">reference</data>
    </edge>
    <edge source="P139554" target="P146548">
      <data key="relation">reference</data>
    </edge>
    <edge source="P146548" target="P101499">
      <data key="relation">reference</data>
    </edge>
  </graph>
</graphml>
