<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="relation" for="edge" attr.name="relation" attr.type="string" />
  <key id="abstract" for="node" attr.name="abstract" attr.type="string" />
  <key id="title" for="node" attr.name="title" attr.type="string" />
  <graph edgedefault="undirected">
    <node id="P141">
      <data key="title">how to improve the outcome of performance evaluations in terms of percentiles for citation frequencies of my papers</data>
      <data key="abstract">Using empirical data I demonstrate that the result of performance evaluations by percentiles can be drastically influenced by the proper choice of the journal in which a manuscript is published.</data>
    </node>
    <node id="P53346">
      <data key="title">the precision of the arithmetic mean geometric mean and percentiles for citation data an experimental simulation modelling approach</data>
      <data key="abstract">When comparing the citation impact of nations, departments or other groups of researchers within individual fields, three approaches have been proposed: arithmetic means, geometric means, and percentage in the top X%. This article compares the precision of these statistics using 97 trillion experimentally simulated citation counts from 6875 sets of different parameters (although all having the same scale parameter) based upon the discretised lognormal distribution with limits from 1000 repetitions for each parameter set. The results show that the geometric mean is the most precise, closely followed by the percentage of a country's articles in the top 50% most cited articles for a field, year and document type. Thus the geometric mean citation count is recommended for future citation-based comparisons between nations. The percentage of a country's articles in the top 1% most cited is a particularly imprecise indicator and is not recommended for international comparisons based on individual fields. Moreover, whereas standard confidence interval formulae for the geometric mean appear to be accurate, confidence interval formulae are less accurate and consistent for percentile indicators. These recommendations assume that the scale parameters of the samples are the same but the choice of indicator is complex and partly conceptual if they are not.</data>
    </node>
    <node id="P84380">
      <data key="title">the leiden ranking 2011 2012 data collection indicators and interpretation</data>
      <data key="abstract">The Leiden Ranking 2011/2012 is a ranking of universities based on bibliometric indicators of publication output, citation impact, and scientific collaboration. The ranking includes 500 major universities from 41 different countries. This paper provides an extensive discussion of the Leiden Ranking 2011/2012. The ranking is compared with other global university rankings, in particular the Academic Ranking of World Universities (commonly known as the Shanghai Ranking) and the Times Higher Education World University Rankings. Also, a detailed description is offered of the data collection methodology of the Leiden Ranking 2011/2012 and of the indicators used in the ranking. Various innovations in the Leiden Ranking 2011/2012 are presented. These innovations include (1) an indicator based on counting a university's highly cited publications, (2) indicators based on fractional rather than full counting of collaborative publications, (3) the possibility of excluding non-English language publications, and (4) the use of stability intervals. Finally, some comments are made on the interpretation of the ranking, and a number of limitations of the ranking are pointed out.</data>
    </node>
    <node id="P129901">
      <data key="title">are there too many uncited articles zero inflated variants of the discretised lognormal and hooked power law distributions</data>
      <data key="abstract">Although statistical models fit many citation data sets reasonably well with the best fitting models being the hooked power law and discretised lognormal distribution, the fits are rarely close. One possible reason is that there might be more uncited articles than would be predicted by any model if some articles are inherently uncitable. Using data from 23 different Scopus categories, this article tests the assumption that removing a proportion of uncited articles from a citation dataset allows statistical distributions to have much closer fits. It also introduces two new models, zero inflated discretised lognormal distribution and the zero inflated hooked power law distribution and algorithms to fit them. In all 23 cases, the zero inflated version of the discretised lognormal distribution was an improvement on the standard version and in 16 out of 23 cases the zero inflated version of the hooked power law was an improvement on the standard version. Without zero inflation the discretised lognormal models fit the data better than the hooked power law distribution 6 out of 23 times and with it, the discretised lognormal models fit the data better than the hooked power law distribution 9 out of 23 times. Apparently uncitable articles seem to occur due to the presence of academic-related magazines in Scopus categories. In conclusion, future citation analysis and research indicators should take into account uncitable articles, and the best fitting distribution for sets of citation counts from a single subject and year is either the zero inflated discretised lognormal or zero inflated hooked power law.</data>
    </node>
    <node id="P161639">
      <data key="title">on the calculation of percentile based bibliometric indicators</data>
      <data key="abstract">A percentile-based bibliometric indicator is an indicator that values publications based on their position within the citation distribution of their field. The most straightforward percentile-based indicator is the proportion of frequently cited publications, for instance, the proportion of publications that belong to the top 10% most frequently cited of their field. Recently, more complex percentile-based indicators have been proposed. A difficulty in the calculation of percentile-based indicators is caused by the discrete nature of citation distributions combined with the presence of many publications with the same number of citations. We introduce an approach to calculating percentile-based indicators that deals with this difficulty in a more satisfactory way than earlier approaches suggested in the literature. We show in a formal mathematical framework that our approach leads to indicators that do not suffer from biases in favor of or against particular fields of science.</data>
    </node>
    <node id="P151774">
      <data key="title">a new methodology for constructing a publication level classification system of science</data>
      <data key="abstract">Classifying journals or publications into research areas is an essential element of many bibliometric analyses. Classification usually takes place at the level of journals, where the Web of Science subject categories are the most popular classification system. However, journal-level classification systems have two important limitations: They offer only a limited amount of detail, and they have difficulties with multidisciplinary journals. To avoid these limitations, we introduce a new methodology for constructing classification systems at the level of individual publications. In the proposed methodology, publications are clustered into research areas based on citation relations. The methodology is able to deal with very large numbers of publications. We present an application in which a classification system is produced that includes almost ten million publications. Based on an extensive analysis of this classification system, we discuss the strengths and the limitations of the proposed methodology. Important strengths are the transparency and relative simplicity of the methodology and its fairly modest computing and memory requirements. The main limitation of the methodology is its exclusive reliance on direct citation relations between publications. The accuracy of the methodology can probably be increased by also taking into account other types of relations, for instance based on bibliographic coupling.</data>
    </node>
    <node id="P20082">
      <data key="title">identifying potential breakthrough publications using refined citation analyses three related explorative approaches</data>
      <data key="abstract">The article presents three advanced citation-based methods used to detect potential breakthrough papers among very highly cited papers. We approach the detection of such papers from three different perspectives in order to provide different typologies of breakthrough papers. In all three cases we use the classification of scientific publications developed at CWTS based on direct citation relationships. This classification establishes clusters of papers at three levels of aggregation. Papers are clustered based on their similar citation orientations and it is assumed that they are focused on similar research interests. We use the clustering as the context for detecting potential breakthrough papers. We utilize the Characteristics Scores and Scales (CSS) approach to partition citation distributions and implement a specific filtering algorithm to sort out potential highly-cited followers, papers not considered breakthroughs in themselves. After invoking thresholds and filtering, three methods are explored: A very exclusive one where only the highest cited paper in a micro-cluster is considered as a potential breakthrough paper (M1); as well as two conceptually different methods, one that detects potential breakthrough papers among the two percent highest cited papers according to CSS (M2a), and finally a more restrictive version where, in addition to the CSS two percent filter, knowledge diffusion is also taken in as an extra parameter (M2b). The advance citation-based methods are explored and evaluated using specifically validated publication sets linked to different Danish funding instruments including centres of excellence.</data>
    </node>
    <node id="P54132">
      <data key="title">the normalization of citation counts based on classification systems</data>
      <data key="abstract">If we want to assess whether the paper in question has had a particularly high or low citation impact compared to other papers, the standard practice in bibliometrics is to normalize citations in respect of the subject category and publication year. A number of proposals for an improved procedure in the normalization of citation impact have been put forward in recent years. Against the background of these proposals this study describes an ideal solution for the normalization of citation impact: in a first step, the reference set for the publication in question is collated by means of a classification scheme, where every publication is associated with a single principal research field or subfield entry (e. g. via Chemical Abstracts sections) and a publication year. In a second step, percentiles of citation counts are calculated for this set and used to assign the normalized citation impact score to the publications (and also to the publication in question).</data>
    </node>
    <node id="P54536">
      <data key="title">the substantive and practical significance of citation impact differences between institutions guidelines for the analysis of percentiles using effect sizes and confidence intervals</data>
      <data key="abstract">In this chapter we address the statistical analysis of percentiles: How should the citation impact of institutions be compared? In educational and psychological testing, percentiles are already used widely as a standard to evaluate an individual’s test scores—intelligence tests for example—by comparing them with the scores of a calibrated sample. Percentiles, or percentile rank classes, are also a very suitable method for bibliometrics to normalize citations of publications in terms of the subject category and the publication year and, unlike the mean-based indicators (the relative citation rates), percentiles are scarcely affected by skewed distributions of citations. The percentile of a certain publication provides information about the citation impact this publication has achieved in comparison to other similar publications in the same subject category and publication year. Analyses of percentiles, however, have not always been presented in the most effective and meaningful way. New APA guidelines (Association American Psychological, Publication manual of the American Psychological Association (6 ed.). Washington, DC: American Psychological Association (APA), 2010) suggest a lesser emphasis on significance tests and a greater emphasis on the substantive and practical significance of findings. Drawing on work by Cumming (Understanding the new statistics: effect sizes, confidence intervals, and meta-analysis. London: Routledge, 2012) we show how examinations of effect sizes (e.g., Cohen’s d statistic) and confidence intervals can lead to a clear understanding of citation impact differences.</data>
    </node>
    <node id="P76634">
      <data key="title">which percentile based approach should be preferred for calculating normalized citation impact values an empirical comparison of five approaches including a newly developed citation rank approach p100</data>
      <data key="abstract">Percentile-based approaches have been proposed as a non-parametric alternative to parametric central-tendency statistics to normalize observed citation counts. Percentiles are based on an ordered set of citation counts in a reference set, whereby the fraction of papers at or below the citation counts of a focal paper is used as an indicator for its relative citation impact in the set. In this study, we pursue two related objectives: (1) although different percentile-based approaches have been developed, an approach is hitherto missing that satisfies a number of criteria such as scaling of the percentile ranks from zero (all other papers perform better) to 100 (all other papers perform worse), and solving the problem with tied citation ranks unambiguously. We introduce a new citation-rank approach having these properties, namely P100. (2) We compare the reliability of P100 empirically with other percentile-based approaches, such as the approaches developed by the SCImago group, the Centre for Science and Technology Studies (CWTS), and Thomson Reuters (InCites), using all papers published in 1980 in Thomson Reuters Web of Science (WoS). How accurately can the different approaches predict the long-term citation impact in 2010 (in year 31) using citation impact measured in previous time windows (years 1 to 30)? The comparison of the approaches shows that the method used by InCites overestimates citation impact (because of using the highest percentile rank when papers are assigned to more than a single subject category) whereas the SCImago indicator shows higher power in predicting the long-term citation impact on the basis of citation rates in early years. Since the results show a disadvantage in this predictive ability for P100 against the other approaches, there is still room for further improvements.</data>
    </node>
    <node id="P80356">
      <data key="title">turning the tables in citation analysis one more time principles for comparing sets of documents</data>
      <data key="abstract">We submit newly developed citation impact indicators based not on arithmetic averages of citations but on percentile ranks. Citation distributions are-as a rule-highly skewed and should not be arithmetically averaged. With percentile ranks, the citation of each paper is rated in terms of its percentile in the citation distribution. The percentile ranks approach allows for the formulation of a more abstract indicator scheme that can be used to organize and/or schematize different impact indicators according to three degrees of freedom: the selection of the reference sets, the evaluation criteria, and the choice of whether or not to define the publication sets as independent. Bibliometric data of seven principal investigators (PIs) of the Academic Medical Center of the University of Amsterdam is used as an exemplary data set. We demonstrate that the proposed indicators [R(6), R(100), R(6,k), R(100,k)] are an improvement of averages-based indicators because one can account for the shape of the distributions of citations over papers.</data>
    </node>
    <node id="P100725">
      <data key="title">how much do different ways of calculating percentiles influence the derived performance indicators a case study</data>
      <data key="abstract">Bibliometric indicators can be determined by comparing specific citation records with the percentiles of a reference set. However, there exists an ambiguity in the computation of percentiles because usually a significant number of papers with the same citation count are found at the border between percentile rank classes. The present case study of the citations to the journal Europhysics Letters (EPL) in comparison with all physics papers from the Web of Science shows the deviations which occur due to the different ways of treating the tied papers in the evaluation of the percentage of highly cited publications. A strong bias can occur, if the papers tied at the threshold number of citations are all considered as highly cited or all considered as not highly cited.</data>
    </node>
    <node id="P43525">
      <data key="title">percentile ranks and the integrated impact indicator i3</data>
      <data key="abstract">We tested Rousseau's (in press) recent proposal to define percentile classes in the case of the Integrated Impact Indicator (I3) so that the largest number in a set always belongs to the highest (100th) percentile rank class. In the case a set of nine uncited papers and one with citation, however, the uncited papers would all be placed in the 90th percentile rank. A lowly-cited document set would thus be advantaged when compared with a highly-cited one. Notwithstanding our reservations, we extended the program for computing I3 in Web-of-Science data (at this http URL) with this option; the quantiles without a correction are now the default. As Rousseau mentions, excellence indicators (e.g., the top-10%) can be considered as special cases of I3: only two percentile rank classes are distinguished for the evaluation. Both excellence and impact indicators can be tested statistically using the z-test for independent proportions.</data>
    </node>
    <node id="P67364">
      <data key="title">accounting for the uncertainty in the evaluation of percentile ranks</data>
      <data key="abstract">In a recent paper entitled "Inconsistencies of Recently Proposed Citation Impact Indicators and how to Avoid Them," Schreiber (2012, at arXiv:1202.3861) proposed (i) a method to assess tied ranks consistently and (ii) fractional attribution to percentile ranks in the case of relatively small samples (e.g., for n &lt; 100). Schreiber's solution to the problem of how to handle tied ranks is convincing, in my opinion (cf. Pudovkin &amp; Garfield, 2009). The fractional attribution, however, is computationally intensive and cannot be done manually for even moderately large batches of documents. Schreiber attributed scores fractionally to the six percentile rank classes used in the Science and Engineering Indicators of the U.S. National Science Board, and thus missed, in my opinion, the point that fractional attribution at the level of hundred percentiles-or equivalently quantiles as the continuous random variable-is only a linear, and therefore much less complex problem. Given the quantile-values, the non-linear attribution to the six classes or any other evaluation scheme is then a question of aggregation. A new routine based on these principles (including Schreiber's solution for tied ranks) is made available as software for the assessment of documents retrieved from the Web of Science (at this http URL).</data>
    </node>
    <node id="P65313">
      <data key="title">citation analysis with microsoft academic</data>
      <data key="abstract">We explore if and how Microsoft Academic (MA) could be used for bibliometric analyses. First, we examine the Academic Knowledge API (AK API), an interface to access MA data, and compare it to Google Scholar (GS). Second, we perform a comparative citation analysis of researchers by normalizing data from MA and Scopus. We find that MA offers structured and rich metadata, which facilitates data retrieval, handling and processing. In addition, the AK API allows retrieving frequency distributions of citations. We consider these features to be a major advantage of MA over GS. However, we identify four main limitations regarding the available metadata. First, MA does not provide the document type of a publication. Second, the "fields of study" are dynamic, too specific and field hierarchies are incoherent. Third, some publications are assigned to incorrect years. Fourth, the metadata of some publications did not include all authors. Nevertheless, we show that an average-based indicator (i.e. the journal normalized citation score; JNCS) as well as a distribution-based indicator (i.e. percentile rank classes; PR classes) can be calculated with relative ease using MA. Hence, normalization of citation counts is feasible with MA. The citation analyses in MA and Scopus yield uniform results. The JNCS and the PR classes are similar in both databases, and, as a consequence, the evaluation of the researchers' publication impact is congruent in MA and Scopus. Given the fast development in the last year, we postulate that MA has the potential to be used for full-fledged bibliometric analyses.</data>
    </node>
    <node id="P110892">
      <data key="title">citation analysis with medical subject headings mesh using the web of knowledge a new routine</data>
      <data key="abstract">Citation analysis of documents retrieved from the Medline database (at the Web of Knowledge) has been possible only on a case-by-case basis. A technique is here developed for citation analysis in batch mode using both Medical Subject Headings (MeSH) at the Web of Knowledge and the Science Citation Index at the Web of Science. This freeware routine is applied to the case of "Brugada Syndrome," a specific disease and field of research (since 1992). The journals containing these publications, for example, are attributed to Web-of-Science Categories other than "Cardiac and Cardiovascular Systems"), perhaps because of the possibility of genetic testing for this syndrome in the clinic. With this routine, all the instruments available for citation analysis can now be used on the basis of MeSH terms. Other options for crossing between Medline, WoS, and Scopus are also reviewed.</data>
    </node>
    <node id="P6644">
      <data key="title">a review of the characteristics of 108 author level bibliometric indicators</data>
      <data key="abstract">An increasing demand for bibliometric assessment of individuals has led to a growth of new bibliometric indicators as well as new variants or combinations of established ones. The aim of this review is to contribute with objective facts about the usefulness of bibliometric indicators of the effects of publication activity at the individual level. This paper reviews 108 indicators that can potentially be used to measure performance on individual author-level, and examines the complexity of their calculations in relation to what they are supposed to reflect and ease of end-user application. As such we provide a schematic overview of author-level indicators, where the indicators are broadly categorised into indicators of publication count, indicators that qualify output (on the level of the researcher and journal), indicators of the effect of output (effect as citations, citations normalized to field or the researcher's body of work), indicators that rank the individual's work and indicators of impact over time. Supported by an extensive appendix we present how the indicators are computed, the complexity of the mathematical calculation and demands to data-collection, their advantages and limitations as well as references to surrounding discussion in the bibliometric community. The Appendix supporting this study is available online as supplementary material.</data>
    </node>
    <node id="P132580">
      <data key="title">excellence networks in science a web based application based on bayesian multilevel logistic regression bmlr for the identification of institutions collaborating successfully</data>
      <data key="abstract">In this study we present an application which can be accessed via www.excellence-networks.net and which represents networks of scientific institutions worldwide. The application is based on papers (articles, reviews and conference papers) published between 2007 and 2011. It uses (network) data, on which the SCImago Institutions Ranking is based (Scopus data from Elsevier). Using this data, institutional networks have been estimated with statistical models (Bayesian multilevel logistic regression, BMLR) for a number of Scopus subject areas. Within single subject areas, we have investigated and visualized how successfully overall an institution (reference institution) has collaborated (compared to all the other institutions in a subject area), and with which other institutions (network institutions) a reference institution has collaborated particularly successfully. The "best paper rate" (statistically estimated) was used as an indicator for evaluating the collaboration success of an institution. This gives the proportion of highly cited papers from an institution, and is considered generally as an indicator for measuring impact in bibliometrics.</data>
    </node>
    <node id="P14727">
      <data key="title">climate change research in view of bibliometrics</data>
      <data key="abstract">This bibliometric study of a large publication set dealing with research on climate change aims at mapping the relevant literature from a bibliometric perspective and presents a multitude of quantitative data: (1) The growth of the overall publication output as well as (2) of some major subfields, (3) the contributing journals and countries as well as their citation impact, and (4) a title word analysis aiming to illustrate the time evolution and relative importance of specific research topics. The study is based on 222,060 papers (articles and reviews only) published between 1980 and 2014. The total number of papers shows a strong increase with a doubling every 5–6 years. Continental biomass related research is the major subfield, closely followed by climate modeling. Research dealing with adaptation, mitigation, risks, and vulnerability of global warming is comparatively small, but their share of papers increased exponentially since 2005. Research on vulnerability and on adaptation published the largest proportion of very important papers (in terms of citation impact). Climate change research has become an issue also for disciplines beyond the natural sciences. The categories Engineering and Social Sciences show the strongest field-specific relative increase. The Journal of Geophysical Research, the Journal of Climate, the Geophysical Research Letters, and Climatic Change appear at the top positions in terms of the total number of papers published. Research on climate change is quantitatively dominated by the USA, followed by the UK, Germany, and Canada. The citation-based indicators exhibit consistently that the UK has produced the largest proportion of high impact papers compared to the other countries (having published more than 10,000 papers). Also, Switzerland, Denmark and also The Netherlands (with a publication output between around 3,000 and 6,000 papers) perform top—the impact of their contributions is on a high level. The title word analysis shows that the term climate change comes forward with time. Furthermore, the term impact arises and points to research dealing with the various effects of climate change. The discussion of the question of human induced climate change towards a clear fact (for the majority of the scientific community) stimulated research on future pathways for adaptation and mitigation. Finally, the term model and related terms prominently appear independent of time, indicating the high relevance of climate modeling.</data>
    </node>
    <node id="P3504">
      <data key="title">the use of percentiles and percentile rank classes in the analysis of bibliometric data opportunities and limits</data>
      <data key="abstract">Percentiles have been established in bibliometrics as an important alternative to mean-based indicators for obtaining a normalized citation impact of publications. Percentiles have a number of advantages over standard bibliometric indicators used frequently: for example, their calculation is not based on the arithmetic mean which should not be used for skewed bibliometric data. This study describes the opportunities and limits and the advantages and disadvantages of using percentiles in bibliometrics. We also address problems in the calculation of percentiles and percentile rank classes for which there is not (yet) a satisfactory solution. It will be hard to compare the results of different percentile-based studies with each other unless it is clear that the studies were done with the same choices for percentile calculation and rank assignment.</data>
    </node>
    <edge source="P141" target="P161639">
      <data key="relation">reference</data>
    </edge>
    <edge source="P141" target="P100725">
      <data key="relation">reference</data>
    </edge>
    <edge source="P141" target="P3504">
      <data key="relation">reference</data>
    </edge>
    <edge source="P141" target="P151774">
      <data key="relation">reference</data>
    </edge>
    <edge source="P141" target="P54132">
      <data key="relation">reference</data>
    </edge>
    <edge source="P141" target="P67364">
      <data key="relation">reference</data>
    </edge>
    <edge source="P141" target="P43525">
      <data key="relation">reference</data>
    </edge>
    <edge source="P141" target="P76634">
      <data key="relation">reference</data>
    </edge>
    <edge source="P141" target="P110892">
      <data key="relation">reference</data>
    </edge>
    <edge source="P53346" target="P161639">
      <data key="relation">reference</data>
    </edge>
    <edge source="P53346" target="P100725">
      <data key="relation">reference</data>
    </edge>
    <edge source="P53346" target="P3504">
      <data key="relation">reference</data>
    </edge>
    <edge source="P53346" target="P129901">
      <data key="relation">reference</data>
    </edge>
    <edge source="P84380" target="P54536">
      <data key="relation">reference</data>
    </edge>
    <edge source="P84380" target="P161639">
      <data key="relation">reference</data>
    </edge>
    <edge source="P84380" target="P132580">
      <data key="relation">reference</data>
    </edge>
    <edge source="P84380" target="P129901">
      <data key="relation">reference</data>
    </edge>
    <edge source="P84380" target="P3504">
      <data key="relation">reference</data>
    </edge>
    <edge source="P84380" target="P54132">
      <data key="relation">reference</data>
    </edge>
    <edge source="P84380" target="P76634">
      <data key="relation">reference</data>
    </edge>
    <edge source="P129901" target="P161639">
      <data key="relation">reference</data>
    </edge>
    <edge source="P161639" target="P54536">
      <data key="relation">reference</data>
    </edge>
    <edge source="P161639" target="P80356">
      <data key="relation">reference</data>
    </edge>
    <edge source="P161639" target="P67364">
      <data key="relation">reference</data>
    </edge>
    <edge source="P161639" target="P43525">
      <data key="relation">reference</data>
    </edge>
    <edge source="P161639" target="P6644">
      <data key="relation">reference</data>
    </edge>
    <edge source="P161639" target="P100725">
      <data key="relation">reference</data>
    </edge>
    <edge source="P161639" target="P20082">
      <data key="relation">reference</data>
    </edge>
    <edge source="P161639" target="P132580">
      <data key="relation">reference</data>
    </edge>
    <edge source="P161639" target="P14727">
      <data key="relation">reference</data>
    </edge>
    <edge source="P161639" target="P65313">
      <data key="relation">reference</data>
    </edge>
    <edge source="P161639" target="P3504">
      <data key="relation">reference</data>
    </edge>
    <edge source="P161639" target="P54132">
      <data key="relation">reference</data>
    </edge>
    <edge source="P161639" target="P76634">
      <data key="relation">reference</data>
    </edge>
    <edge source="P151774" target="P20082">
      <data key="relation">reference</data>
    </edge>
    <edge source="P54132" target="P3504">
      <data key="relation">reference</data>
    </edge>
    <edge source="P54536" target="P3504">
      <data key="relation">reference</data>
    </edge>
    <edge source="P54536" target="P67364">
      <data key="relation">reference</data>
    </edge>
    <edge source="P54536" target="P43525">
      <data key="relation">reference</data>
    </edge>
    <edge source="P76634" target="P132580">
      <data key="relation">reference</data>
    </edge>
    <edge source="P76634" target="P100725">
      <data key="relation">reference</data>
    </edge>
    <edge source="P76634" target="P3504">
      <data key="relation">reference</data>
    </edge>
    <edge source="P76634" target="P67364">
      <data key="relation">reference</data>
    </edge>
    <edge source="P76634" target="P43525">
      <data key="relation">reference</data>
    </edge>
    <edge source="P100725" target="P67364">
      <data key="relation">reference</data>
    </edge>
    <edge source="P43525" target="P67364">
      <data key="relation">reference</data>
    </edge>
    <edge source="P67364" target="P3504">
      <data key="relation">reference</data>
    </edge>
    <edge source="P65313" target="P3504">
      <data key="relation">reference</data>
    </edge>
  </graph>
</graphml>
