<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="relation" for="edge" attr.name="relation" attr.type="string" />
  <key id="abstract" for="node" attr.name="abstract" attr.type="string" />
  <key id="title" for="node" attr.name="title" attr.type="string" />
  <graph edgedefault="undirected">
    <node id="P298">
      <data key="title">image edge detection based on swarm intelligence using memristive networks</data>
      <data key="abstract">Recent advancements in the development of memristive devices has opened new opportunities for hardware implementation of non-Boolean computing. To this end, the suitability of memristive devices for swarm intelligence algorithms has enabled researchers to solve a maze in hardware. In this paper, we utilize swarm intelligence of memristive networks to perform image edge detection. First, we propose a hardware-friendly algorithm for image edge detection based on ant colony. Second, we implement the image edge detection algorithm using memristive networks. Furthermore, we explain the impact of various parameters of the memristors on the efficacy of the implementation. Our results show 28% improvement in the energy compared to a low power CMOS hardware implementation based on stochastic circuits. Furthermore, our design occupies up to 5x less area.</data>
    </node>
    <node id="P125727">
      <data key="title">applicability of well established memristive models for simulations of resistive switching devices</data>
      <data key="abstract">Highly accurate and predictive models of resistive switching devices are needed to enable future memory and logic design. Widely used is the memristive modeling approach considering resistive switches as dynamical systems. Here we introduce three evaluation criteria for memristor models, checking for plausibility of the I-V characteristics, the presence of a sufficiently nonlinearity of the switching kinetics, and the feasibility of predicting the behavior of two antiserially connected devices correctly. We analyzed two classes of models: the first class comprises common linear memristor models and the second class widely used nonlinear memristive models. The linear memristor models are based on Strukov's initial memristor model extended by different window functions, while the nonlinear models include Pickett's physics-based memristor model and models derived thereof. This study reveals lacking predictivity of the first class of models, independent of the applied window function. Only the physics-based model is able to fulfill most of the basic evaluation criteria.</data>
    </node>
    <node id="P16303">
      <data key="title">mixed precision in memory computing</data>
      <data key="abstract">As complementary metal–oxide–semiconductor (CMOS) scaling reaches its technological limits, a radical departure from traditional von Neumann systems, which involve separate processing and memory units, is needed in order to extend the performance of today’s computers substantially. In-memory computing is a promising approach in which nanoscale resistive memory devices, organized in a computational memory unit, are used for both processing and memory. However, to reach the numerical accuracy typically required for data analytics and scientific computing, limitations arising from device variability and non-ideal device characteristics need to be addressed. Here we introduce the concept of mixed-precision in-memory computing, which combines a von Neumann machine with a computational memory unit. In this hybrid system, the computational memory unit performs the bulk of a computational task, while the von Neumann machine implements a backward method to iteratively improve the accuracy of the solution. The system therefore benefits from both the high precision of digital computing and the energy/areal efficiency of in-memory computing. We experimentally demonstrate the efficacy of the approach by accurately solving systems of linear equations, in particular, a system of 5,000 equations using 998,752 phase-change memory devices.</data>
    </node>
    <node id="P1802">
      <data key="title">accelerating deep learning with memcomputing</data>
      <data key="abstract">Restricted Boltzmann machines (RBMs) and their extensions, often called "deep-belief networks", are very powerful neural networks that have found widespread applicability in the fields of machine learning and big data. The standard way to training these models resorts to an iterative unsupervised procedure based on Gibbs sampling, called "contrastive divergence", and additional supervised tuning via back-propagation. However, this procedure has been shown not to follow any gradient and can lead to suboptimal solutions. In this paper, we show a very efficient alternative to contrastive divergence by means of simulations of digital memcomputing machines (DMMs). We test our approach on pattern recognition using the standard MNIST data set of hand-written numbers. DMMs sample very effectively the vast phase space defined by the probability distribution of RBMs over the test sample inputs, and provide a very good approximation close to the optimum. This efficient search significantly reduces the number of generative pre-training iterations necessary to achieve a given level of accuracy in the MNIST data set, as well as a total performance gain over the traditional approaches. In fact, the acceleration of the pre-training achieved by simulating DMMs is comparable to, in number of iterations, the recently reported hardware application of the quantum annealing method on the same network and data set. Notably, however, DMMs perform far better than the reported quantum annealing results in terms of quality of the training. Our approach is agnostic about the connectivity of the network. Therefore, it can be extended to train full Boltzmann machines, and even deep networks at once.</data>
    </node>
    <node id="P29226">
      <data key="title">slime mould memristors</data>
      <data key="abstract">In laboratory experiments we demonstrate that protoplasmic tubes of acellular slime mould \emph{Physarum polycephalum} show current versus voltage profiles consistent with memristive systems and that the effect is due to the living protoplasm of the mould. This complements previous findings on memristive properties of other living systems (human skin and blood) and contributes to development of self-growing bio-electronic circuits. Distinctive asymmetric V-I curves which were occasionally observed when the internal current is on the same order as the driven current, are well-modelled by the concept of active memristors.</data>
    </node>
    <node id="P2775">
      <data key="title">universal memcomputing machines</data>
      <data key="abstract">We introduce the notion of  universal memcomputing machines  (UMMs): a class of brain-inspired general-purpose computing machines based on systems with memory, whereby processing and storing of information occur on the same physical location. We analytically prove that the memory properties of UMMs endow them with universal computing power (they are Turing-complete),  intrinsic parallelism, functional polymorphism, and information overhead , namely, their collective states can support exponential data compression directly in memory. We also demonstrate that a UMM has the same computational power as a nondeterministic Turing machine, namely, it can solve nondeterministic polynomial (NP)-complete problems in polynomial time. However, by virtue of its information overhead, a UMM needs only an amount of memory cells (memprocessors) that grows polynomially with the problem size. As an example, we provide the polynomial-time solution of the subset-sum problem and a simple hardware implementation of the same. Even though these results do not prove the statement NP = P within the Turing paradigm, the practical realization of these UMMs would represent a paradigm shift from the present von Neumann architectures, bringing us closer to brain-like neural computation.</data>
    </node>
    <node id="P162041">
      <data key="title">perspectives of using oscillators for computing and signal processing</data>
      <data key="abstract">It is an intriguing concept to use oscillators as fundamental building blocks of electronic computers. The idea is not new, but is currently subject to intense research as a part of the quest for 'beyond Moore' electronic devices. In this paper we give an engineering-minded survey of oscillator-based computing architectures, with the goal of understanding their promise and limitations for next-generation computing. We will mostly discuss non-Boolean, neurally-inspired computing concepts and put the emphasis on hardware and on circuits where the oscillators are realized from emerging, nanoscale building blocks. Despite all the promise that oscillatory computing holds, existing literature gives very few clear-cut arguments about the possible benefits of using oscillators in place of other analog nonlinear circuit elements. In this survey we will argue for finding the rationale of using oscillatory building blocks and call for benchmarking studies that compare oscillatory computing circuits to level-based (analog) implementations.</data>
    </node>
    <node id="P69064">
      <data key="title">memcomputing and swarm intelligence</data>
      <data key="abstract">We explore the relation between memcomputing, namely computing with and in memory, and swarm intelligence algorithms. In particular, we show that one can design memristive networks to solve short-path optimization problems that can also be solved by ant-colony algorithms. By employing appropriate memristive elements one can demonstrate an almost one-to-one correspondence between memcomputing and ant colony optimization approaches. However, the memristive network has the capability of finding the solution in one deterministic step, compared to the stochastic multi-step ant colony optimization. This result paves the way for nanoscale hardware implementations of several swarm intelligence algorithms that are presently explored, from scheduling problems to robotics.</data>
    </node>
    <node id="P78646">
      <data key="title">on the universality of memcomputing machines</data>
      <data key="abstract">Universal memcomputing machines (UMMs) [IEEE Trans. Neural Netw. Learn. Syst. 26, 2702 (2015)] represent a novel computational model in which memory (time non-locality) accomplishes both tasks of storing and processing of information. UMMs have been shown to be Turing-complete, namely they can simulate any Turing machine. In this paper, using set theory and cardinality arguments, we compare them with liquid-state machines (or "reservoir computing") and quantum machines ("quantum computing"). We show that UMMs can simulate both types of machines, hence they are both "liquid-" or "reservoir-complete" and "quantum-complete". Of course, these statements pertain only to the type of problems these machines can solve, and not to the amount of resources required for such simulations. Nonetheless, the method presented here provides a general framework in which to describe the relation between UMMs and any other type of computational model.</data>
    </node>
    <node id="P47112">
      <data key="title">a scalable method to find the shortest path in a graph with circuits of memristors</data>
      <data key="abstract">Finding the shortest path in a graph has applications to a wide range of optimization problems. However, algorithmic methods scale with the size of the graph in terms of time and energy. We propose a method to solve the shortest path problem using circuits of nanodevices called memristors and validate it on graphs of different sizes and topologies. It is both valid for an experimentally derived memristor model and robust to device variability. The time and energy of the computation scale with the length of the shortest path rather than with the size of the graph, making this method particularly attractive for solving large graphs with small path lengths.</data>
    </node>
    <node id="P50152">
      <data key="title">transient dynamics of pulse driven memristors in the presence of a stable fixed point</data>
      <data key="abstract">Some memristors are quite interesting from the point of view of dynamical systems. When driven by narrow pulses of alternating polarities, their dynamics has a stable fixed point, which may be useful for future applications. We study the transient dynamics of two types of memristors characterized by a stable fixed point using a time-averaged evolution equation. Time-averaged trajectories of the Biolek window function memristor and resistor-threshold type memristor circuit (an effective memristor) are determined analytically, and the times of relaxation to the stable fixed point are found. Our analytical results are in perfect agreement with the results of numerical simulations.</data>
    </node>
    <node id="P159743">
      <data key="title">memristors for the curious outsiders</data>
      <data key="abstract">We present both an overview and a perspective of recent experimental advances and proposed new approaches to performing computation using memristors. A memristor is a 2-terminal passive component with a dynamic resistance depending on an internal parameter. We provide an brief historical introduction, as well as an overview over the physical mechanism that lead to memristive behavior. This review is meant to guide nonpractitioners in the field of memristive circuits and their connection to machine learning and neural computation.</data>
    </node>
    <node id="P100717">
      <data key="title">memcomputing np complete problems in polynomial time using polynomial resources and collective states</data>
      <data key="abstract">Memcomputing is a novel non-Turing paradigm of computation that uses interacting memory cells (memprocessors for short) to store and process information on the same physical platform. It was recently proven mathematically that memcomputing machines have the same computational power of nondeterministic Turing machines. Therefore, they can solve NP-complete problems in polynomial time and, using the appropriate architecture, with resources that only grow polynomially with the input size. The reason for this computational power stems from properties inspired by the brain and shared by any universal memcomputing machine, in particular intrinsic parallelism and information overhead, namely, the capability of compressing information in the collective state of the memprocessor network. We show an experimental demonstration of an actual memcomputing architecture that solves the NP-complete version of the subset sum problem in only one step and is composed of a number of memprocessors that scales linearly with the size of the problem. We have fabricated this architecture using standard microelectronic technology so that it can be easily realized in any laboratory setting. Although the particular machine presented here is eventually limited by noise—and will thus require error-correcting codes to scale to an arbitrary number of memprocessors—it represents the first proof of concept of a machine capable of working with the collective state of interacting memory cells, unlike the present-day single-state machines built using the von Neumann architecture.</data>
    </node>
    <node id="P146579">
      <data key="title">memcomputing numerical inversion with self organizing logic gates</data>
      <data key="abstract">We propose to use digital memcomputing machines (DMMs), implemented with self-organizing logic gates (SOLGs), to solve the problem of numerical inversion. Starting from fixed-point scalar inversion, we describe the generalization to solving linear systems and matrix inversion. This method, when realized in hardware, will output the result in only one computational step. As an example, we perform simulations of the scalar case using a 5-bit logic circuit made of SOLGs, and show that the circuit successfully performs the inversion. Our method can be extended efficiently to any level of precision, since we prove that producing    $n$   -bit precision in the output requires extending the circuit by at most    $n$    bits. This type of numerical inversion can be implemented by DMM units in hardware; it is scalable, and thus of great benefit to any real-time computing application.</data>
    </node>
    <node id="P139036">
      <data key="title">dynamic computing random access memory</data>
      <data key="abstract">The present von Neumann computing paradigm involves a significant amount of information transfer between a central processing unit and memory, with concomitant limitations in the actual execution speed. However, it has been recently argued that a different form of computation, dubbed memcomputing (Di Ventra and Pershin 2013 Nat. Phys.  9 200?2) and inspired by the operation of our brain, can resolve the intrinsic limitations of present day architectures by allowing for computing and storing of information on the same physical platform. Here we show a simple and practical realization of memcomputing that utilizes easy-to-build memcapacitive systems. We name this architecture dynamic computing random access memory (DCRAM). We show that DCRAM provides massively-parallel and polymorphic digital logic, namely it allows for different logic operations with the same architecture, by varying only the control signals. In addition, by taking into account realistic parameters, its energy expenditures can be as low as a few fJ per operation. DCRAM is fully compatible with CMOS technology, can be realized with current fabrication facilities, and therefore can really serve as an alternative to the present computing technology.</data>
    </node>
    <node id="P111273">
      <data key="title">self organization and solution of shortest path optimization problems with memristive networks</data>
      <data key="abstract">We show that memristive networks-namely networks of resistors with memory-can efficiently solve shortest-path optimization problems. Indeed, the presence of memory (time non-locality) promotes self organization of the network into the shortest possible path(s). We introduce a network entropy function to characterize the self-organized evolution, show the solution of the shortest-path problem and demonstrate the healing property of the solution path. Finally, we provide an algorithm to solve the traveling salesman problem. Similar considerations apply to networks of memcapacitors and meminductors, and networks with memory in various dimensions.</data>
    </node>
    <node id="P30909">
      <data key="title">an fpga based massively parallel neuromorphic cortex simulator</data>
      <data key="abstract">This paper presents a massively parallel and scalable neuromorphic cortex simulator designed for simulating large and structurally connected spiking neural networks, such as complex models of various areas of the cortex. The main novelty of this work is the abstraction of a neuromorphic architecture into clusters represented by minicolumns and hypercolumns, analogously to the fundamental structural units observed in neurobiology. Without this approach, simulating large-scale fully connected networks needs prohibitively large memory to store look-up tables for point-to-point connections. Instead, we use a novel architecture, based on the structural connectivity in the neocortex, such that all the required parameters and connections can be stored in on-chip memory. The cortex simulator can be easily reconfigured for simulating different neural networks without any change in hardware structure by programming the memory. A hierarchical communication scheme allows one neuron to have a fan-out of up to 200k neurons. As a proof-of-concept, an implementation on one Altera Stratix V FPGA was able to simulate 20 million to 2.6 billion leaky-integrate-and-fire (LIF) neurons in real time. We verified the system by emulating a simplified auditory cortex (with 100 million neurons). This cortex simulator achieved a low power dissipation of 1.62 {\mu}W per neuron. With the advent of commercially available FPGA boards, our system offers an accessible and scalable tool for the design, real-time simulation, and analysis of large-scale spiking neural networks.</data>
    </node>
    <node id="P142069">
      <data key="title">in memory computing on a photonic platform</data>
      <data key="abstract">Collocated data processing and storage are the norm in biological systems. Indeed, the von Neumann computing architecture, that physically and temporally separates processing and memory, was born more of pragmatism based on available technology. As our ability to create better hardware improves, new computational paradigms are being explored. Integrated photonic circuits are regarded as an attractive solution for on-chip computing using only light, leveraging the increased speed and bandwidth potential of working in the optical domain, and importantly, removing the need for time and energy sapping electro-optical conversions. Here we show that we can combine the emerging area of integrated optics with collocated data storage and processing to enable all-photonic in-memory computations. By employing non-volatile photonic elements based on the phase-change material, Ge2Sb2Te5, we are able to achieve direct scalar multiplication on single devices. Featuring a novel single-shot Write/Erase and a drift-free process, such elements can multiply two scalar numbers by mapping their values to the energy of an input pulse and to the transmittance of the device, codified in the crystallographic state of the element. The output pulse, carrying the information of the light-matter interaction, is the result of the computation. Our all-optical approach is novel, easy to fabricate and operate, and sets the stage for development of entirely photonic computers.</data>
    </node>
    <node id="P104686">
      <data key="title">polynomial time solution of prime factorization and np complete problems with digital memcomputing machines</data>
      <data key="abstract">We introduce a class of digital machines, we name Digital Memcomputing Machines, (DMMs) able to solve a wide range of problems including Non-deterministic Polynomial (NP) ones with polynomial resources (in time, space, and energy). An abstract DMM with this power must satisfy a set of compatible mathematical constraints underlying its practical realization. We prove this by making a connection with the dynamical systems theory. This leads us to a set of physical constraints for poly-resource resolvability. Once the mathematical requirements have been assessed, we propose a practical scheme to solve the above class of problems based on the novel concept of self-organizing logic gates and circuits (SOLCs). These are logic gates and circuits able to accept input signals from any terminal, without distinction between conventional input and output terminals. They can solve boolean problems by self-organizing into their solution. They can be fabricated either with circuit elements with memory (such as memristor...</data>
    </node>
    <node id="P20016">
      <data key="title">the parallel approach</data>
      <data key="abstract">A class of two-terminal passive circuit elements that can also act as memories could be the building blocks of a form of massively parallel computation known as memcomputing.</data>
    </node>
    <edge source="P298" target="P69064">
      <data key="relation">reference</data>
    </edge>
    <edge source="P298" target="P47112">
      <data key="relation">reference</data>
    </edge>
    <edge source="P125727" target="P20016">
      <data key="relation">reference</data>
    </edge>
    <edge source="P16303" target="P20016">
      <data key="relation">reference</data>
    </edge>
    <edge source="P16303" target="P2775">
      <data key="relation">reference</data>
    </edge>
    <edge source="P16303" target="P142069">
      <data key="relation">reference</data>
    </edge>
    <edge source="P1802" target="P78646">
      <data key="relation">reference</data>
    </edge>
    <edge source="P1802" target="P20016">
      <data key="relation">reference</data>
    </edge>
    <edge source="P1802" target="P2775">
      <data key="relation">reference</data>
    </edge>
    <edge source="P1802" target="P104686">
      <data key="relation">reference</data>
    </edge>
    <edge source="P1802" target="P146579">
      <data key="relation">reference</data>
    </edge>
    <edge source="P29226" target="P47112">
      <data key="relation">reference</data>
    </edge>
    <edge source="P29226" target="P159743">
      <data key="relation">reference</data>
    </edge>
    <edge source="P2775" target="P69064">
      <data key="relation">reference</data>
    </edge>
    <edge source="P2775" target="P20016">
      <data key="relation">reference</data>
    </edge>
    <edge source="P2775" target="P139036">
      <data key="relation">reference</data>
    </edge>
    <edge source="P2775" target="P111273">
      <data key="relation">reference</data>
    </edge>
    <edge source="P2775" target="P100717">
      <data key="relation">reference</data>
    </edge>
    <edge source="P2775" target="P104686">
      <data key="relation">reference</data>
    </edge>
    <edge source="P2775" target="P78646">
      <data key="relation">reference</data>
    </edge>
    <edge source="P2775" target="P142069">
      <data key="relation">reference</data>
    </edge>
    <edge source="P2775" target="P146579">
      <data key="relation">reference</data>
    </edge>
    <edge source="P2775" target="P159743">
      <data key="relation">reference</data>
    </edge>
    <edge source="P162041" target="P20016">
      <data key="relation">reference</data>
    </edge>
    <edge source="P162041" target="P100717">
      <data key="relation">reference</data>
    </edge>
    <edge source="P69064" target="P20016">
      <data key="relation">reference</data>
    </edge>
    <edge source="P69064" target="P111273">
      <data key="relation">reference</data>
    </edge>
    <edge source="P78646" target="P20016">
      <data key="relation">reference</data>
    </edge>
    <edge source="P78646" target="P100717">
      <data key="relation">reference</data>
    </edge>
    <edge source="P78646" target="P104686">
      <data key="relation">reference</data>
    </edge>
    <edge source="P47112" target="P111273">
      <data key="relation">reference</data>
    </edge>
    <edge source="P47112" target="P30909">
      <data key="relation">reference</data>
    </edge>
    <edge source="P50152" target="P20016">
      <data key="relation">reference</data>
    </edge>
    <edge source="P50152" target="P111273">
      <data key="relation">reference</data>
    </edge>
    <edge source="P159743" target="P20016">
      <data key="relation">reference</data>
    </edge>
    <edge source="P159743" target="P111273">
      <data key="relation">reference</data>
    </edge>
    <edge source="P159743" target="P100717">
      <data key="relation">reference</data>
    </edge>
    <edge source="P159743" target="P104686">
      <data key="relation">reference</data>
    </edge>
    <edge source="P100717" target="P20016">
      <data key="relation">reference</data>
    </edge>
    <edge source="P100717" target="P139036">
      <data key="relation">reference</data>
    </edge>
    <edge source="P100717" target="P104686">
      <data key="relation">reference</data>
    </edge>
    <edge source="P100717" target="P146579">
      <data key="relation">reference</data>
    </edge>
    <edge source="P146579" target="P20016">
      <data key="relation">reference</data>
    </edge>
    <edge source="P139036" target="P20016">
      <data key="relation">reference</data>
    </edge>
    <edge source="P139036" target="P104686">
      <data key="relation">reference</data>
    </edge>
    <edge source="P142069" target="P20016">
      <data key="relation">reference</data>
    </edge>
    <edge source="P104686" target="P20016">
      <data key="relation">reference</data>
    </edge>
  </graph>
</graphml>
