<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="relation" for="edge" attr.name="relation" attr.type="string" />
  <key id="abstract" for="node" attr.name="abstract" attr.type="string" />
  <key id="title" for="node" attr.name="title" attr.type="string" />
  <graph edgedefault="undirected">
    <node id="P83777">
      <data key="title">the bethe permanent of a nonnegative matrix</data>
      <data key="abstract">It has recently been observed that the permanent of a nonnegative square matrix, i.e., of a square matrix containing only nonnegative real entries, can very well be approximated by solving a certain Bethe free energy function minimization problem with the help of the sum-product algorithm. We call the resulting approximation of the permanent the Bethe permanent. In this paper, we give reasons why this approach to approximating the permanent works well. Namely, we show that the Bethe free energy function is convex and that the sum-product algorithm finds its minimum efficiently. We then discuss the fact that the permanent is lower bounded by the Bethe permanent, and we comment on potential upper bounds on the permanent based on the Bethe permanent. We also present a combinatorial characterization of the Bethe permanent in terms of permanents of so-called lifted versions of the matrix under consideration. Moreover, we comment on possibilities to modify the Bethe permanent so that it approximates the permanent even better, and we conclude the paper with some observations and conjectures about permanent-based pseudocodewords and permanent-based kernels.</data>
    </node>
    <node id="P18610">
      <data key="title">the capacity of finite state channels in the high noise regime</data>
      <data key="abstract">This paper considers the derivative of the entropy rate of a hidden Markov process with respect to the observation probabilities. The main result is a compact formula for the derivative that can be evaluated easily using Monte Carlo methods. It is applied to the problem of computing the capacity of a finite-state channel (FSC) and, in the high-noise regime, the formula has a simple closed-form expression that enables series expansion of the capacity of a FSC. This expansion is evaluated for a binary-symmetric channel under a (0,1) run-length limited constraint and an intersymbol-interference channel with Gaussian noise.</data>
    </node>
    <node id="P11872">
      <data key="title">on the performance of short block codes over finite state channels in the rare transition regime</data>
      <data key="abstract">As the mobile application landscape expands, wireless networks are tasked with supporting different connection profiles, including real-time traffic and delay-sensitive communications. Among many ensuing engineering challenges is the need to better understand the fundamental limits of forward error correction in non-asymptotic regimes. This article characterizes the performance of random block codes over finite-state channels and evaluates their queueing performance under maximum-likelihood decoding. In particular, classical results from information theory are revisited in the context of channels with rare transitions, and bounds on the probabilities of decoding failure are derived for random codes. This creates an analysis framework where channel dependencies within and across codewords are preserved. Such results are subsequently integrated into a queueing problem formulation. For instance, it is shown that, for random coding on the Gilbert-Elliott channel, the performance analysis based on upper bounds on error probability provides very good estimates of system performance and optimum code parameters. Overall, this study offers new insights about the impact of channel correlation on the performance of delay-aware, point-to-point communication links. It also provides novel guidelines on how to select code rates and block lengths for real-time traffic over wireless communication infrastructures.</data>
    </node>
    <node id="P163752">
      <data key="title">asymptotics of entropy rate in special families of hidden markov chains</data>
      <data key="abstract">We derive an asymptotic formula for entropy rate of a hidden Markov chain around a "weak Black Hole". We also discuss applications of the asymptotic formula to the asymptotic behaviors of certain channels.</data>
    </node>
    <node id="P83594">
      <data key="title">a note on the strong data processing inequalities in bayesian networks</data>
      <data key="abstract">The data-processing inequality, that is, $I(U;Y) \le I(U;X)$ for a Markov chain $U \to X \to Y$, has been the method of choice for proving impossibility (converse) results in information theory and many other disciplines. Various channel-dependent strengthenings of this inequality have been proposed both classically and more recently. This note considers the basic information-theoretic question: given strong data-processing inequality (SDPI) for each constituent channel in a Bayesian network, how does one produce an end-to-end SDPI? #R##N#Our approach is based on the (extract of the) Evans-Schulman method, which is demonstrated for three different kinds of SDPIs, namely, the usual Ahslwede-G\'acs type contraction coefficients (mutual information), Dobrushin's contraction coefficients (total variation), and finally the $F_I$-curve (the best possible SDPI for a given channel). As an example, it is shown how to obtain SDPI for an $n$-letter memoryless channel with feedback given an SDPI for $n=1$.</data>
    </node>
    <node id="P62475">
      <data key="title">the entropy of a binary hidden markov process</data>
      <data key="abstract">The entropy of a binary symmetric Hidden Markov Process is calculated as an expansion in the noise parameter e. We map the problem onto a one-dimensional Ising model in a large field of random signs and calculate the expansion coefficients up to second order in e. Using a conjecture we extend the calculation to 11th order and discuss the convergence of the resulting series</data>
    </node>
    <node id="P58130">
      <data key="title">on continuous time gaussian channels</data>
      <data key="abstract">We establish natural connections between continuous-time Gaussian feedback/memory channels and their associated discrete-time versions in the forms of sampling and approximating theorems. It turns out that these connections, together with relevant tools from stochastic calculus, can enhance our understanding of continuous-time Gaussian channels in terms of giving alternative interpretations to some long-held "folklores", recovering known results from new perspectives, and obtaining new results inspired by the insights and ideas that come along with the connections. In particular, we derive the capacity regions of a continuous-time white Gaussian multiple access channel, a continuous-time white Gaussian interference channel, and a continuous-time white Gaussian broadcast channel, furthermore, applying the the sampling and approximating theorems and the ideas and techniques in their proofs, we analyze how feedback affects the capacity regions of families of continuous-time multi-user one-hop Gaussian channels: feedback will increase the capacity regions of some continuous-time white Gaussian broadcast and interference channels, while it will not increase capacity regions of continuous-time white Gaussian multiple access channels.</data>
    </node>
    <node id="P92359">
      <data key="title">delay sensitive communication over fading channel queueing behavior and code parameter selection</data>
      <data key="abstract">This article examines the queueing performance of communication systems that transmit encoded data over unreliable channels. A fading formulation suitable for wireless environments is considered where errors are caused by a discrete channel with correlated behavior over time. Random codes and BCH codes are employed as means to study the relationship between code-rate selection and the queueing performance of point-to-point data links. For carefully selected channel models and arrival processes, a tractable Markov structure composed of queue length and channel state is identified. This facilitates the analysis of the stationary behavior of the system, leading to evaluation criteria such as bounds on the probability of the queue exceeding a threshold. Specifically, this article focuses on system models with scalable arrival profiles, which are based on Poisson processes, and finite-state channels with memory. These assumptions permit the rigorous comparison of system performance for codes with arbitrary block lengths and code rates. Based on the resulting characterizations, it is possible to select the best code parameters for delay-sensitive applications over various channels. The methodology introduced herein offers a new perspective on the joint queueing-coding analysis of finitestate channels with memory, and it is supported by numerical simulations.</data>
    </node>
    <node id="P63335">
      <data key="title">limit theorems in hidden markov models</data>
      <data key="abstract">In this paper, under mild assumptions, we derive a law of large numbers, a central limit theorem with an error estimate, an almost sure invariance principle and a variant of Chernoff bound in finite-state hidden Markov models. These limit theorems are of interest in certain ares in statistics and information theory. Particularly, we apply the limit theorems to derive the rate of convergence of the maximum likelihood estimator in finite-state hidden Markov models.</data>
    </node>
    <node id="P147212">
      <data key="title">novel lower bounds on the entropy rate of binary hidden markov processes</data>
      <data key="abstract">Recently, Samorodnitsky proved a strengthened version of Mrs. Gerber’s Lemma, where the output entropy of a binary symmetric channel is bounded in terms of the average entropy of the input projected on a random subset of coordinates. Here, this result is applied for deriving novel lower bounds on the entropy rate of binary hidden Markov processes. For symmetric underlying Markov processes, our bound improves upon the best known bound in the very noisy regime. The nonsymmetric case is also considered, and explicit bounds are derived for Markov processes that satisfy the (1,∞)-RLL constraint.</data>
    </node>
    <node id="P13395">
      <data key="title">capacity analysis of discrete energy harvesting channels</data>
      <data key="abstract">We study the channel capacity of a general discrete energy harvesting channel with a finite battery. Contrary to traditional communication systems, the transmitter of such a channel is powered by a device that harvests energy from a random exogenous energy source and has a finite-sized battery. As a consequence, at each transmission opportunity the system can only transmit a symbol whose energy is no more than the energy currently available. This new type of power supply introduces an unprecedented input constraint for the channel, which is simultaneously random, instantaneous, and influenced by the full history of the inputs and the energy harvesting process. Furthermore, naturally, in such a channel the energy information is observed causally at the transmitter. Both of these characteristics pose great challenges for the analysis of the channel capacity. In this work we use techniques developed for channels with side information and finite state channels, to obtain lower and upper bounds on the capacity of energy harvesting channels. In particular, in a general case with Markov energy harvesting processes we use stationarity and ergodicity theory to compute and optimize the achievable rates for the channels, and derive series of computable capacity upper and lower bounds.</data>
    </node>
    <node id="P59998">
      <data key="title">strong data processing inequalities for channels and bayesian networks</data>
      <data key="abstract">The data-processing inequality, that is, I(U; Y ) ≤ I(U; X) for a Markov chain U → X → Y, has been the method of choice for proving impossibility (converse) results in information theory and many other disciplines. Various channel-dependent improvements (called strong data-processing inequalities, or SDPIs) of this inequality have been proposed both classically and more recently. In this note we first survey known results relating various notions of contraction for a single channel. Then we consider the basic extension: given SDPI for each constituent channel in a Bayesian network, how to produce an end-to-end SDPI?</data>
    </node>
    <node id="P51877">
      <data key="title">on the entropy of a noisy function</data>
      <data key="abstract">Let $0   0$. Namely, if $X$ is uniformly distributed in $\{0,1\}^n$ and $Y$ is obtained by flipping each coordinate of $X$ independently with probability $\epsilon$, then, provided $\epsilon \ge 1/2 - \delta$, for any boolean function $f$ holds $I\Big(f(X);Y\Big) \le 1 - H(\epsilon)$.</data>
    </node>
    <node id="P67607">
      <data key="title">computable lower bounds for capacities of input driven finite state channels</data>
      <data key="abstract">This paper studies the capacities of input-driven finite-state channels, i.e., channels whose current state is a time-invariant deterministic function of the previous state and the current input. We lower bound the capacity of such a channel using a dynamic programming formulation of a bound on the maximum reverse directed information rate. We show that the dynamic programming-based bounds can be simplified by solving the corresponding Bellman equation explicitly. In particular, we provide analytical lower bounds on the capacities of $(d, k)$-runlength-limited input-constrained binary symmetric and binary erasure channels. Furthermore, we provide a single-letter lower bound based on a class of input distributions with memory.</data>
    </node>
    <node id="P104668">
      <data key="title">stationary and transition probabilities in slow mixing long memory markov processes</data>
      <data key="abstract">We observe a length-$n$ sample generated by an unknown,stationary ergodic Markov process (\emph{model}) over a finite alphabet $\mathcal{A}$. Given any string $\bf{w}$ of symbols from $\mathcal{A}$ we want estimates of the conditional probability distribution of symbols following $\bf{w}$, as well as the stationary probability of $\bf{w}$. Two distinct problems that complicate estimation in this setting are (i) long memory, and (ii) \emph{slow mixing} which could happen even with only one bit of memory. #R##N#Any consistent estimator in this setting can only converge pointwise over the class of all ergodic Markov models. Namely, given any estimator and any sample size $n$, the underlying model could be such that the estimator performs poorly on a sample of size $n$ with high probability. But can we look at a length-$n$ sample and identify \emph{if} an estimate is likely to be accurate? #R##N#Since the memory is unknown \emph{a-priori}, a natural approach is to estimate a potentially coarser model with memory $k_n=\mathcal{O}(\log n)$. As $n$ grows, pointwise consistent estimates that hold eventually almost surely (eas) are known so long as the scaling of $k_n$ is not superlogarithmic in $n$. Here, rather than eas convergence results, we want the best answers possible with a length-$n$ sample. Combining results in universal compression with Aldous' coupling arguments, we obtain sufficient conditions on the length-$n$ sample (even for slow mixing models) to identify when naive (i) estimates of the conditional probabilities and (ii) estimates related to the stationary probabilities are accurate; and also bound the deviations of the naive estimates from true values.</data>
    </node>
    <node id="P47276">
      <data key="title">a randomized approach to the capacity of finite state channels</data>
      <data key="abstract">Inspired by the ideas from the field of stochastic approximation, we propose a randomized algorithm to compute the capacity of a finite-state channel with a Markovian input. When the mutual information rate of the channel is concave with respect to the chosen parameterization, we show that the proposed algorithm will almost surely converge to the capacity of the channel and derive the rate of convergence. We also discuss the convergence behavior of the algorithm without the concavity assumption.</data>
    </node>
    <node id="P47">
      <data key="title">entropy rate for hidden markov chains with rare transitions</data>
      <data key="abstract">We consider Hidden Markov Chains obtained by passing a Markov Chain with rare transitions through a noisy memoryless channel. We obtain asymptotic estimates for the entropy of the resulting Hidden Markov Chain as the transition rate is reduced to zero. Let (Xn) be a Markov chain with finite state space S and transition matrix P(p) and let (Yn) be the Hidden Markov chain observed by passing (Xn) through a homogeneous noisy memoryless channel (i.e. Y takes values in a set T, and there exists a matrix Q such that P(Yn = jjXn = i;X n−1 −1 ;X 1+1;Y n−1 −1 ;Y 1 n+1) = Qij). We make the additional assumption on the channel that the rows of Q are distinct. In this case we call the channel statistically distinguishing. We assume that P(p) is of the form I + pA where A is a matrix with negative entries on the diagonal, non-negative entries in the off-diagonal terms and zero row sums. We further assume that for small positive p, the Markov chain with transition matrix P(p) is irreducible. Notice that for Markov chains of this form, the invariant distribution (�i)i2 S does not depend on p. In this case, we say that for small positive values of p, the Markov chain is in a rare transition regime. We will adopt the convention that H is used to denote the entropy of a fi- nite partition, whereas h is used to denote the entropy of a process (the en- tropy rate in information theory terminology). Given an irreducible Markov chain with transition matrix P, we let h(P) be the entropy of the Markov chain (i.e. h(P) = − P i;jiPij logPij wherei is the (unique) invariant distribution of the Markov chain and as usual we adopt the convention that 0log0 = 0). We also let Hchan(i) be the entropy of the output of the channel when the input symbol is i (i.e. Hchan(i) = − P j2 T Qij logQij). Let h(Y ) denote the entropy of Y (i.e.</data>
    </node>
    <node id="P140360">
      <data key="title">entropy of hidden markov processes via cycle expansion</data>
      <data key="abstract">Hidden Markov Processes (HMP) is one of the basic tools of the modern probabilistic modeling. The characterization of their entropy remains however an open problem. Here the entropy of HMP is calculated via the cycle expansion of the zeta-function, a method adopted from the theory of dynamical systems. For a class of HMP this method produces exact results both for the entropy and the moment-generating function. The latter allows to estimate, via the Chernoff bound, the probabilities of large deviations for the HMP. More generally, the method offers a representation of the moment-generating function and of the entropy via convergent series.</data>
    </node>
    <node id="P54172">
      <data key="title">derivatives of entropy rate in special families of hidden markov chains</data>
      <data key="abstract">Consider a hidden Markov chain obtained as the observation process of an ordinary Markov chain corrupted by noise. Zuk, et. al. [13], [14] showed how, in principle, one can explicitly compute the derivatives of the entropy rate of at extreme values of the noise. Namely, they showed that the derivatives of standard upper approximations to the entropy rate actually stabilize at an explicit finite time. We generalize this result to a natural class of hidden Markov chains called ``Black Holes.'' We also discuss in depth special cases of binary Markov chains observed in binary symmetric noise, and give an abstract formula for the first derivative in terms of a measure on the simplex due to Blackwell.</data>
    </node>
    <node id="P79154">
      <data key="title">non asymptotic equipartition properties for independent and identically distributed sources</data>
      <data key="abstract">Given an independent and identically distributed source $X = \{X_i \}_{i=1}^{\infty}$ with finite Shannon entropy or differential entropy (as the case may be) $H(X)$, the non-asymptotic equipartition property (NEP) with respect to $H(X)$ is established, which characterizes, for any finite block length $n$, how close $-{1\over n} \ln p(X_1 X_2...X_n)$ is to $H(X)$ by determining the information spectrum of $X_1 X_2...X_n $, i.e., the distribution of $-{1\over n} \ln p(X_1 X_2...X_n)$. Non-asymptotic equipartition properties (with respect to conditional entropy, mutual information, and relative entropy) in a similar nature are also established. These non-asymptotic equipartition properties are instrumental to the development of non-asymptotic coding (including both source and channel coding) results in information theory in the same way as the asymptotic equipartition property to all asymptotic coding theorems established so far in information theory. As an example, the NEP with respect to $H(X)$ is used to establish a non-asymptotic fixed rate source coding theorem, which reveals, for any finite block length $n$, a complete picture about the tradeoff between the minimum rate of fixed rate coding of $X_1...X_n$ and error probability when the error probability is a constant, or goes to 0 with block length $n$ at a sub-polynomial, polynomial or sub-exponential speed. With the help of the NEP with respect to other information quantities, non-asymptotic channel coding theorems of similar nature will be established in a separate paper.</data>
    </node>
    <edge source="P83777" target="P47276">
      <data key="relation">reference</data>
    </edge>
    <edge source="P18610" target="P62475">
      <data key="relation">reference</data>
    </edge>
    <edge source="P18610" target="P54172">
      <data key="relation">reference</data>
    </edge>
    <edge source="P18610" target="P147212">
      <data key="relation">reference</data>
    </edge>
    <edge source="P18610" target="P63335">
      <data key="relation">reference</data>
    </edge>
    <edge source="P11872" target="P47">
      <data key="relation">reference</data>
    </edge>
    <edge source="P11872" target="P104668">
      <data key="relation">reference</data>
    </edge>
    <edge source="P11872" target="P92359">
      <data key="relation">reference</data>
    </edge>
    <edge source="P163752" target="P47276">
      <data key="relation">reference</data>
    </edge>
    <edge source="P163752" target="P63335">
      <data key="relation">reference</data>
    </edge>
    <edge source="P163752" target="P62475">
      <data key="relation">reference</data>
    </edge>
    <edge source="P163752" target="P54172">
      <data key="relation">reference</data>
    </edge>
    <edge source="P83594" target="P51877">
      <data key="relation">reference</data>
    </edge>
    <edge source="P83594" target="P147212">
      <data key="relation">reference</data>
    </edge>
    <edge source="P62475" target="P140360">
      <data key="relation">reference</data>
    </edge>
    <edge source="P62475" target="P47276">
      <data key="relation">reference</data>
    </edge>
    <edge source="P62475" target="P63335">
      <data key="relation">reference</data>
    </edge>
    <edge source="P62475" target="P54172">
      <data key="relation">reference</data>
    </edge>
    <edge source="P58130" target="P63335">
      <data key="relation">reference</data>
    </edge>
    <edge source="P63335" target="P47276">
      <data key="relation">reference</data>
    </edge>
    <edge source="P63335" target="P47">
      <data key="relation">reference</data>
    </edge>
    <edge source="P63335" target="P79154">
      <data key="relation">reference</data>
    </edge>
    <edge source="P63335" target="P54172">
      <data key="relation">reference</data>
    </edge>
    <edge source="P147212" target="P51877">
      <data key="relation">reference</data>
    </edge>
    <edge source="P147212" target="P47">
      <data key="relation">reference</data>
    </edge>
    <edge source="P147212" target="P59998">
      <data key="relation">reference</data>
    </edge>
    <edge source="P147212" target="P54172">
      <data key="relation">reference</data>
    </edge>
    <edge source="P147212" target="P67607">
      <data key="relation">reference</data>
    </edge>
    <edge source="P13395" target="P47276">
      <data key="relation">reference</data>
    </edge>
    <edge source="P59998" target="P51877">
      <data key="relation">reference</data>
    </edge>
    <edge source="P47276" target="P47">
      <data key="relation">reference</data>
    </edge>
    <edge source="P47276" target="P140360">
      <data key="relation">reference</data>
    </edge>
    <edge source="P47276" target="P79154">
      <data key="relation">reference</data>
    </edge>
    <edge source="P47276" target="P54172">
      <data key="relation">reference</data>
    </edge>
  </graph>
</graphml>
