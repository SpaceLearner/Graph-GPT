<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="relation" for="edge" attr.name="relation" attr.type="string" />
  <key id="abstract" for="node" attr.name="abstract" attr.type="string" />
  <key id="title" for="node" attr.name="title" attr.type="string" />
  <graph edgedefault="undirected">
    <node id="P4546">
      <data key="title">unrealrox an extremely photorealistic virtual reality environment for robotics simulations and synthetic data generation</data>
      <data key="abstract">Data-driven algorithms have surpassed traditional techniques in almost every aspect in robotic vision problems. Such algorithms need vast amounts of quality data to be able to work properly after their training process. Gathering and annotating that sheer amount of data in the real world is a time-consuming and error-prone task. Those problems limit scale and quality. Synthetic data generation has become increasingly popular since it is faster to generate and automatic to annotate. However, most of the current datasets and environments lack realism, interactions, and details from the real world. UnrealROX is an environment built over Unreal Engine 4 which aims to reduce that reality gap by leveraging hyperrealistic indoor scenes that are explored by robot agents which also interact with objects in a visually realistic manner in that simulated world. Photorealistic scenes and robots are rendered by Unreal Engine into a virtual reality headset which captures gaze so that a human operator can move the robot and use controllers for the robotic hands; scene information is dumped on a per-frame basis so that it can be reproduced offline to generate raw data and ground truth annotations. This virtual reality environment enables robotic vision researchers to generate realistic and visually plausible data with full ground truth for a wide variety of problems such as class and instance semantic segmentation, object detection, depth estimation, visual grasping, and navigation.</data>
    </node>
    <node id="P6">
      <data key="title">webvrgis based city bigdata 3d visualization and analysis</data>
      <data key="abstract">This paper shows the WEBVRGIS platform overlying multiple types of data about Shenzhen over a 3d globe. The amount of information that can be visualized with this platform is overwhelming, and the GIS-based navigational scheme allows to have great flexibility to access the different available data sources. For example,visualising historical and forecasted passenger volume at stations could be very helpful when overlaid with other social data.</data>
    </node>
    <node id="P448">
      <data key="title">characterizing a spectrum of audience interactivity</data>
      <data key="abstract">This research overviews and iterates on a Spectrum of Audience Interactivity, a tool to help designers and artists consider the dimensions and range of audience interactivity characteristics. Previous research has introduced a spectrum of audience participation in live performances based on children's co-design sessions. We expand on these findings with a review of literature in theater, games, and theme parks paired with expert interviews in those domains. From this work, we present a framework describing the spheres of audience influence, a new spectrum of audience interactivity, and a set of orthogonal spectrum dimensions that collectively describe audience interactivity. We pair our new framework with design considerations for practitioners; interactivity should be thoughtfully designed, should anticipate audience challenges, and can support multiple levels of the audience interactivity.</data>
    </node>
    <node id="P68668">
      <data key="title">serious game based dysphonic rehabilitation tool</data>
      <data key="abstract">The purpose of this work is designing and implementing a rehabilitation software for dysphonic patients. Constant training is a key factor for this type of therapy. The patient can play the game as well as conduct the voice training simultaneously guided by therapists at clinic or exercise independently at home. The voice information can be recorded and extracted for evaluating the long-time rehabilitation progress.</data>
    </node>
    <node id="P167601">
      <data key="title">towards the design of effective freehand gestural interaction for interactive tv</data>
      <data key="abstract">As interactive devices become pervasive, people are beginning to looking for more advanced interaction with televisions in the living room. Interactive television has the potential to offer a very engaging experience. But most common user tasks are still challenging with such systems, such as menu selection or text input. And little work has been done on understanding and sup-porting the effective design of freehand interaction with an TV in the living room. In this paper, we perform two studies investi-gating freehand gestural interaction with a consumer level sensor, which is suitable for TV scenarios. In the first study, we inves-tigate a range of design factors for tiled layout menu selection, including wearable feedback, push gesture depth, target size and position in motor space. The results show that tactile and audio feedback have no significant effect on performance and prefer-ence, and these results inform potential designs for high selection performance. In the second study, we investigate a common TV user task of text input using freehand gesture. We design and evaluate two virtual keyboard layouts and three freehand selec-tion methods. Results show that ease of use and error tolerance can be both achieved using a text entry method utilizing a dual circle layout and an expanding target selection technique. Finally, we propose design guidelines for effective, usable and com-fortable freehand gestural interaction for interactive TV based on the findings.</data>
    </node>
    <node id="P102655">
      <data key="title">drift robust non rigid optical flow enhancement for long sequences</data>
      <data key="abstract">It is hard to densely track a nonrigid object in long term, which is a fundamental research issue in the computer vision community. This task often relies on estimating pairwise correspondences between images over time where the error is accumulated and leads to a drift issue. In this paper, we introduce a novel optimization framework with an Anchor Patch constraint. It is supposed to significantly reduce overall errors given long sequences containing non-rigidly deformable objects. Our framework can be applied to any dense tracking algorithm, e.g. optical flow. We demonstrate the success of our approach by showing significant error reduction on 6 popular optical flow algorithms applied to a range of real-world nonrigid benchmarks. We also provide quantitative analysis of our approach given synthetic occlusions and image noise.</data>
    </node>
    <node id="P28046">
      <data key="title">learning hand eye coordination for robotic grasping with deep learning and large scale data collection</data>
      <data key="abstract">We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware. Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing.</data>
    </node>
    <node id="P145320">
      <data key="title">touch less interactive augmented reality game on vision based wearable device</data>
      <data key="abstract">There is an increasing interest in creating pervasive games based on emerging interaction technologies. In order to develop touch-less, interactive and augmented reality games on vision-based wearable device, a touch-less motion interaction technology is designed and evaluated in this work. Users interact with the augmented reality games with dynamic hands/feet gestures in front of the camera, which triggers the interaction event to interact with the virtual object in the scene. Three primitive augmented reality games with eleven dynamic gestures are developed based on the proposed touch-less interaction technology as proof. At last, a comparing evaluation is proposed to demonstrate the social acceptability and usability of the touch-less approach, running on a hybrid wearable framework or with Google Glass, as well as workload assessment, user's emotions and satisfaction.</data>
    </node>
    <node id="P34631">
      <data key="title">preprint intuitive evaluation of kinect2 based balance measurement software</data>
      <data key="abstract">This is the preprint version of our paper on REHAB2015. A balance measurement software based on Kinect2 sensor is evaluated by comparing to golden standard balance measure platform intuitively. The software analysis the tracked body data from the user by Kinect2 sensor and get user's center of mass(CoM) as well as its motion route on a plane. The software is evaluated by several comparison tests, the evaluation results preliminarily prove the reliability of the software.</data>
    </node>
    <node id="P32459">
      <data key="title">a visually plausible grasping system for object manipulation and interaction in virtual reality environments</data>
      <data key="abstract">Interaction in virtual reality (VR) environments is essential to achieve a pleasant and immersive experience. Most of the currently existing VR applications, lack of robust object grasping and manipulation, which are the cornerstone of interactive systems. Therefore, we propose a realistic, flexible and robust grasping system that enables rich and real-time interactions in virtual environments. It is visually realistic because it is completely user-controlled, flexible because it can be used for different hand configurations, and robust because it allows the manipulation of objects regardless their geometry, i.e. hand is automatically fitted to the object shape. In order to validate our proposal, an exhaustive qualitative and quantitative performance analysis has been carried out. On the one hand, qualitative evaluation was used in the assessment of the abstract aspects such as: hand movement realism, interaction realism and motor control. On the other hand, for the quantitative evaluation a novel error metric has been proposed to visually analyze the performed grips. This metric is based on the computation of the distance from the finger phalanges to the nearest contact point on the object surface. These contact points can be used with different application purposes, mainly in the field of robotics. As a conclusion, system evaluation reports a similar performance between users with previous experience in virtual reality applications and inexperienced users, referring to a steep learning curve.</data>
    </node>
    <node id="P85771">
      <data key="title">improving the performance of unimodal dynamic hand gesture recognition with multimodal training</data>
      <data key="abstract">We present an efficient approach for leveraging the knowledge from multiple modalities in training unimodal 3D convolutional neural networks (3D-CNNs) for the task of dynamic hand gesture recognition. Instead of explicitly combining multimodal information, which is commonplace in many state-of-the-art methods, we propose a different framework in which we embed the knowledge of multiple modalities in individual networks so that each unimodal network can achieve an improved performance. In particular, we dedicate separate networks per available modality and enforce them to collaborate and learn to develop networks with common semantics and better representations. We introduce a "spatiotemporal semantic alignment" loss (SSA) to align the content of the features from different networks. In addition, we regularize this loss with our proposed "focal regularization parameter" to avoid negative knowledge transfer. Experimental results show that our framework improves the test time recognition accuracy of unimodal networks, and provides the state-of-the-art performance on various dynamic hand gesture recognition datasets.</data>
    </node>
    <node id="P158342">
      <data key="title">streambed training citizen scientists to make qualitative judgments using embodied virtual reality training</data>
      <data key="abstract">Environmental citizen science frequently relies on experience-based assessment, however volunteers are not trained to make qualitative judgments. Embodied learning in virtual reality (VR) has been explored as a way to train behavior, but has not fully been considered as a way to train judgment. This preliminary research explores embodied learning in VR through the design, evaluation, and redesign of StreamBED, a water quality monitoring training environment that teaches volunteers to make qualitative assessments by exploring, assessing and comparing virtual watersheds.</data>
    </node>
    <node id="P74590">
      <data key="title">video interpolation using optical flow and laplacian smoothness</data>
      <data key="abstract">Non-rigid video interpolation is a common computer vision task. In this paper we present an optical flow approach which adopts a Laplacian Cotangent Mesh constraint to enhance the local smoothness. Similar to Li et al., our approach adopts a mesh to the image with a resolution up to one vertex per pixel and uses angle constraints to ensure sensible local deformations between image pairs. The Laplacian Mesh constraints are expressed wholly inside the optical flow optimization, and can be applied in a straightforward manner to a wide range of image tracking and registration problems. We evaluate our approach by testing on several benchmark datasets, including the Middlebury and Garg et al. datasets. In addition, we show application of our method for constructing 3D Morphable Facial Models from dynamic 3D data.</data>
    </node>
    <node id="P90439">
      <data key="title">learn to model motion from blurry footages</data>
      <data key="abstract">It is difficult to recover the motion field from a real-world footage given a mixture of camera shake and other photometric effects. In this paper we propose a hybrid framework by interleaving a Convolutional Neural Network (CNN) and a traditional optical flow energy. We first conduct a CNN architecture using a novel learnable directional filtering layer. Such layer encodes the angle and distance similarity matrix between blur and camera motion, which is able to enhance the blur features of the camera-shake footages. The proposed CNNs are then integrated into an iterative optical flow framework, which enable the capability of modelling and solving both the blind deconvolution and the optical flow estimation problems simultaneously. Our framework is trained end-to-end on a synthetic dataset and yields competitive precision and performance against the state-of-the-art approaches.</data>
    </node>
    <node id="P15605">
      <data key="title">big city 3d visual analysis</data>
      <data key="abstract">A big city visual analysis platform based on Web Virtual Reality Geographical Information System (WEBVRGIS) is presented. Extensive model editing functions and spatial analysis functions are available, including terrain analysis, spatial analysis, sunlight analysis, traffic analysis, population analysis and community analysis.</data>
    </node>
    <node id="P159105">
      <data key="title">preprint bigdata oriented multimedia mobile health applications</data>
      <data key="abstract">This is the preprint version of our paper on JOMS. In this paper, two mHealth applications are introduced, which can be employed as the terminals of bigdata based health service to collect information for electronic medical records (EMRs). The first one is a hybrid system for improving the user experience in the hyperbaric oxygen chamber by 3D stereoscopic virtual reality glasses and immersive perception. Several HMDs have been tested and compared. The second application is a voice interactive serious game as a likely solution for providing assistive rehabilitation tool for therapists. The recorder of the voice of patients could be analysed to evaluate the long-time rehabilitation results and further to predict the rehabilitation process.</data>
    </node>
    <node id="P33997">
      <data key="title">regularized maximum correntropy machine</data>
      <data key="abstract">In this paper we investigate the usage of regularized correntropy framework for learning of classifiers from noisy labels. The class label predictors learned by minimizing transitional loss functions are sensitive to the noisy and outlying labels of training samples, because the transitional loss functions are equally applied to all the samples. To solve this problem, we propose to learn the class label predictors by maximizing the correntropy between the predicted labels and the true labels of the training samples, under the regularized Maximum Correntropy Criteria (MCC) framework. Moreover, we regularize the predictor parameter to control the complexity of the predictor. The learning problem is formulated by an objective function considering the parameter regularization and MCC simultaneously. By optimizing the objective function alternately, we develop a novel predictor learning algorithm. The experiments on two chal- lenging pattern classification tasks show that it significantly outperforms the machines with transitional loss functions.</data>
    </node>
    <node id="P1338">
      <data key="title">preprint traffic management and forecasting system based on 3d gis</data>
      <data key="abstract">This is the preprint version of our paper on 2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGrid). This paper takes Shenzhen Futian comprehensive transportation junction as the case, and makes use of continuous multiple real-time dynamic traffic information to carry out monitoring and analysis on spatial and temporal distribution of passenger flow under different means of transportation and service capacity of junction from multi-dimensional space-time perspectives such as different period and special period. Virtual reality geographic information system is employed to present the forecasting result.</data>
    </node>
    <node id="P87110">
      <data key="title">a game based assistive tool for rehabilitation of dysphonic patients</data>
      <data key="abstract">An assistive training tool for rehabilitation of dysphonic patients is designed and developed according to the practical clinical needs. The assistive tool employs a space flight game as the attractive logic part, and microphone arrays as input device, which is getting rid of ambient noise by setting a specific orientation. The therapist can guide the patient to play the game as well as the voice training simultaneously side by side, while not interfere the patient voice. The voice information can be recorded and extracted for evaluating the long-time rehabilitation progress. This paper outlines a design science approach for the development of an initial useful software prototype of such a tool, considering 'Intuitive', 'Entertainment', 'Incentive' as main design factors.</data>
    </node>
    <node id="P126543">
      <data key="title">dense motion estimation for smoke</data>
      <data key="abstract">Motion estimation for highly dynamic phenomena such as smoke is an open challenge for Computer Vision. Traditional dense motion estimation algorithms have difficulties with non-rigid and large motions, both of which are frequently observed in smoke motion. We propose an algorithm for dense motion estimation of smoke. Our algorithm is robust, fast, and has better performance over different types of smoke compared to other dense motion estimation algorithms, including state of the art and neural network approaches. The key to our contribution is to use skeletal flow, without explicit point matching, to provide a sparse flow. This sparse flow is upgraded to a dense flow. In this paper we describe our algorithm in greater detail, and provide experimental evidence to support our claims.</data>
    </node>
    <edge source="P4546" target="P28046">
      <data key="relation">reference</data>
    </edge>
    <edge source="P4546" target="P32459">
      <data key="relation">reference</data>
    </edge>
    <edge source="P6" target="P1338">
      <data key="relation">reference</data>
    </edge>
    <edge source="P6" target="P145320">
      <data key="relation">reference</data>
    </edge>
    <edge source="P448" target="P158342">
      <data key="relation">reference</data>
    </edge>
    <edge source="P448" target="P145320">
      <data key="relation">reference</data>
    </edge>
    <edge source="P68668" target="P34631">
      <data key="relation">reference</data>
    </edge>
    <edge source="P68668" target="P159105">
      <data key="relation">reference</data>
    </edge>
    <edge source="P68668" target="P87110">
      <data key="relation">reference</data>
    </edge>
    <edge source="P68668" target="P145320">
      <data key="relation">reference</data>
    </edge>
    <edge source="P167601" target="P74590">
      <data key="relation">reference</data>
    </edge>
    <edge source="P167601" target="P90439">
      <data key="relation">reference</data>
    </edge>
    <edge source="P167601" target="P102655">
      <data key="relation">reference</data>
    </edge>
    <edge source="P167601" target="P145320">
      <data key="relation">reference</data>
    </edge>
    <edge source="P102655" target="P74590">
      <data key="relation">reference</data>
    </edge>
    <edge source="P28046" target="P32459">
      <data key="relation">reference</data>
    </edge>
    <edge source="P145320" target="P34631">
      <data key="relation">reference</data>
    </edge>
    <edge source="P145320" target="P159105">
      <data key="relation">reference</data>
    </edge>
    <edge source="P145320" target="P74590">
      <data key="relation">reference</data>
    </edge>
    <edge source="P145320" target="P32459">
      <data key="relation">reference</data>
    </edge>
    <edge source="P145320" target="P15605">
      <data key="relation">reference</data>
    </edge>
    <edge source="P145320" target="P87110">
      <data key="relation">reference</data>
    </edge>
    <edge source="P145320" target="P85771">
      <data key="relation">reference</data>
    </edge>
    <edge source="P34631" target="P87110">
      <data key="relation">reference</data>
    </edge>
    <edge source="P74590" target="P126543">
      <data key="relation">reference</data>
    </edge>
    <edge source="P90439" target="P126543">
      <data key="relation">reference</data>
    </edge>
    <edge source="P159105" target="P33997">
      <data key="relation">reference</data>
    </edge>
    <edge source="P159105" target="P87110">
      <data key="relation">reference</data>
    </edge>
    <edge source="P1338" target="P87110">
      <data key="relation">reference</data>
    </edge>
  </graph>
</graphml>
