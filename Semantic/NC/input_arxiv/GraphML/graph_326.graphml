<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="relation" for="edge" attr.name="relation" attr.type="string" />
  <key id="abstract" for="node" attr.name="abstract" attr.type="string" />
  <key id="title" for="node" attr.name="title" attr.type="string" />
  <graph edgedefault="undirected">
    <node id="P34566">
      <data key="title">individual research performance a proposal for comparing apples to oranges</data>
      <data key="abstract">The evaluation of performance at the individual level is of fundamental importance in informing management decisions. The literature provides various indicators and types of measures, however a problem that is still unresolved and little addressed is how to compare the performance of researchers working in different fields (apples to oranges). In this work we propose a solution, testing various scaling factors for the distributions of research productivity in 174 scientific fields. The analysis is based on the observation of scientific production by all Italian university researchers active in the hard sciences over the period 2004–2008, as indexed by the Web of Science. The most effective scaling factor is the average of the productivity distribution of researchers with productivity above zero.</data>
    </node>
    <node id="P75687">
      <data key="title">citation impacts revisited how novel impact measures reflect interdisciplinarity and structural change at the local and global level</data>
      <data key="abstract">Citation networks have fed numerous works in scientific evaluation, science mapping (and more recently large-scale network studies) for decades. The variety of citation behavior across scientific fields is both a research topic in sociology of science, and a problem in scientific evaluation. Normalization, tantamount to a particular weighting of links in the citation network, is necessary for allowing across-field comparisons of citation scores and interdisciplinary studies. In addition to classical normalization which drastically reduces all variability factors altogether, two tracks of research have emerged in the recent years. One is the revival of iterative "influence measures". The second is the "citing-side" normalization, whose only purpose is to control for the main factor of variability, the inequality in citing propensity, letting other aspects play: knowledge export/imports and growth. When all variables are defined at the same field-level, two propositions are established: (a) the gross impact measure identifies with the product of relative growth rate, gross balance of citation exchanges, and relative number of references (b) the normalized impact identifies with the product of relative growth rate and normalized balance. At the science level, the variance of growth rate over domains is a proxy for change in the system, and the variance of balance a measure of inter-disciplinary dependences. This opens a new perspective, where the resulting variance of normalized impact, and a related measure, the sum of these variances proposed as a Change-Exchange Indicator, summarize important aspects of science structure and dynamism. Results based on a decade's data are discussed. The behavior of normalized impact according to scale changes is also briefly discussed.</data>
    </node>
    <node id="P61146">
      <data key="title">a heuristic approach to author name disambiguation in bibliometrics databases for large scale research assessments</data>
      <data key="abstract">National exercises for the evaluation of research activity by universities are becoming regular practice in ever more countries. These exercises have mainly been conducted through the application of peer-review methods. Bibliometrics has not been able to offer a valid large-scale alternative because of almost overwhelming difficulties in identifying the true author of each publication. We will address this problem by presenting a heuristic approach to author name disambiguation in bibliometric datasets for large-scale research assessments. The application proposed concerns the Italian university system, comprising 80 universities and a research staff of over 60,000 scientists. The key advantage of the proposed approach is the ease of implementation. The algorithms are of practical application and have considerably better scalability and expandability properties than state-of-the-art unsupervised approaches. Moreover, the performance in terms of precision and recall, which can be further improved, seems thoroughly adequate for the typical needs of large-scale bibliometric research assessments. © 2011 Wiley Periodicals, Inc.</data>
    </node>
    <node id="P403">
      <data key="title">a simple centrality index for scientific social recognition</data>
      <data key="abstract">We introduce a new centrality index for bipartite network of papers and authors that we call $K$-index. The $K$-index grows with the citation performance of the papers that cite a given researcher and can seen as a measure of scientific social recognition. Indeed, the $K$-index measures the number of hubs, defined in a self-consistent way in the bipartite network, that cites a given author. We show that the $K$-index can be computed by simple inspection of the Web of Science platform and presents several advantages over other centrality indexes, in particular Hirsch $h$-index. The $K$-index is robust to self-citations, is not limited by the total number of papers published by a researcher as occurs for the $h$-index and can distinguish in a consistent way researchers that have the same $h$-index but very different scientific social recognition. The $K$-index easily detects a known case of a researcher with inflated number of papers, citations and $h$-index due to scientific misconduct. Finally, we show that, in a sample of twenty-eight physics Nobel laureates and twenty-eight highly cited non-Nobel-laureate physicists, the $K$-index correlates better to the achievement of the prize than the number of papers, citations, citations per paper, citing articles or the $h$-index. Clustering researchers in a $K$ versus $h$ plot reveals interesting outliers that suggest that these two indexes can present complementary independent information.</data>
    </node>
    <node id="P123033">
      <data key="title">ranking research institutions by the number of highly cited articles per scientist</data>
      <data key="abstract">In the literature and on the Web we can readily find research excellence rankings for organizations and countries by either total number of highly-cited articles (HCAs) or by ratio of HCAs to total publications. Neither are indicators of efficiency. In the current work we propose an indicator of efficiency, the number of HCAs per scientist, which can complement the productivity indicators based on impact of total output. We apply this indicator to measure excellence in the research of Italian universities as a whole, and in each field and discipline of the hard sciences.</data>
    </node>
    <node id="P162462">
      <data key="title">the h index can be easily manipulated</data>
      <data key="abstract">We prove two complexity results about the H-index concerned with the Google scholar merge operation on one's scientific articles. The results show that, although it is hard to merge one's articles in an optimal way, it is easy to merge them in such a way that one's H-index increases. This suggests the need for an alternative scientific performance measure that is resistant to this type of manipulation.</data>
    </node>
    <node id="P124324">
      <data key="title">measuring science irresistible temptations easy shortcuts and dangerous consequences</data>
      <data key="abstract">In benchmarking international research, although publication and citation analyses should not be used to compare different disciplines, scientometrists frequently fail to resist the temptation to present rankings based on total publications and citations. Such measures are affected by significant distortions, due to the uneven fertility across scientific disciplines and the dishomogeneity of scientific specialisation among nations and universities. In this paper, we provide an indication of the extent of the distortions when comparative bibliometric analyses fail to recognise the range of levels of scientific fertility, not only within a given major disciplinary area but also within the different scientific disciplines encompassed by the same area.</data>
    </node>
    <node id="P20273">
      <data key="title">character networks and book genre classification</data>
      <data key="abstract">We compare the social character networks of biographical, legendary and fictional texts, in search for marks of genre differentiation. We examine the degree distribution of character appearance and find a power law that does not depend on the literary genre or historical content. We also analyze local and global complex networks measures, in particular, correlation plots between the recently introduced Lobby (or Hirsh $H(1)$) index and Degree, Betweenness and Closeness centralities. Assortativity plots, which previous literature claims to separate fictional from real social networks, were also studied. We've found no relevant differences in the books for these network measures and we give a plausible explanation why the previous assortativity result is not correct.</data>
    </node>
    <node id="P98146">
      <data key="title">a principal component analysis of 39 scientific impact measures</data>
      <data key="abstract">Background: The impact of scientific publications has traditionally been expressed in terms of citation counts. However, scientific activity has moved online over the past decade. To better capture scientific impact in the digital era, a variety of new impact measures has been proposed on the basis of social network analysis and usage log data. Here we investigate how these new measures relate to each other, and how accurately and completely they express scientific impact. Methodology: We performed a principal component analysis of the rankings produced by 39 existing and proposed measures of scholarly impact that were calculated on the basis of both citation and usage log data. Conclusions: Our results indicate that the notion of scientific impact is a multi-dimensional construct that can not be adequately measured by any single indicator, although some measures are more suitable than others. The commonly used citation Impact Factor is not positioned at the core of this construct, but at its periphery, and should thus be used with caution.</data>
    </node>
    <node id="P4824">
      <data key="title">highly cited papers in library and information science lis authors institutions and network structures</data>
      <data key="abstract">As a follow-up to the highly-cited authors list published by Thomson Reuters in June 2014, we analyze the top-1% most frequently cited papers published between 2002 and 2012 included in the Web of Science (WoS) subject category "Information Science &amp; Library Science." 798 authors contributed to 305 top-1% publications; these authors were employed at 275 institutions. The authors at Harvard University contributed the largest number of papers, when the addresses are whole-number counted. However, Leiden University leads the ranking, if fractional counting is used. #R##N#Twenty-three of the 798 authors were also listed as most highly-cited authors by Thomson Reuters in June 2014 (this http URL). Twelve of these 23 authors were involved in publishing four or more of the 305 papers under study. Analysis of co-authorship relations among the 798 highly-cited scientists shows that co-authorships are based on common interests in a specific topic. Three topics were important between 2002 and 2012: (1) collection and exploitation of information in clinical practices, (2) the use of internet in public communication and commerce, and (3) scientometrics.</data>
    </node>
    <node id="P116006">
      <data key="title">mapping excellence in national research systems the case of italy</data>
      <data key="abstract">The study of the concept of "scientific excellence" and the methods for its measurement and evaluation is taking on increasing importance in the development of research policies in many nations. However, scientific excellence results as difficult to define in both conceptual and operational terms, because of its multi-dimensional and highly complex character. The literature on the theme is limited to few studies of an almost pioneering character. This work intends to contribute to the state of the art by exploring a bibliometric methodology which is effective, simple and inexpensive, and which further identifies "excellent" centers of research by beginning from excellence of the individual researchers affiliated with such centers. The study concentrates on the specific case of public research organizations in Italy, analyzing 109 scientific categories of research in the so called "hard" sciences and identifying 157 centers of excellence operating in 60 of these categories. The findings from this first application of the methodology should be considered exploratory and indicative. With a longer period of observation and the addition of further measurements, making the methodology more robust, it can be extended and adapted to a variety of national and supranational contexts, aiding with policy decisions at various levels.</data>
    </node>
    <node id="P75658">
      <data key="title">ranking de revistas cientificas en latinoamerica mediante el indice h estudio de caso colombia</data>
      <data key="abstract">The future of scientific journals in Latin America is uncertain. The inability to calculate bibliometric indicators to classify and homologate journals according to globally-used quartiles reduces journal visibility. For instance, Colombian journals classified in the National Bibliographic Database Publindex (BBNP) under categories A1, A2, B and C, have low rates of indexing in international databases, such as Web of Science or Scopus, thus limiting the estimation of bibliometric indicators for comparison at an international level. In this case study, articles published between 2003 and 2007 were taken from 211 journals selected from the BBNP- 2008. Using Google Scholar, Publish or Perish software and the Scopus database, impact and productivity indicators were quantified and compared creating a regional ranking (Q1-Q4). They were subsequently homologated internationally with the Scimago Journal Ranking (SJR). The h index proved the best indicator to generate the ranking of 170 Colombian journals based on quartiles (Q1-Q4) able to be homologated with SJR quartiles. We propose this methodology for classifying and homologating Latin American journals that are not indexed internationally, as a useful tool for publishers, publishing houses, information agencies and public policy decision-makers concerning education, science and technology, both regionally and globally.</data>
    </node>
    <node id="P119145">
      <data key="title">are the authors of highly cited articles also the most productive ones</data>
      <data key="abstract">Ever more frequently, governments have decided to implement policy measures intended to foster and reward excellence in scientific research. This is in fact the intended purpose of national research assessment exercises. These are typically based on the analysis of the quality of the best research products; however, a different approach to analysis and intervention is based on the measure of productivity of the individual scientists, meaning the overall impact of their entire scientific production over the period under observation. This work analyzes the convergence of the two approaches, asking if and to what measure the most productive scientists achieve highly cited articles; or vice versa, what share of highly cited articles is achieved by scientists that are “non-top” for productivity. To do this we use bibliometric indicators, applied to the 2004–2008 publications authored by academics of Italian universities and indexed in the Web of Science.</data>
    </node>
    <node id="P133198">
      <data key="title">the inconsistency of the h index</data>
      <data key="abstract">The h-index is a popular bibliometric indicator for assessing individual scientists. We criticize the h-index from a theoretical point of view. We argue that for the purpose of measuring the overall scientific impact of a scientist (or some other unit of analysis) the h-index behaves in a counterintuitive way. In certain cases, the mechanism used by the h-index to aggregate publication and citation statistics into a single number leads to inconsistencies in the way in which scientists are ranked. Our conclusion is that the h-index cannot be considered an appropriate indicator of a scientist's overall scientific impact. Based on recent theoretical insights, we discuss what kind of indicators can be used as an alternative to the h-index. We pay special attention to the highly cited publications indicator. This indicator has a lot in common with the h-index, but unlike the h-index it does not produce inconsistent rankings.</data>
    </node>
    <node id="P159004">
      <data key="title">revisiting the scaling of citations for research assessment</data>
      <data key="abstract">Over the past decade, national research evaluation exercises, traditionally conducted using the peer review method, have begun opening to bibliometric indicators. The citations received by a publication are assumed as proxy for its quality, but they require standardization prior to use in comparative evaluation of organizations or individual scientists: the citation data must be standardized, due to the varying citation behavior across research fields. The objective of this paper is to compare the effectiveness of the different methods of normalizing citations, in order to provide useful indications to research assessment practitioners. Simulating a typical national research assessment exercise, he analysis is conducted for all subject categories in the hard sciences and is based on the Thomson Reuters Science Citation Index-Expanded®. Comparisons show that the citations average is the most effective scaling parameter, when the average is based only on the publications actually cited.</data>
    </node>
    <node id="P54588">
      <data key="title">the importance of accounting for the number of co authors and their order when assessing research performance at the individual level in the life sciences</data>
      <data key="abstract">Accurate measurement of research productivity should take account of both the number of co-authors of every scientific work and of the different contributions of the individuals. For researchers in the life sciences, common practice is to indicate such contributions through position in the authors list. In this work, we measure the distortion introduced to bibliometric ranking lists for scientific productivity when the number of co-authors or their position in the list is ignored. The field of observation consists of all Italian university professors working in the life sciences, with scientific production examined over the period 2004–2008. The outcomes of the study lead to a recommendation against using indicators or evaluation methods that ignore the different authors’ contributions to the research results.</data>
    </node>
    <node id="P92799">
      <data key="title">a field standardized application of dea to national scale research assessment of universities</data>
      <data key="abstract">The current work proposes an application of DEA methodology for measurement of technical and allocative efficiency of university research activity. The analysis is based on bibliometric data from the Italian university system for the five-year period 2004–2008. Technical and allocative efficiency is measured with input being considered as a university's research staff, classified according to academic rank, and with output considered as the field-standardized impact of the research product realized by these staff. The analysis is applied to all scientific disciplines of the so-called hard sciences, and conducted at subfield level, thus at a greater level of detail than ever before achieved in national-scale research assessments.</data>
    </node>
    <node id="P128480">
      <data key="title">the relationship between scientists research performance and the degree of internationalization of their research</data>
      <data key="abstract">Policy makers, at various levels of governance, generally encourage the development of research collaboration. However the underlying determinants of collaboration are not completely clear. In particular, the literature lacks studies that, taking the individual researcher as the unit of analysis, attempt to understand if and to what extent the researcher's scientific performance might impact on his/her degree of collaboration with foreign colleagues. The current work examines the international collaborations of Italian university researchers for the period 2001---2005, and puts them in relation to each individual's research performance. The results of the investigation, which assumes co-authorship as proxy of research collaboration, show that both research productivity and average quality of output have positive effects on the degree of international collaboration achieved by a scientist.</data>
    </node>
    <node id="P57224">
      <data key="title">the contribution of star scientists to overall sex differences in research productivity</data>
      <data key="abstract">The state of the art on the issue of sex differences in research efficiency agrees in recognizing higher performances for males, however there are divergences in explaining the possible causes. One of the causes advanced is that there are sex differences in the availability of aptitude at the “high end”. By comparing sex differences in concentration and performance of Italian academic star scientists to the case in the population complement, this work aims to verify if star, or “high-end”, scientists play a preponderant role in determining higher performance among males. The study reveals the existence of a greater relative concentration of males among star scientists, as well as a performance gap between male and female star scientists that is greater than for the rest of the population. In the latter subpopulation the performance gap between the two sexes is seen as truly marginal.</data>
    </node>
    <node id="P168250">
      <data key="title">lobby index as a network centrality measure</data>
      <data key="abstract">We study the lobby index (l-index for short) as a local node centrality measure for complex networks. The l-index is compared with degree (a local measure), betweenness and Eigenvector centralities (two global measures) in the case of a biological network (Yeast interaction protein–protein network) and a linguistic network (Moby Thesaurus II). In both networks, the l-index has a poor correlation with betweenness but correlates with degree and Eigenvector centralities. Although being local, the l-index carries more information about its neighbors than degree centrality. Also, it requires much less time to compute when compared with Eigenvector centrality. Results show that the l-index produces better results than degree and Eigenvector centrality for ranking purposes.</data>
    </node>
    <edge source="P34566" target="P119145">
      <data key="relation">reference</data>
    </edge>
    <edge source="P34566" target="P124324">
      <data key="relation">reference</data>
    </edge>
    <edge source="P34566" target="P61146">
      <data key="relation">reference</data>
    </edge>
    <edge source="P34566" target="P159004">
      <data key="relation">reference</data>
    </edge>
    <edge source="P75687" target="P133198">
      <data key="relation">reference</data>
    </edge>
    <edge source="P61146" target="P123033">
      <data key="relation">reference</data>
    </edge>
    <edge source="P61146" target="P119145">
      <data key="relation">reference</data>
    </edge>
    <edge source="P61146" target="P54588">
      <data key="relation">reference</data>
    </edge>
    <edge source="P61146" target="P92799">
      <data key="relation">reference</data>
    </edge>
    <edge source="P403" target="P119145">
      <data key="relation">reference</data>
    </edge>
    <edge source="P403" target="P168250">
      <data key="relation">reference</data>
    </edge>
    <edge source="P403" target="P133198">
      <data key="relation">reference</data>
    </edge>
    <edge source="P403" target="P162462">
      <data key="relation">reference</data>
    </edge>
    <edge source="P123033" target="P119145">
      <data key="relation">reference</data>
    </edge>
    <edge source="P123033" target="P54588">
      <data key="relation">reference</data>
    </edge>
    <edge source="P123033" target="P159004">
      <data key="relation">reference</data>
    </edge>
    <edge source="P123033" target="P116006">
      <data key="relation">reference</data>
    </edge>
    <edge source="P124324" target="P119145">
      <data key="relation">reference</data>
    </edge>
    <edge source="P124324" target="P57224">
      <data key="relation">reference</data>
    </edge>
    <edge source="P124324" target="P116006">
      <data key="relation">reference</data>
    </edge>
    <edge source="P20273" target="P168250">
      <data key="relation">reference</data>
    </edge>
    <edge source="P98146" target="P168250">
      <data key="relation">reference</data>
    </edge>
    <edge source="P4824" target="P119145">
      <data key="relation">reference</data>
    </edge>
    <edge source="P116006" target="P119145">
      <data key="relation">reference</data>
    </edge>
    <edge source="P75658" target="P133198">
      <data key="relation">reference</data>
    </edge>
    <edge source="P119145" target="P54588">
      <data key="relation">reference</data>
    </edge>
    <edge source="P119145" target="P92799">
      <data key="relation">reference</data>
    </edge>
    <edge source="P119145" target="P159004">
      <data key="relation">reference</data>
    </edge>
    <edge source="P119145" target="P57224">
      <data key="relation">reference</data>
    </edge>
    <edge source="P119145" target="P128480">
      <data key="relation">reference</data>
    </edge>
  </graph>
</graphml>
