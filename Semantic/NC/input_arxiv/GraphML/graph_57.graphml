<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="relation" for="edge" attr.name="relation" attr.type="string" />
  <key id="abstract" for="node" attr.name="abstract" attr.type="string" />
  <key id="title" for="node" attr.name="title" attr.type="string" />
  <graph edgedefault="undirected">
    <node id="P40047">
      <data key="title">capacity bounds for diamond networks with an orthogonal broadcast channel</data>
      <data key="abstract">A class of diamond networks is studied where the broadcast component is orthogonal and modeled by two independent bit-pipes. New upper and lower bounds on the capacity are derived. The proof technique for the upper bound generalizes bounding techniques of Ozarow for the Gaussian multiple description problem (1981) and Kang and Liu for the Gaussian diamond network (2011). The lower bound is based on Marton's coding technique and superposition coding. The bounds are evaluated for Gaussian and binary adder multiple access channels (MACs). For Gaussian MACs, both the lower and upper bounds strengthen the Kang-Liu bounds and establish capacity for interesting ranges of bit-pipe capacities. For binary adder MACs, the capacity is established for all ranges of bit-pipe capacities.</data>
    </node>
    <node id="P88940">
      <data key="title">entropy bounds on abelian groups and the ruzsa divergence</data>
      <data key="abstract">Over the past few years, a family of interesting new inequalities for the entropies of sums and differences of random variables has been developed by Ruzsa, Tao and others, motivated by analogous results in additive combinatorics. The present work extends these earlier results to the case of random variables taking values in $\mathbb{R}^n$ or, more generally, in arbitrary locally compact and Polish abelian groups. We isolate and study a key quantity, the Ruzsa divergence between two probability distributions, and we show that its properties can be used to extend the earlier inequalities to the present general setting. The new results established include several variations on the theme that the entropies of the sum and the difference of two independent random variables severely constrain each other. Although the setting is quite general, the result are already of interest (and new) for random vectors in $\mathbb{R}^n$. In that special case, quantitative bounds are provided for the stability of the equality conditions in the entropy power inequality; a reverse entropy power inequality for log-concave random vectors is proved; an information-theoretic analog of the Rogers-Shephard inequality for convex bodies is established; and it is observed that some of these results lead to new inequalities for the determinants of positive-definite matrices. Moreover, by considering the multiplicative subgroups of the complex plane, one obtains new inequalities for the differential entropies of products and ratios of nonzero, complex-valued random variables.</data>
    </node>
    <node id="P153577">
      <data key="title">lower bounds on information divergence</data>
      <data key="abstract">In this paper we establish lower bounds on informa- tion divergence from a distribution to certain important classes of distributions as Gaussian, exponential, Gamma, Poisson, ge- ometric, and binomial. These lower bounds are tight and for several convergence theorems where a rate of convergence can be computed, this rate is determined by the lower bounds proved in this paper. General techniques for getting lower bounds in terms of moments are developed. I. I NTRODUCTION AND NOTATIONS In 2004, O. Johnson and A. Barron have proved (1) that the rate of convergence in the information theoretic Central Limit Theorem is upper bounded by c=n under suitable conditions. P. Harremoes extended this work in (2) based on a maximum entropy approach. Similar results have been obtained for the convergence of binomial distributions to Poisson distributions. Finally the rate of convergence of convolutions of distribu tions on the unit circle toward the uniform distribution can be bounded. In each of these cases lower bounds on information divergence in terms of moments of orthogonal polynomials or trigonometric functions give lower bounds on the rate of convergence. In this paper, we provide more lower bounds on information divergence using mainly orthogonal polynomials and the related exponential families. We will identify x! with ( x + 1) even when x is not an integer. Similarly the generalized binomial coefficient x � equals x(x − 1)���(x − n + 1)=n! when x is not an integer. We useas short for 2� .</data>
    </node>
    <node id="P35796">
      <data key="title">interactive computation of type threshold functions in collocated broadcast superposition networks</data>
      <data key="abstract">In wireless sensor networks, various applications involve learning one or multiple functions of the measurements observed by sensors, rather than the measurements themselves. This paper focuses on type-threshold functions, e.g., the maximum and indicator functions. Previous work studied this problem under the collocated collision network model and showed that under many probabilistic models for the measurements, the achievable computation rates converge to zero as the number of sensors increases. This paper considers two network models reflecting both the broadcast and superposition properties of wireless channels: the collocated linear finite field network and the collocated Gaussian network. A general multi-round coding scheme exploiting not only the broadcast property but particularly also the superposition property of the networks is developed. Through careful scheduling of concurrent transmissions to reduce redundancy, it is shown that given any independent measurement distribution, all type-threshold functions can be computed reliably with a non-vanishing rate in the collocated Gaussian network, even if the number of sensors tends to infinity.</data>
    </node>
    <node id="P72">
      <data key="title">an information theoretic perspective of the poisson approximation via the chen stein method</data>
      <data key="abstract">The first part of this work considers the entropy of the sum of ( possibly dependent and non-identically distributed) Bernoulli random variables. Upper bounds on the error that follows from an approximation of this entropy by the entropy of a Poisson random variable with the same mean are derived via the Chen-Stein method. The second part of this work derives new lower bounds on the total variation distance and relative entropy between the distribution of the sum of independent Bernoulli random variables and the Poisson distribution. The starting point of the derivation of the new bounds in the second part of this work is an introduction of a new lower bound on the total variation distance, whose derivation generalizes and refines the anal ysis by Barbour and Hall (1984), based on the Chen-Stein method for the Poisson approximation. A new lower bound on the relative entropy between these two distributions is introd uced, and this lower bound is compared to a previously reported upper bound on the relative entropy by Kontoyiannis et al. (2005). The derivation of the new lower bound on the relative entropy follows from the new lower bound on the total variation distance, combined with a distribution-dependent refine ment of Pinsker’s inequality by Ordentlich and Weinberger (2005). Upper and lower bounds on the Bhattacharyya parameter, Chernoff information and Hellinger distance between the distribution of the sum of independent Bernoulli random variables and the Poisson distribution with the same mean are derived as well via some relations between these quantities with the total variation distance and the relative entropy. The analysis in this work combines elements of information theory with the Chen-Stein method for the Poisson approximation. The resulting bounds are easy to compute, and their applicability is exemplified.</data>
    </node>
    <node id="P117920">
      <data key="title">tight bounds for symmetric divergence measures and a refined bound for lossless source coding</data>
      <data key="abstract">Tight bounds for several symmetric divergence measures are derived in terms of the total variation distance. It is shown that each of these bounds is attained by a pair of two- or three-element probability distributions. An application of these bounds for lossless source coding is provided, refining and improving a certain bound by Csiszar. Another application of these bounds has been recently introduced by Yardi  et al.  for channel-code detection.</data>
    </node>
    <node id="P113827">
      <data key="title">entropy power inequality for a family of discrete random variables</data>
      <data key="abstract">It is known that the Entropy Power Inequality (EPI) always holds if the random variables have density. Not much work has been done to identify discrete distributions for which the inequality holds with the differential entropy replaced by the discrete entropy. Harremoes and Vignat showed that it holds for the pair (B(m, p),B(n, p)), m, n ∈ ℕ, (where B(n, p) is a Binomial distribution with n trials each with success probability p) for p = 0.5. In this paper, we considerably expand the set of Binomial distributions for which the inequality holds and, in particular, identify n 0 (p) such that for all m, n ≥ n 0 (p), the EPI holds for (B(m, p),B(n, p)). We further show that the EPI holds for the discrete random variables that can be expressed as the sum of n independent and identically distributed (IID) discrete random variables for large n.</data>
    </node>
    <node id="P16110">
      <data key="title">on renyi entropy power inequalities</data>
      <data key="abstract">This paper gives improved R\'{e}nyi entropy power inequalities (R-EPIs). Consider a sum $S_n = \sum_{k=1}^n X_k$ of $n$ independent continuous random vectors taking values on $\mathbb{R}^d$, and let $\alpha \in [1, \infty]$. An R-EPI provides a lower bound on the order-$\alpha$ R\'enyi entropy power of $S_n$ that, up to a multiplicative constant (which may depend in general on $n, \alpha, d$), is equal to the sum of the order-$\alpha$ R\'enyi entropy powers of the $n$ random vectors $\{X_k\}_{k=1}^n$. For $\alpha=1$, the R-EPI coincides with the well-known entropy power inequality by Shannon. The first improved R-EPI is obtained by tightening the recent R-EPI by Bobkov and Chistyakov which relies on the sharpened Young's inequality. A further improvement of the R-EPI also relies on convex optimization and results on rank-one modification of a real-valued diagonal matrix.</data>
    </node>
    <node id="P166939">
      <data key="title">monotonic convergence in an information theoretic law of small numbers</data>
      <data key="abstract">An "entropy increasing to the maximum" result analogous to the entropic central limit theorem (Barron 1986; Artstein 2004) is obtained in the discrete setting. This involves the thinning operation and a Poisson limit. Monotonic convergence in relative entropy is established for general discrete distributions, while monotonic increase of Shannon entropy is proved for the special class of ultra-log-concave distributions. Overall we extend the parallel between the information-theoretic central limit theorem and law of small numbers explored by Kontoyiannis (2005) and HarremoEumls (2007, 2008, 2009). Ingredients in the proofs include convexity, majorization, and stochastic orders.</data>
    </node>
    <node id="P124665">
      <data key="title">a de bruijn identity for discrete random variables</data>
      <data key="abstract">We discuss properties of the “beamsplitter addition” operation, which provides a non-standard scaled convolution of random variables supported on the non-negative integers. We give a simple expression for the action of beamsplitter addition using generating functions. We use this to give a self-contained and purely classical proof of a heat equation and de Bruijn identity, satisfied when one of the variables is geometric.</data>
    </node>
    <node id="P87531">
      <data key="title">sharp bounds on the entropy of the poisson law and related quantities</data>
      <data key="abstract">One of the difficulties in calculating the capacity of certain Poisson channels is that H(?), the entropy of the Poisson distribution with mean ?, is not available in a simple form. In this paper, we derive upper and lower bounds for H(?) that are asymptotically tight and easy to compute. The derivation of such bounds involves only simple probabilistic and analytic tools. This complements the asymptotic expansions of Knessl (1998), Jacquet and Szpankowski (1999), and Flajolet (1999). The same method yields tight bounds on the relative entropy D(n, p) between a binomial and a Poisson, thus refining the work of Harremoe?s and Ruzankin (2004). Bounds on the entropy of the binomial also follow easily.</data>
    </node>
    <node id="P13953">
      <data key="title">generalized entropy power inequalities and monotonicity properties of information</data>
      <data key="abstract">New families of Fisher information and entropy power inequalities for sums of independent random variables are presented. These inequalities relate the information in the sum of $n$ independent random variables to the information contained in sums over subsets of the random variables, for an arbitrary collection of subsets. As a consequence, a simple proof of the monotonicity of information in central limit theorems is obtained, both in the setting of i.i.d. summands as well as in the more general setting of independent summands with variance-standardized sums.</data>
    </node>
    <node id="P96863">
      <data key="title">entropies of weighted sums in cyclic groups and an application to polar codes</data>
      <data key="abstract">In this note, the following basic question is explored: in a cyclic group, how are the Shannon entropies of the sum and difference of i.i.d. random variables related to each other? For the integer group, we show that they can differ by any real number additively, but not too much multiplicatively; on the other hand, for $\mathbb{Z}/3\mathbb{Z}$, the entropy of the difference is always at least as large as that of the sum. These results are closely related to the study of more-sum-than-difference (i.e. MSTD) sets in additive combinatorics. We also investigate polar codes for $q$-ary input channels using non-canonical kernels to construct the generator matrix, and present applications of our results to constructing polar codes with significantly improved error probability compared to the canonical construction.</data>
    </node>
    <node id="P10030">
      <data key="title">a new entropy power inequality for integer valued random variables</data>
      <data key="abstract">The entropy power inequality (EPI) provides lower bounds on the differential entropy of the sum of two independent real-valued random variables in terms of the individual entropies. Versions of the EPI for discrete random variables have been obtained for special families of distributions with the differential entropy replaced by the discrete entropy, but no universal inequality is known (beyond trivial ones). More recently, the sumset theory for the entropy function provides a sharp inequality $H(X+X')-H(X)\geq 1/2 -o(1)$ when $X,X'$ are i.i.d. with high entropy. This paper provides the inequality $H(X+X')-H(X) \geq g(H(X))$, where $X,X'$ are arbitrary i.i.d. integer-valued random variables and where $g$ is a universal strictly positive function on $\mR_+$ satisfying $g(0)=0$. Extensions to non identically distributed random variables and to conditional entropies are also obtained.</data>
    </node>
    <node id="P51821">
      <data key="title">universal compression of power law distributions</data>
      <data key="abstract">English words and the outputs of many other natural processes are well-known to follow a Zipf distribution. Yet this thoroughly-established property has never been shown to help compress or predict these important processes. We show that the expected redundancy of Zipf distributions of order $\alpha&gt;1$ is roughly the $1/\alpha$ power of the expected redundancy of unrestricted distributions. Hence for these orders, Zipf distributions can be better compressed and predicted than was previously known. Unlike the expected case, we show that worst-case redundancy is roughly the same for Zipf and for unrestricted distributions. Hence Zipf distributions have significantly different worst-case and expected redundancies, making them the first natural distribution class shown to have such a difference.</data>
    </node>
    <node id="P154966">
      <data key="title">entropy bounds for discrete random variables via maximal coupling</data>
      <data key="abstract">This paper derives new bounds on the difference of the entropies of two discrete random variables in terms of the local and total variation distances between their probability mass functions. The derivation of the bounds relies on maximal coupling, and they apply to discrete random variables which are defined over finite or countably infinite alphabets. Loosened versions of these bounds are demonstrated to reproduce some previously reported results. The use of the new bounds is exemplified for the Poisson approximation, where bounds on the local and total variation distances follow from Stein's method.</data>
    </node>
    <node id="P149320">
      <data key="title">adaptive sensing using deterministic partial hadamard matrices</data>
      <data key="abstract">This paper investigates the construction of deterministic matrices preserving the entropy of random vectors with a given probability distribution. In particular, it is shown that for random vectors having i.i.d. discrete components, this is achieved by selecting a subset of rows of a Hadamard matrix such that (i) the selection is deterministic (ii) the fraction of selected rows is vanishing. In contrast, it is shown that for random vectors with i.i.d. continuous components, no partial Hadamard matrix of reduced dimension allows to preserve the entropy. These results are in agreement with the results of Wu-Verdu on almost lossless analog compression. This paper is however motivated by the complexity attribute of Hadamard matrices, which allows the use of efficient and stable reconstruction algorithms. The proof technique is based on a polar code martingale argument and on a new entropy power inequality for integer-valued random variables.</data>
    </node>
    <node id="P14623">
      <data key="title">monotonicity thinning and discrete versions of the entropy power inequality</data>
      <data key="abstract">We consider the entropy of sums of independent discrete random variables, in analogy with Shannon's Entropy Power Inequality, where equality holds for normals. In our case, infinite divisibility suggests that equality should hold for Poisson variables. We show that some natural analogues of the EPI do not in fact hold, but propose an alternative formulation which does always hold. The key to many proofs of Shannon's EPI is the behavior of entropy on scaling of continuous random variables. We believe that Renyi's operation of thinning discrete random variables plays a similar role to scaling, and give a sharp bound on how the entropy of ultra log-concave random variables behaves on thinning. In the spirit of the monotonicity results established by Artstein, Ball, Barthe, and Naor, we prove a stronger version of concavity of entropy, which implies a strengthened form of our discrete EPI.</data>
    </node>
    <node id="P142425">
      <data key="title">note on noisy group testing asymptotic bounds and belief propagation reconstruction</data>
      <data key="abstract">An information theoretic perspective on group testing problems has recently been proposed by Atia and Saligrama, in order to characterise the optimal number of tests. Their results hold in the noiseless case, where only false positives occur, and where only false negatives occur. We extend their results to a model containing both false positives and false negatives, developing simple information theoretic bounds on the number of tests required. Based on these bounds, we obtain an improved order of convergence in the case of false negatives only. Since these results are based on (computationally infeasible) joint typicality decoding, we propose a belief propagation algorithm for the detection of defective items and compare its actual performance to the theoretical bounds.</data>
    </node>
    <node id="P94130">
      <data key="title">continuity of mutual entropy in the large signal to noise ratio limit</data>
      <data key="abstract">This article addresses the issue of the proof of the entropy power inequality (EPI), an important tool in the analysis of Gaussian channels of information transmission, proposed by Shannon. #R##N#We analyse continuity properties of the mutual entropy of the input and output signals in an additive memoryless channel and discuss assumptions under which the entropy-power inequality holds true.</data>
    </node>
    <edge source="P40047" target="P113827">
      <data key="relation">reference</data>
    </edge>
    <edge source="P40047" target="P14623">
      <data key="relation">reference</data>
    </edge>
    <edge source="P40047" target="P10030">
      <data key="relation">reference</data>
    </edge>
    <edge source="P88940" target="P14623">
      <data key="relation">reference</data>
    </edge>
    <edge source="P88940" target="P96863">
      <data key="relation">reference</data>
    </edge>
    <edge source="P88940" target="P13953">
      <data key="relation">reference</data>
    </edge>
    <edge source="P88940" target="P10030">
      <data key="relation">reference</data>
    </edge>
    <edge source="P88940" target="P16110">
      <data key="relation">reference</data>
    </edge>
    <edge source="P153577" target="P72">
      <data key="relation">reference</data>
    </edge>
    <edge source="P35796" target="P87531">
      <data key="relation">reference</data>
    </edge>
    <edge source="P72" target="P14623">
      <data key="relation">reference</data>
    </edge>
    <edge source="P72" target="P87531">
      <data key="relation">reference</data>
    </edge>
    <edge source="P72" target="P166939">
      <data key="relation">reference</data>
    </edge>
    <edge source="P72" target="P154966">
      <data key="relation">reference</data>
    </edge>
    <edge source="P72" target="P117920">
      <data key="relation">reference</data>
    </edge>
    <edge source="P117920" target="P154966">
      <data key="relation">reference</data>
    </edge>
    <edge source="P113827" target="P14623">
      <data key="relation">reference</data>
    </edge>
    <edge source="P113827" target="P166939">
      <data key="relation">reference</data>
    </edge>
    <edge source="P113827" target="P13953">
      <data key="relation">reference</data>
    </edge>
    <edge source="P113827" target="P16110">
      <data key="relation">reference</data>
    </edge>
    <edge source="P113827" target="P10030">
      <data key="relation">reference</data>
    </edge>
    <edge source="P16110" target="P14623">
      <data key="relation">reference</data>
    </edge>
    <edge source="P16110" target="P13953">
      <data key="relation">reference</data>
    </edge>
    <edge source="P16110" target="P10030">
      <data key="relation">reference</data>
    </edge>
    <edge source="P166939" target="P14623">
      <data key="relation">reference</data>
    </edge>
    <edge source="P166939" target="P87531">
      <data key="relation">reference</data>
    </edge>
    <edge source="P166939" target="P13953">
      <data key="relation">reference</data>
    </edge>
    <edge source="P124665" target="P14623">
      <data key="relation">reference</data>
    </edge>
    <edge source="P124665" target="P13953">
      <data key="relation">reference</data>
    </edge>
    <edge source="P87531" target="P51821">
      <data key="relation">reference</data>
    </edge>
    <edge source="P87531" target="P154966">
      <data key="relation">reference</data>
    </edge>
    <edge source="P13953" target="P14623">
      <data key="relation">reference</data>
    </edge>
    <edge source="P13953" target="P10030">
      <data key="relation">reference</data>
    </edge>
    <edge source="P96863" target="P14623">
      <data key="relation">reference</data>
    </edge>
    <edge source="P96863" target="P10030">
      <data key="relation">reference</data>
    </edge>
    <edge source="P10030" target="P14623">
      <data key="relation">reference</data>
    </edge>
    <edge source="P10030" target="P149320">
      <data key="relation">reference</data>
    </edge>
    <edge source="P149320" target="P14623">
      <data key="relation">reference</data>
    </edge>
    <edge source="P14623" target="P94130">
      <data key="relation">reference</data>
    </edge>
    <edge source="P14623" target="P142425">
      <data key="relation">reference</data>
    </edge>
  </graph>
</graphml>
