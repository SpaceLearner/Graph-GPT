<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="relation" for="edge" attr.name="relation" attr.type="string" />
  <key id="abstract" for="node" attr.name="abstract" attr.type="string" />
  <key id="title" for="node" attr.name="title" attr.type="string" />
  <graph edgedefault="undirected">
    <node id="P40338">
      <data key="title">potential and pitfalls of multi armed bandits for decentralized spatial reuse in wlans</data>
      <data key="abstract">Abstract   Spatial Reuse (SR) has recently gained attention to maximize the performance of IEEE 802.11 Wireless Local Area Networks (WLANs). Decentralized mechanisms are expected to be key in the development of SR solutions for next-generation WLANs, since many deployments are characterized by being uncoordinated by nature. However, the potential of decentralized mechanisms is limited by the significant lack of knowledge with respect to the overall wireless environment. To shed some light on this subject, we show the main considerations and possibilities of applying online learning to address the SR problem in uncoordinated WLANs. In particular, we provide a solution based on Multi-Armed Bandits (MABs) whereby independent WLANs dynamically adjust their frequency channel, transmit power and sensitivity threshold. To that purpose, we provide two different strategies, which refer to selfish and environment-aware learning. While the former stands for pure individual behavior, the second one considers the performance experienced by surrounding networks, thus taking into account the impact of individual actions on the environment. Through these two strategies we delve into practical issues of applying MABs in wireless networks, such as convergence guarantees or adversarial effects. Our simulation results illustrate the potential of the proposed solutions for enabling SR in future WLANs. We show that substantial improvements on network performance can be achieved regarding throughput and fairness.</data>
    </node>
    <node id="P14714">
      <data key="title">multi user lax communications a multi armed bandit approach</data>
      <data key="abstract">Inspired by cognitive radio networks, we consider a setting where multiple users share several channels modeled as a multi-user multi-armed bandit (MAB) problem. The characteristics of each channel are unknown and are different for each user. Each user can choose between the channels, but her success depends on the particular channel chosen as well as on the selections of other users: if two users select the same channel their messages collide and none of them manages to send any data. Our setting is fully distributed, so there is no central control. As in many communication systems, the users cannot set up a direct communication protocol, so information exchange must be limited to a minimum. We develop an algorithm for learning a stable configuration for the multi-user MAB problem. We further offer both convergence guarantees and experiments inspired by real communication networks, including comparison to state-of-the-art algorithms.</data>
    </node>
    <node id="P119454">
      <data key="title">spectrum access in cognitive radio using a two stage reinforcement learning approach</data>
      <data key="abstract">With the advent of the fifth generation of wireless standards and an increasing demand for higher throughput, methods to improve spectral efficiency of wireless systems have become very important. In the context of cognitive radio, a substantial increase in throughput is possible if the secondary user can make smart decisions regarding which channel to sense and when or how often to sense. Here, we propose an algorithm to not only select a channel for data transmission, but also to predict how long the channel will remain unoccupied so that the time spent on channel sensing can be minimized. Our algorithm learns in two stagesâ€”a reinforcement learning approach for channel selection and a Bayesian approach to determine the duration for which sensing can be skipped. Comparisons with other methods are provided through extensive simulations. We show that the number of sensing operations is minimized with negligible increase in primary user interference; this implies that less energy is spent by the secondary user in sensing, and also higher throughput is achieved by saving the time spent on sensing.</data>
    </node>
    <node id="P10090">
      <data key="title">approximation algorithms for restless bandit problems</data>
      <data key="abstract">The restless bandit problem is one of the most well-studied generalizations of the celebrated stochastic multi-armed bandit problem in decision theory. In its ultimate generality, the restless bandit problem is known to be PSPACE-Hard to approximate to any non-trivial factor, and little progress has been made despite its importance in modeling activity allocation under uncertainty. #R##N#We consider a special case that we call Feedback MAB, where the reward obtained by playing each of n independent arms varies according to an underlying on/off Markov process whose exact state is only revealed when the arm is played. The goal is to design a policy for playing the arms in order to maximize the infinite horizon time average expected reward. This problem is also an instance of a Partially Observable Markov Decision Process (POMDP), and is widely studied in wireless scheduling and unmanned aerial vehicle (UAV) routing. Unlike the stochastic MAB problem, the Feedback MAB problem does not admit to greedy index-based optimal policies. #R##N#We develop a novel and general duality-based algorithmic technique that yields a surprisingly simple and intuitive 2+epsilon-approximate greedy policy to this problem. We then define a general sub-class of restless bandit problems that we term Monotone bandits, for which our policy is a 2-approximation. Our technique is robust enough to handle generalizations of these problems to incorporate various side-constraints such as blocking plays and switching costs. This technique is also of independent interest for other restless bandit problems. By presenting the first (and efficient) O(1) approximations for non-trivial instances of restless bandits as well as of POMDPs, our work initiates the study of approximation algorithms in both these contexts.</data>
    </node>
    <node id="P87350">
      <data key="title">learning to coordinate without communication in multi user multi armed bandit problems</data>
      <data key="abstract">We consider a setting where multiple users share multiple channels modeled as a multi-user multi-armed bandit (MAB) problem. The characteristics of each channel are initially unknown and may differ between the users. Each user can choose between the channels, but her success depends on the particular channel as well as on the selections of other users: if two users select the same channel their messages collide and none of them manages to send any data. Our setting is fully distributed, so there is no central control and every user only observes the channel she currently uses. As in many communication systems such as cognitive radio networks, the users cannot communicate among themselves so coordination must be achieved without direct communication. We develop algorithms for learning a stable configuration for the multiple user MAB problem. We further offer both convergence guarantees and experiments inspired by real communication networks.</data>
    </node>
    <node id="P113892">
      <data key="title">extended ucb policy for multi armed bandit with light tailed reward distributions</data>
      <data key="abstract">We consider the multi-armed bandit problems in which a player aims to accrue reward#N#         by sequentially playing a given set of arms with unknown reward statistics. In the classic#N#         work, policies were proposed to achieve the optimal logarithmic regret order for some#N#         special classes of light-tailed reward distributions, e.g., Auer et al.'s UCB1 index policy#N#         for reward distributions with finite support. In this paper, we extend Auer et al.'s UCB1#N#         index policy to achieve the optimal logarithmic regret order for all light-tailed (or#N#         equivalently, locally sub-Gaussian) reward distributions defined by the (local) existence#N#         of the moment-generating function.</data>
    </node>
    <node id="P32286">
      <data key="title">combinatorial multi objective multi armed bandit problem</data>
      <data key="abstract">In this paper, we introduce the COmbinatorial Multi-Objective Multi-Armed Bandit (COMO-MAB) problem that captures the challenges of combinatorial and multi-objective online learning simultaneously. In this setting, the goal of the learner is to choose an action at each time, whose reward vector is a linear combination of the reward vectors of the arms in the action, to learn the set of super Pareto optimal actions, which includes the Pareto optimal actions and actions that become Pareto optimal after adding an arbitrary small positive number to their expected reward vectors. We define the Pareto regret performance metric and propose a fair learning algorithm whose Pareto regret is $O(N L^3 \log T)$, where $T$ is the time horizon, $N$ is the number of arms and $L$ is the maximum number of arms in an action. We show that COMO-MAB has a wide range of applications, including recommending bundles of items to users and network routing, and focus on a resource-allocation application for multi-user communication in the presence of multidimensional performance metrics, where we show that our algorithm outperforms existing MAB algorithms.</data>
    </node>
    <node id="P85721">
      <data key="title">reinforcement learning a survey</data>
      <data key="abstract">This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.</data>
    </node>
    <node id="P110432">
      <data key="title">sic mmab synchronisation involves communication in multiplayer multi armed bandits</data>
      <data key="abstract">Motivated by cognitive radio networks, we consider the stochastic multiplayer multi-armed bandit problem, where several players pull arms simultaneously and collisions occur if one of them is pulled by several players at the same stage. We present a decentralized algorithm that achieves the same performance as a centralized one, contradicting the existing lower bounds for that problem. This is possible by "hacking" the standard model by constructing a communication protocol between players that deliberately enforces collisions, allowing them to share their information at a negligible cost. This motivates the introduction of a more appropriate dynamic setting without sensing, where similar communication protocols are no longer possible. However, we show that the logarithmic growth of the regret is still achievable for this model with a new algorithm.</data>
    </node>
    <node id="P31486">
      <data key="title">online learning for combinatorial network optimization with restless markovian rewards</data>
      <data key="abstract">Combinatorial network optimization algorithms that compute optimal structures taking into account edge weights form the foundation for many network protocols. Examples include shortest path routing, minimal spanning tree computation, maximum weighted matching on bipartite graphs, etc. We present CLRMR, the first online learning algorithm that efficiently solves the stochastic version of these problems where the underlying edge weights vary as independent Markov chains with unknown dynamics. #R##N#The performance of an online learning algorithm is characterized in terms of regret, defined as the cumulative difference in rewards between a suitably-defined genie, and that obtained by the given algorithm. We prove that, compared to a genie that knows the Markov transition matrices and uses the single-best structure at all times, CLRMR yields regret that is polynomial in the number of edges and nearly-logarithmic in time.</data>
    </node>
    <node id="P141773">
      <data key="title">on distributed cooperative decision making in multiarmed bandits</data>
      <data key="abstract">We study the explore-exploit tradeoff in distributed cooperative decision-making using the context of the multiarmed bandit (MAB) problem. For the distributed cooperative MAB problem, we design the cooperative UCB algorithm that comprises two interleaved distributed processes: (i) running consensus algorithms for estimation of rewards, and (ii) upper-confidence-bound-based heuristics for selection of arms. We rigorously analyze the performance of the cooperative UCB algorithm and characterize the influence of communication graph structure on the decision-making performance of the group.</data>
    </node>
    <node id="P46510">
      <data key="title">towards distribution free multi armed bandits with combinatorial strategies</data>
      <data key="abstract">In this paper we study a generalized version of classical multi-armed bandits (MABs) problem by allowing for arbitrary constraints on constituent bandits at each decision point. The motivation of this study comes from many situations that involve repeatedly making choices subject to arbitrary constraints in an uncertain environment: for instance, regularly deciding which advertisements to display online in order to gain high click-through-rate without knowing user preferences, or what route to drive home each day under uncertain weather and traffic conditions. Assume that there are $K$ unknown random variables (RVs), i.e., arms, each evolving as an \emph{i.i.d} stochastic process over time. At each decision epoch, we select a strategy, i.e., a subset of RVs, subject to arbitrary constraints on constituent RVs. #R##N#We then gain a reward that is a linear combination of observations on selected RVs. #R##N#The performance of prior results for this problem heavily depends on the distribution of strategies generated by corresponding learning policy. For example, if the reward-difference between the best and second best strategy approaches zero, prior result may lead to arbitrarily large regret. #R##N#Meanwhile, when there are exponential number of possible strategies at each decision point, naive extension of a prior distribution-free policy would cause poor performance in terms of regret, computation and space complexity. #R##N#To this end, we propose an efficient Distribution-Free Learning (DFL) policy that achieves zero regret, regardless of the probability distribution of the resultant strategies. #R##N#Our learning policy has both $O(K)$ time complexity and $O(K)$ space complexity. In successive generations, we show that even if finding the optimal strategy at each decision point is NP-hard, our policy still allows for approximated solutions while retaining near zero-regret.</data>
    </node>
    <node id="P55091">
      <data key="title">multi player bandits a trekking approach</data>
      <data key="abstract">We study stochastic multi-armed bandits with many players. The players do not know the number of players, cannot communicate with each other and if multiple players select a common arm they collide and none of them receive any reward. We consider the static scenario, where the number of players remains fixed, and the dynamic scenario, where the players enter and leave at any time. We provide algorithms based on a novel `trekking approach' that guarantees constant regret for the static case and sub-linear regret for the dynamic case with high probability. The trekking approach eliminates the need to estimate the number of players resulting in fewer collisions and improved regret performance compared to the state-of-the-art algorithms. We also develop an epoch-less algorithm that eliminates any requirement of time synchronization across the players provided each player can detect the presence of other players on an arm. We validate our theoretical guarantees using simulation based and real test-bed based experiments.</data>
    </node>
    <node id="P92331">
      <data key="title">almost optimal channel access in multi hop networks with unknown channel variables</data>
      <data key="abstract">We consider distributed channel access in multi-hop cognitive radio networks. Previous works on opportunistic channel access using multi-armed bandits (MAB) mainly focus on single-hop networks that assume complete conflicts among all secondary users. In the multi-hop multi-channel network settings studied here, there is more general competition among different communication pairs. We formulate the problem as a linearly combinatorial MAB problem that involves a maximum weighted independent set (MWIS) problem with unknown weights which need to learn. Existing methods for MAB where each of $N$ nodes chooses from $M$ channels have exponential time and space complexity $O(M^N)$, and poor theoretical guarantee on throughput performance. We propose a distributed channel access algorithm that can achieve $1/\rho$ of the optimum averaged throughput where each node has communication complexity $O(r^2+D)$ and space complexity $O(m)$ in the learning process, and time complexity $O(D m^{\rho^r})$ in strategy decision process for an arbitrary wireless network. Here $\rho=1+\epsilon$ is the approximation ratio to MWIS for a local $r$-hop network with $m&lt;N$ nodes,and $D$ is the number of mini-rounds inside each round of strategy decision. For randomly located networks with an average degree $d$, the time complexity is $O(d^{\rho^r})$.</data>
    </node>
    <node id="P137566">
      <data key="title">let cognitive radios imitate imitation based spectrum access for cognitive radio networks</data>
      <data key="abstract">In this paper, we tackle the problem of opportunistic spectrum access in large-scale cognitive radio networks, where the unlicensed Secondary Users (SU) access the frequency channels partially occupied by the licensed Primary Users (PU). Each channel is characterized by an availability probability unknown to the SUs. We apply evolutionary game theory to model the spectrum access problem and develop distributed spectrum access policies based on imitation, a behavior rule widely applied in human societies consisting of imitating successful behavior. We first develop two imitation-based spectrum access policies based on the basic Proportional Imitation (PI) rule and the more advanced Double Imitation (DI) rule given that a SU can imitate any other SUs. We then adapt the proposed policies to a more practical scenario where a SU can only imitate the other SUs operating on the same channel. A systematic theoretical analysis is presented for both scenarios on the induced imitation dynamics and the convergence properties of the proposed policies to an imitation-stable equilibrium, which is also the $\epsilon$-optimum of the system. Simple, natural and incentive-compatible, the proposed imitation-based spectrum access policies can be implemented distributedly based on solely local interactions and thus is especially suited in decentralized adaptive learning environments as cognitive radio networks.</data>
    </node>
    <node id="P140834">
      <data key="title">multi user communication networks a coordinated multi armed bandit approach</data>
      <data key="abstract">Communication networks shared by many users are a widespread challenge nowadays. In this paper we address several aspects of this challenge simultaneously: learning unknown stochastic network characteristics, sharing resources with other users while keeping coordination overhead to a minimum. The proposed solution combines Multi-Armed Bandit learning with a lightweight signalling-based coordination scheme, and ensures convergence to a stable allocation of resources. Our work considers single-user level algorithms for two scenarios: an unknown fixed number of users, and a dynamic number of users. Analytic performance guarantees, proving convergence to stable marriage configurations, are presented for both setups. The algorithms are designed based on a system-wide perspective, rather than focusing on single user welfare. Thus, maximal resource utilization is ensured. An extensive experimental analysis covers convergence to a stable configuration as well as reward maximization. Experiments are carried out over a wide range of setups, demonstrating the advantages of our approach over existing state-of-the-art methods.</data>
    </node>
    <node id="P25350">
      <data key="title">distributed algorithms for learning and cognitive medium access with logarithmic regret</data>
      <data key="abstract">The problem of distributed learning and channel access is considered in a cognitive network with multiple secondary users. The availability statistics of the channels are initially unknown to the secondary users and are estimated using sensing decisions. There is no explicit information exchange or prior agreement among the secondary users and sensing and access decisions are undertaken by them in a completely distributed manner. We propose policies for distributed learning and access which achieve order-optimal cognitive system throughput (number of successful secondary transmissions) under self play, i.e., when implemented at all the secondary users. Equivalently, our policies minimize the sum regret in distributed learning and access, which is the loss in secondary throughput due to learning and distributed access. For the scenario when the number of secondary users is known to the policy, we prove that the total regret is logarithmic in the number of transmission slots. This policy achieves order-optimal regret based on a logarithmic lower bound for regret under any uniformly-good learning and access policy. We then consider the case when the number of secondary users is fixed but unknown, and is estimated at each user through feedback. We propose a policy whose sum regret grows only slightly faster than logarithmic in the number of transmission slots.</data>
    </node>
    <node id="P17496">
      <data key="title">multi player bandits a musical chairs approach</data>
      <data key="abstract">We consider a variant of the stochastic multi-armed bandit problem, where multiple players simultaneously choose from the same set of arms and may collide, receiving no reward. This setting has been motivated by problems arising in cognitive radio networks, and is especially challenging under the realistic assumption that communication between players is limited. We provide a communication-free algorithm (Musical Chairs) which attains constant regret with high probability, as well as a sublinear-regret, communication-free algorithm (Dynamic Musical Chairs) for the more difficult setting of players dynamically entering and leaving throughout the game. Moreover, both algorithms do not require prior knowledge of the number of players. To the best of our knowledge, these are the first communication-free algorithms with these types of formal guarantees. We also rigorously compare our algorithms to previous works, and complement our theoretical findings with experiments.</data>
    </node>
    <node id="P10">
      <data key="title">online learning in decentralized multiuser resource sharing problems</data>
      <data key="abstract">In this paper, we consider the general scenario of resource sharing in a decentralized system when the resource rewards/qualities are time-varying and unknown to the users, and using the same resource by multiple users leads to reduced quality due to resource sharing. Firstly, we consider a user-independent reward model with no communication between the users, where a user gets feedback about the congestion level in the resource it uses. Secondly, we consider user-specific rewards and allow costly communication between the users. The users have a cooperative goal of achieving the highest system utility. There are multiple obstacles in achieving this goal such as the decentralized nature of the system, unknown resource qualities, communication, computation and switching costs. We propose distributed learning algorithms with logarithmic regret with respect to the optimal allocation. Our logarithmic regret result holds under both i.i.d. and Markovian reward models, as well as under communication, computation and switching costs.</data>
    </node>
    <node id="P103322">
      <data key="title">multi agent q learning of channel selection in multi user cognitive radio systems a two by two case</data>
      <data key="abstract">Resource allocation is an important issue in cognitive radio systems. It can be done by carrying out negotiation among secondary users. However, significant overhead may be incurred by the negotiation since the negotiation needs to be done frequently due to the rapid change of primary users' activity. In this paper, a channel selection scheme without negotiation is considered for multi-user and multi-channel cognitive radio systems. To avoid collision incurred by non-coordination, each secondary user learns how to select channels according to its experience. Multi-agent reinforcement leaning (MARL) is applied in the framework of Q-learning by considering opponent secondary users as a part of the environment. The dynamics of the Q-learning are illustrated using Metrick-Polak plot. A rigorous proof of the convergence of Q-learning is provided via the similarity between the Q-learning and Robinson-Monro algorithm, as well as the analysis of convergence of the corresponding ordinary differential equation (via Lyapunov function). Examples are illustrated and the performance of learning is evaluated by numerical simulations.</data>
    </node>
    <edge source="P40338" target="P103322">
      <data key="relation">reference</data>
    </edge>
    <edge source="P40338" target="P25350">
      <data key="relation">reference</data>
    </edge>
    <edge source="P40338" target="P17496">
      <data key="relation">reference</data>
    </edge>
    <edge source="P14714" target="P25350">
      <data key="relation">reference</data>
    </edge>
    <edge source="P119454" target="P103322">
      <data key="relation">reference</data>
    </edge>
    <edge source="P119454" target="P25350">
      <data key="relation">reference</data>
    </edge>
    <edge source="P10090" target="P10">
      <data key="relation">reference</data>
    </edge>
    <edge source="P87350" target="P25350">
      <data key="relation">reference</data>
    </edge>
    <edge source="P87350" target="P17496">
      <data key="relation">reference</data>
    </edge>
    <edge source="P113892" target="P25350">
      <data key="relation">reference</data>
    </edge>
    <edge source="P113892" target="P141773">
      <data key="relation">reference</data>
    </edge>
    <edge source="P32286" target="P25350">
      <data key="relation">reference</data>
    </edge>
    <edge source="P85721" target="P25350">
      <data key="relation">reference</data>
    </edge>
    <edge source="P110432" target="P25350">
      <data key="relation">reference</data>
    </edge>
    <edge source="P110432" target="P17496">
      <data key="relation">reference</data>
    </edge>
    <edge source="P110432" target="P140834">
      <data key="relation">reference</data>
    </edge>
    <edge source="P31486" target="P10">
      <data key="relation">reference</data>
    </edge>
    <edge source="P141773" target="P25350">
      <data key="relation">reference</data>
    </edge>
    <edge source="P46510" target="P10">
      <data key="relation">reference</data>
    </edge>
    <edge source="P46510" target="P25350">
      <data key="relation">reference</data>
    </edge>
    <edge source="P55091" target="P25350">
      <data key="relation">reference</data>
    </edge>
    <edge source="P55091" target="P17496">
      <data key="relation">reference</data>
    </edge>
    <edge source="P92331" target="P10">
      <data key="relation">reference</data>
    </edge>
    <edge source="P92331" target="P25350">
      <data key="relation">reference</data>
    </edge>
    <edge source="P137566" target="P25350">
      <data key="relation">reference</data>
    </edge>
    <edge source="P140834" target="P25350">
      <data key="relation">reference</data>
    </edge>
    <edge source="P140834" target="P17496">
      <data key="relation">reference</data>
    </edge>
    <edge source="P25350" target="P10">
      <data key="relation">reference</data>
    </edge>
    <edge source="P25350" target="P103322">
      <data key="relation">reference</data>
    </edge>
    <edge source="P25350" target="P17496">
      <data key="relation">reference</data>
    </edge>
  </graph>
</graphml>
