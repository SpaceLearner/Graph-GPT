{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import json\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_entities(path=\"./\"):\n",
    "    \n",
    "    id2aff_df     = pd.read_csv(path+\"e_affiliation.csv\")\n",
    "    id2author_df  = pd.read_csv(path+\"e_author.csv\")\n",
    "    id2concept_df = pd.read_csv(path+\"e_concept.csv\")\n",
    "    auid2affid_df = pd.read_csv(path+\"r_author2affiliation.csv\")\n",
    "    auid2conid_df = pd.read_csv(path+\"r_author2concept.csv\")\n",
    "    \n",
    "    id2aff     = {}\n",
    "    id2author  = {}\n",
    "    id2concept = {} \n",
    "    auid2affid = defaultdict(lambda x : -1)\n",
    "    auid2conid = defaultdict(list)\n",
    "    \n",
    "    for index, row in id2aff_df.iterrows():\n",
    "        id2aff[row[\"affiliationID:ID\"]] = row[\"name\"]\n",
    "    for index, row in id2author_df.iterrows():\n",
    "        id2author[row[\"authorID:ID\"]]   = row[\"name\"]\n",
    "    for index, row in id2concept_df.iterrows():\n",
    "        id2concept[row[\"conceptID:ID\"]] = row[\"name\"]\n",
    "    for index, row in auid2affid_df.iterrows():\n",
    "        auid2affid[row[\":START_ID\"]] = row[\":END_ID\"]\n",
    "    for index, row in auid2conid_df.iterrows():\n",
    "        auid2conid[row[\":START_ID\"]].append(row[\":END_ID\"])\n",
    "    #     if row[\":START_ID\"] not in auid2conid:\n",
    "    #         auid2conid[row[\":START_ID\"]] = [row[\":END_ID\"]]\n",
    "    #     else:\n",
    "    #         auid2conid[row[\":START_ID\"]].append(row[\":END_ID\"])\n",
    "    id2aff[-1] = \"None\"\n",
    "    \n",
    "    return id2author, id2aff, id2concept, auid2affid, auid2conid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m id2author, id2aff, id2concept, auid2affid, auid2conid \u001b[39m=\u001b[39m load_entities()\n",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m, in \u001b[0;36mload_entities\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      3\u001b[0m id2aff_df     \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(path\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39me_affiliation.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m id2author_df  \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(path\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39me_author.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m id2concept_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(path\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39me_concept.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m auid2affid_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(path\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mr_author2affiliation.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m auid2conid_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(path\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mr_author2concept.csv\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chatgpt/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chatgpt/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chatgpt/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chatgpt/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chatgpt/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m         nrows\n\u001b[1;32m   1780\u001b[0m     )\n\u001b[1;32m   1781\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chatgpt/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[1;32m    231\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chatgpt/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chatgpt/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chatgpt/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chatgpt/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "id2author, id2aff, id2concept, auid2affid, auid2conid = load_entities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "pkl.dump(id2author,  open(\"id2author.pkl\",  \"wb\"))\n",
    "pkl.dump(id2aff,     open(\"id2aff.pkl\",     \"wb\"))\n",
    "pkl.dump(id2concept, open(\"id2concept.pkl\", \"wb\"))\n",
    "pkl.dump(json.loads(json.dumps(auid2affid)), open(\"auid2affid.pkl\", \"wb\"))\n",
    "pkl.dump(auid2conid, open(\"auid2conid.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id2author  = pkl.load(open(\"/Users/goojy/Documents/Project/GPT-GraphMaster/HIN/input/id2author.pkl\", \"rb\"))\n",
    "id2aff     = pkl.load(open(\"/Users/goojy/Documents/Project/GPT-GraphMaster/HIN/input/id2aff.pkl\", \"rb\"))\n",
    "id2concept = pkl.load(open(\"/Users/goojy/Documents/Project/GPT-GraphMaster/HIN/input/id2concept.pkl\", \"rb\"))\n",
    "auid2affid = pkl.load(open(\"/Users/goojy/Documents/Project/GPT-GraphMaster/HIN/input/auid2affid.pkl\", \"rb\"))\n",
    "auid2conid = pkl.load(open(\"/Users/goojy/Documents/Project/GPT-GraphMaster/HIN/input/auid2conid.pkl\", \"rb\"))\n",
    "r_coauthor = pd.read_csv(\"/Users/goojy/Documents/Project/GPT-GraphMaster/HIN/preprocessing/r_coauthor.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_from_id = r_coauthor[\":START_ID\"].values.tolist()\n",
    "author_from    = [id2author[idx] for idx in author_from_id]\n",
    "author_to_id   = r_coauthor[\":END_ID\"].values.tolist()\n",
    "author_to      = [id2author[idx] for idx in author_to_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "College of Geography Science, Nanjing Normal University, Nanjing 210097, China\n",
      "['1', '2', '4', '5', '7', '8', '10', '13', '15', '20', '21', '22', '23', '27', '28', '29', '32', '33', '34', '35', '36', '37', '40', '41', '43', '44', '45', '46', '51', '52', '53', '66', '67', '68', '71', '74', '79', '80', '81', '90', '93', '95', '96', '97', '98', '99', '100', '101', '103', '105', '106', '108', '111', '113', '114', '115', '117', '129', '130', '131', '132', '138', '140', '141', '147', '148', '150', '151', '153', '156', '157', '158', '159', '163', '165', '169', '171', '173', '175', '178', '181', '189', '194', '195', '196', '199', '206', '207', '208', '214', '216', '218', '221', '222', '223', '225', '228', '231', '232', '240']\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101]\n"
     ]
    }
   ],
   "source": [
    "print(id2aff[3])\n",
    "print(list(auid2affid.keys())[:100])\n",
    "print(list(auid2conid.keys())[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.Graph()\n",
    "nodes = [(id2author[key], {\"affiliation\": id2aff[auid2affid[str(key)]] if str(key) in auid2affid else \"Nan\", \"research\": \", \".join([id2concept[value] for value in auid2conid[key]][:3])}) for key in list(id2author.keys())]\n",
    "graph.add_nodes_from(nodes)\n",
    "edges = list(zip(author_from, author_to))\n",
    "# edges = [(edge, {\"relation\": \"collaboration\"}) for edge in edges]\n",
    "graph.add_edges_from(edges, relation=\"collaborate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Xinping Yan', {'affiliation': 'Nan', 'research': 'Intelligent Transport Systems, Intelligent Transportation Systems, Intelligent Transportation Systems Center'}), ('Tracey Rizzuto', {'affiliation': 'Nan', 'research': 'common error, Urban Legend, Common Errors'}), ('Jung Min Kong', {'affiliation': 'Nan', 'research': 'new data, Rule Refinement, Extended Data ExpressionThe rule'}), ('Raymond Leroy Springston', {'affiliation': 'Nan', 'research': 'CSE capstone design course'}), ('Lidija Petkovska', {'affiliation': 'Nan', 'research': 'dimensional field, electrical machine, permanent magnet'}), ('Hugo Ortega', {'affiliation': 'Columbia University', 'research': 'multimedia content, multimedia technology, ACM Multimedia Interactive Art'}), ('Chris Morrone', {'affiliation': 'Lawrence Livermore National Laboratory', 'research': 'Lustre file system, file system software, file system testing'}), ('Woochul Shin', {'affiliation': 'School of Computer Science, Inha University, Incheon, Korea', 'research': 'Web Service, context information, mobile device'}), ('Katsushi Nishino', {'affiliation': 'Nan', 'research': 'GaN quantum, metalorganic chemical vapor deposition'}), ('Kazuhiko Nishino', {'affiliation': 'Nan', 'research': 'Designing Neural Networks, Genetic Algorithms, Structural Learning'})]\n"
     ]
    }
   ],
   "source": [
    "print(list(graph.nodes(data=True))[60:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:02,  7.35it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "cnt = 0\n",
    "for idx, node in tqdm(enumerate(list(graph.nodes(data=False))[:500])):\n",
    "    nodesl = nx.bfs_successors(graph, node, depth_limit=4)\n",
    "    nodesl = [nodel[0] for nodel in list(nodesl)][:20]\n",
    "    if len(nodesl) < 10:\n",
    "        continue\n",
    "    subgraph = nx.subgraph(graph, nodesl)\n",
    "    if subgraph.number_of_edges() > 50:\n",
    "        continue\n",
    "    nx.write_gml(subgraph, path=\"../input/GML/graph_\"  + str(cnt) + \".gml\")\n",
    "    nx.write_graphml(subgraph, path=\"../input/GraphML/graph_\"  + str(cnt) + \".graphml\", named_key_ids=True, encoding='utf-8')\n",
    "    nx.write_edgelist(subgraph, path=\"../input/EdgeList/graph_\" + str(cnt) + \".edgelist\", delimiter=\"\\t\", encoding=\"utf-8\")\n",
    "    nx.write_adjlist(subgraph, path=\"../input/AdjList/graph_\"  + str(cnt) + \".adjlist\", delimiter=\"\\t\", encoding=\"utf-8\")\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3f3a7aee03d57ba3bf39c6821762f519bd3b626d900ab04cd2acd986fdaeaff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
